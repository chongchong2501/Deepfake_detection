# å†…å­˜ç®¡ç†ä¼˜åŒ–æ¨¡å— - RTX4070ä¸“ç”¨

import torch
import gc
import psutil
import time
import threading
from typing import Dict, List, Optional, Callable
from dataclasses import dataclass
from collections import deque
import warnings

@dataclass
class MemoryStats:
    """å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
    cpu_percent: float
    cpu_memory_gb: float
    cpu_memory_percent: float
    gpu_memory_allocated_gb: float
    gpu_memory_cached_gb: float
    gpu_memory_reserved_gb: float
    gpu_memory_percent: float
    timestamp: float

class MemoryManager:
    """æ™ºèƒ½å†…å­˜ç®¡ç†å™¨ - RTX4070ä¼˜åŒ– v2.0"""
    
    def __init__(self, 
                 gpu_memory_threshold: float = 0.75,  # é™ä½GPUå†…å­˜ä½¿ç”¨é˜ˆå€¼
                 cpu_memory_threshold: float = 0.85,  # æé«˜CPUå†…å­˜ä½¿ç”¨é˜ˆå€¼
                 auto_cleanup_interval: float = 60.0,  # å¢åŠ è‡ªåŠ¨æ¸…ç†é—´éš”(ç§’)
                 enable_monitoring: bool = True,
                 verbose_output: bool = False):  # æ–°å¢ï¼šæ§åˆ¶è¾“å‡ºè¯¦ç»†ç¨‹åº¦
        
        self.gpu_memory_threshold = gpu_memory_threshold
        self.cpu_memory_threshold = cpu_memory_threshold
        self.auto_cleanup_interval = auto_cleanup_interval
        self.enable_monitoring = enable_monitoring
        self.verbose_output = verbose_output
        
        # å†…å­˜ç»Ÿè®¡å†å²
        self.memory_history: deque = deque(maxlen=1000)
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring_thread: Optional[threading.Thread] = None
        self.stop_monitoring = threading.Event()
        
        # å›è°ƒå‡½æ•°
        self.cleanup_callbacks: List[Callable] = []
        self.warning_callbacks: List[Callable] = []
        
        # è¾“å‡ºæ§åˆ¶
        self.last_cleanup_time = 0
        self.cleanup_message_interval = 30.0  # æ¸…ç†æ¶ˆæ¯é—´éš”
        
        # GPUä¿¡æ¯
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if torch.cuda.is_available():
            self.gpu_total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        else:
            self.gpu_total_memory = 0
        
        # CPUä¿¡æ¯
        self.cpu_total_memory = psutil.virtual_memory().total / (1024**3)
        
        if self.verbose_output:
            print(f"ğŸ”§ å†…å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
            print(f"   GPUæ€»å†…å­˜: {self.gpu_total_memory:.2f}GB")
            print(f"   CPUæ€»å†…å­˜: {self.cpu_total_memory:.2f}GB")
            print(f"   GPUé˜ˆå€¼: {gpu_memory_threshold*100:.0f}%")
            print(f"   CPUé˜ˆå€¼: {cpu_memory_threshold*100:.0f}%")
    
    def get_memory_stats(self) -> MemoryStats:
        """è·å–å½“å‰å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        # CPUç»Ÿè®¡
        cpu_percent = psutil.cpu_percent()
        cpu_memory = psutil.virtual_memory()
        cpu_memory_gb = cpu_memory.used / (1024**3)
        cpu_memory_percent = cpu_memory.percent / 100.0
        
        # GPUç»Ÿè®¡
        if torch.cuda.is_available():
            gpu_memory_allocated = torch.cuda.memory_allocated() / (1024**3)
            gpu_memory_cached = torch.cuda.memory_reserved() / (1024**3)
            gpu_memory_reserved = torch.cuda.memory_reserved() / (1024**3)
            gpu_memory_percent = gpu_memory_reserved / self.gpu_total_memory
        else:
            gpu_memory_allocated = gpu_memory_cached = gpu_memory_reserved = gpu_memory_percent = 0
        
        return MemoryStats(
            cpu_percent=cpu_percent,
            cpu_memory_gb=cpu_memory_gb,
            cpu_memory_percent=cpu_memory_percent,
            gpu_memory_allocated_gb=gpu_memory_allocated,
            gpu_memory_cached_gb=gpu_memory_cached,
            gpu_memory_reserved_gb=gpu_memory_reserved,
            gpu_memory_percent=gpu_memory_percent,
            timestamp=time.time()
        )
    
    def cleanup_gpu_memory(self, force: bool = False) -> float:
        """æ¸…ç†GPUå†…å­˜"""
        if not torch.cuda.is_available():
            return 0.0
        
        # è®°å½•æ¸…ç†å‰çš„å†…å­˜
        before_memory = torch.cuda.memory_allocated() / (1024**3)
        
        # æ¸…ç†ç¼“å­˜
        torch.cuda.empty_cache()
        
        if force:
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            torch.cuda.empty_cache()
            
            # åŒæ­¥GPUæ“ä½œ
            torch.cuda.synchronize()
        
        # è®°å½•æ¸…ç†åçš„å†…å­˜
        after_memory = torch.cuda.memory_allocated() / (1024**3)
        freed_memory = before_memory - after_memory
        
        if freed_memory > 0.1:  # é‡Šæ”¾è¶…è¿‡100MBæ—¶æ‰“å°ä¿¡æ¯
            print(f"ğŸ§¹ GPUå†…å­˜æ¸…ç†: é‡Šæ”¾äº† {freed_memory:.2f}GB")
        
        return freed_memory
    
    def cleanup_cpu_memory(self) -> None:
        """æ¸…ç†CPUå†…å­˜ï¼ˆå‡å°‘é‡å¤è¾“å‡ºï¼‰"""
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        if collected > 0 and self.verbose_output:
            print(f"ğŸ§¹ CPUå†…å­˜æ¸…ç†: å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
    
    def smart_cleanup(self) -> Dict[str, float]:
        """æ™ºèƒ½å†…å­˜æ¸…ç†ï¼ˆå‡å°‘é‡å¤è¾“å‡ºï¼‰"""
        stats = self.get_memory_stats()
        results = {'gpu_freed': 0.0, 'cpu_objects': 0}
        current_time = time.time()
        
        # GPUå†…å­˜æ¸…ç†
        if stats.gpu_memory_percent > self.gpu_memory_threshold:
            # æ§åˆ¶è¾“å‡ºé¢‘ç‡ï¼Œé¿å…é‡å¤æ¶ˆæ¯
            if (current_time - self.last_cleanup_time) > self.cleanup_message_interval or self.verbose_output:
                print(f"âš ï¸ GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {stats.gpu_memory_percent*100:.1f}%")
                self.last_cleanup_time = current_time
            
            results['gpu_freed'] = self.cleanup_gpu_memory(force=True)
            
            # æ‰§è¡Œæ³¨å†Œçš„æ¸…ç†å›è°ƒ
            for callback in self.cleanup_callbacks:
                try:
                    callback()
                except Exception as e:
                    if self.verbose_output:
                        print(f"æ¸…ç†å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
        
        # CPUå†…å­˜æ¸…ç†ï¼ˆæ›´æ™ºèƒ½çš„é˜ˆå€¼æ£€æŸ¥ï¼‰
        if stats.cpu_memory_percent > self.cpu_memory_threshold:
            # æ§åˆ¶è¾“å‡ºé¢‘ç‡
            if (current_time - self.last_cleanup_time) > self.cleanup_message_interval or self.verbose_output:
                print(f"âš ï¸ CPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {stats.cpu_memory_percent*100:.1f}%")
                self.last_cleanup_time = current_time
            
            before_objects = len(gc.get_objects())
            self.cleanup_cpu_memory()
            after_objects = len(gc.get_objects())
            results['cpu_objects'] = before_objects - after_objects
        
        return results
    
    def register_cleanup_callback(self, callback: Callable) -> None:
        """æ³¨å†Œæ¸…ç†å›è°ƒå‡½æ•°"""
        self.cleanup_callbacks.append(callback)
    
    def register_warning_callback(self, callback: Callable) -> None:
        """æ³¨å†Œè­¦å‘Šå›è°ƒå‡½æ•°"""
        self.warning_callbacks.append(callback)
    
    def start_monitoring(self) -> None:
        """å¯åŠ¨å†…å­˜ç›‘æ§çº¿ç¨‹"""
        if not self.enable_monitoring or self.monitoring_thread is not None:
            return
        
        def monitor_loop():
            while not self.stop_monitoring.wait(self.auto_cleanup_interval):
                try:
                    stats = self.get_memory_stats()
                    self.memory_history.append(stats)
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
                    if (stats.gpu_memory_percent > self.gpu_memory_threshold or 
                        stats.cpu_memory_percent > self.cpu_memory_threshold):
                        self.smart_cleanup()
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘å‡ºè­¦å‘Š
                    if stats.gpu_memory_percent > 0.95:
                        for callback in self.warning_callbacks:
                            try:
                                callback(f"GPUå†…å­˜ä¸¥é‡ä¸è¶³: {stats.gpu_memory_percent*100:.1f}%")
                            except Exception as e:
                                print(f"è­¦å‘Šå›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
                
                except Exception as e:
                    print(f"å†…å­˜ç›‘æ§é”™è¯¯: {e}")
        
        self.monitoring_thread = threading.Thread(target=monitor_loop, daemon=True)
        self.monitoring_thread.start()
        print("ğŸ“Š å†…å­˜ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring_thread(self) -> None:
        """åœæ­¢å†…å­˜ç›‘æ§çº¿ç¨‹"""
        if self.monitoring_thread is not None:
            self.stop_monitoring.set()
            self.monitoring_thread.join(timeout=5.0)
            self.monitoring_thread = None
            print("ğŸ“Š å†…å­˜ç›‘æ§å·²åœæ­¢")
    
    def print_memory_report(self) -> None:
        """æ‰“å°è¯¦ç»†å†…å­˜æŠ¥å‘Š"""
        stats = self.get_memory_stats()
        
        print("\n" + "="*50)
        print("ğŸ“Š å†…å­˜ä½¿ç”¨æŠ¥å‘Š")
        print("="*50)
        
        # CPUä¿¡æ¯
        print(f"ğŸ–¥ï¸ CPU:")
        print(f"   ä½¿ç”¨ç‡: {stats.cpu_percent:.1f}%")
        print(f"   å†…å­˜: {stats.cpu_memory_gb:.2f}GB / {self.cpu_total_memory:.2f}GB ({stats.cpu_memory_percent*100:.1f}%)")
        
        # GPUä¿¡æ¯
        if torch.cuda.is_available():
            print(f"ğŸ® GPU:")
            print(f"   å·²åˆ†é…: {stats.gpu_memory_allocated_gb:.2f}GB")
            print(f"   å·²ç¼“å­˜: {stats.gpu_memory_cached_gb:.2f}GB")
            print(f"   å·²ä¿ç•™: {stats.gpu_memory_reserved_gb:.2f}GB / {self.gpu_total_memory:.2f}GB ({stats.gpu_memory_percent*100:.1f}%)")
        
        # å†å²ç»Ÿè®¡
        if len(self.memory_history) > 1:
            gpu_usage_history = [s.gpu_memory_percent for s in self.memory_history]
            cpu_usage_history = [s.cpu_memory_percent for s in self.memory_history]
            
            print(f"ğŸ“ˆ å†å²ç»Ÿè®¡ (æœ€è¿‘{len(self.memory_history)}æ¬¡):")
            print(f"   GPUå¹³å‡ä½¿ç”¨ç‡: {sum(gpu_usage_history)/len(gpu_usage_history)*100:.1f}%")
            print(f"   GPUå³°å€¼ä½¿ç”¨ç‡: {max(gpu_usage_history)*100:.1f}%")
            print(f"   CPUå¹³å‡ä½¿ç”¨ç‡: {sum(cpu_usage_history)/len(cpu_usage_history)*100:.1f}%")
            print(f"   CPUå³°å€¼ä½¿ç”¨ç‡: {max(cpu_usage_history)*100:.1f}%")
        
        print("="*50)
    
    def get_optimization_suggestions(self) -> List[str]:
        """è·å–å†…å­˜ä¼˜åŒ–å»ºè®®"""
        stats = self.get_memory_stats()
        suggestions = []
        
        # GPUä¼˜åŒ–å»ºè®®
        if stats.gpu_memory_percent > 0.9:
            suggestions.append("ğŸ”´ GPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å‡å°batch_size")
        elif stats.gpu_memory_percent > 0.8:
            suggestions.append("ğŸŸ¡ GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®å¯ç”¨æ¢¯åº¦ç´¯ç§¯")
        
        if stats.gpu_memory_cached_gb > stats.gpu_memory_allocated_gb * 1.5:
            suggestions.append("ğŸ”µ GPUç¼“å­˜è¿‡å¤šï¼Œå»ºè®®å®šæœŸè°ƒç”¨torch.cuda.empty_cache()")
        
        # CPUä¼˜åŒ–å»ºè®®
        if stats.cpu_memory_percent > 0.9:
            suggestions.append("ğŸ”´ CPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å‡å°‘num_workers")
        elif stats.cpu_memory_percent > 0.8:
            suggestions.append("ğŸŸ¡ CPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®ç¦ç”¨pin_memory")
        
        # æ•°æ®åŠ è½½ä¼˜åŒ–å»ºè®®
        if len(self.memory_history) > 10:
            recent_gpu_usage = [s.gpu_memory_percent for s in list(self.memory_history)[-10:]]
            if max(recent_gpu_usage) - min(recent_gpu_usage) > 0.3:
                suggestions.append("ğŸ”µ GPUå†…å­˜ä½¿ç”¨æ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®é¢„å¤„ç†")
        
        if not suggestions:
            suggestions.append("âœ… å†…å­˜ä½¿ç”¨çŠ¶å†µè‰¯å¥½")
        
        return suggestions
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.start_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.stop_monitoring_thread()
        self.cleanup_gpu_memory(force=True)
        self.cleanup_cpu_memory()

# å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹
_global_memory_manager: Optional[MemoryManager] = None

def get_memory_manager() -> MemoryManager:
    """è·å–å…¨å±€å†…å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _global_memory_manager
    if _global_memory_manager is None:
        _global_memory_manager = MemoryManager()
    return _global_memory_manager

def cleanup_memory(force: bool = False) -> Dict[str, float]:
    """å¿«é€Ÿå†…å­˜æ¸…ç†å‡½æ•°"""
    manager = get_memory_manager()
    return manager.smart_cleanup() if not force else {
        'gpu_freed': manager.cleanup_gpu_memory(force=True),
        'cpu_objects': 0
    }

def print_memory_info() -> None:
    """å¿«é€Ÿæ‰“å°å†…å­˜ä¿¡æ¯"""
    manager = get_memory_manager()
    manager.print_memory_report()

def get_memory_suggestions() -> List[str]:
    """å¿«é€Ÿè·å–å†…å­˜ä¼˜åŒ–å»ºè®®"""
    manager = get_memory_manager()
    return manager.get_optimization_suggestions()

# è£…é¥°å™¨ï¼šè‡ªåŠ¨å†…å­˜ç®¡ç†
def auto_memory_management(cleanup_interval: int = 50):
    """è‡ªåŠ¨å†…å­˜ç®¡ç†è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            manager = get_memory_manager()
            
            # æ‰§è¡Œå‰æ£€æŸ¥
            if hasattr(wrapper, '_call_count'):
                wrapper._call_count += 1
            else:
                wrapper._call_count = 1
            
            try:
                result = func(*args, **kwargs)
                
                # å®šæœŸæ¸…ç†
                if wrapper._call_count % cleanup_interval == 0:
                    manager.smart_cleanup()
                
                return result
            
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"ğŸš¨ æ£€æµ‹åˆ°å†…å­˜ä¸è¶³é”™è¯¯ï¼Œå°è¯•æ¸…ç†å†…å­˜...")
                    manager.cleanup_gpu_memory(force=True)
                    manager.cleanup_cpu_memory()
                    print(f"ğŸ’¡ å»ºè®®: {'; '.join(manager.get_optimization_suggestions())}")
                raise
        
        return wrapper
    return decorator