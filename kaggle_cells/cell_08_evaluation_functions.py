# Cell 8: è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–

def evaluate_model_optimized(model, test_loader, criterion, device):
    """ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°å‡½æ•°"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_targets = []
    all_scores = []
    
    inference_times = []
    
    print("ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(tqdm(test_loader, desc="è¯„ä¼°è¿›åº¦")):
            data, target = data.to(device), target.to(device)
            
            # è®°å½•æ¨ç†æ—¶é—´
            start_time = time.time()
            output, attention_weights = model(data)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output, target)
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœ (åº”ç”¨ sigmoid è·å¾—æ¦‚ç‡)
            probs = torch.sigmoid(output)
            predictions = (probs > 0.5).float()
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_scores.extend(probs.cpu().numpy())
    
    avg_loss = total_loss / len(test_loader)
    avg_inference_time = np.mean(inference_times)
    total_inference_time = np.sum(inference_times)
    
    print(f"âœ… è¯„ä¼°å®Œæˆ")
    print(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time*1000:.2f} ms/batch")
    
    return {
        'loss': avg_loss,
        'predictions': np.array(all_predictions),
        'targets': np.array(all_targets),
        'scores': np.array(all_scores),
        'avg_inference_time': avg_inference_time,
        'total_inference_time': total_inference_time
    }

def calculate_comprehensive_metrics(predictions, targets, scores):
    """è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…å«ç±»åˆ«ä¸å¹³è¡¡åˆ†æ"""
    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(targets, predictions)
    balanced_acc = balanced_accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, zero_division=0)
    recall = recall_score(targets, predictions, zero_division=0)
    f1 = f1_score(targets, predictions, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(targets, predictions)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
    
    # ç‰¹å¼‚æ€§å’Œè´Ÿé¢„æµ‹å€¼
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0
    
    # ç±»åˆ«ç‰¹å®šæŒ‡æ ‡
    real_total = np.sum(targets == 0)
    fake_total = np.sum(targets == 1)
    real_correct = tn  # çœŸå®è§†é¢‘æ­£ç¡®é¢„æµ‹ä¸ºçœŸå®
    fake_correct = tp  # ä¼ªé€ è§†é¢‘æ­£ç¡®é¢„æµ‹ä¸ºä¼ªé€ 
    
    real_accuracy = real_correct / real_total if real_total > 0 else 0
    fake_accuracy = fake_correct / fake_total if fake_total > 0 else 0
    
    # ç±»åˆ«ä¸å¹³è¡¡åˆ†æ
    class_distribution = {
        'real_samples': int(real_total),
        'fake_samples': int(fake_total),
        'imbalance_ratio': fake_total / real_total if real_total > 0 else float('inf')
    }
    
    # AUCæŒ‡æ ‡
    try:
        auc_roc = roc_auc_score(targets, scores)
    except:
        auc_roc = 0.0
    
    try:
        precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)
        auc_pr = auc(recall_curve, precision_curve)
    except:
        auc_pr = 0.0
    
    return {
        'accuracy': accuracy,
        'balanced_accuracy': balanced_acc,
        'precision': precision,
        'recall': recall,
        'specificity': specificity,
        'f1': f1,
        'auc_roc': auc_roc,
        'auc_pr': auc_pr,
        'npv': npv,
        'confusion_matrix': cm,
        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,
        'real_accuracy': real_accuracy,
        'fake_accuracy': fake_accuracy,
        'class_distribution': class_distribution
    }

def plot_enhanced_confusion_matrix(cm, save_path):
    """ç»˜åˆ¶å¢å¼ºçš„æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(10, 8))
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ ‡ç­¾
    labels = np.array([[
        f'{cm[i,j]}\n({cm_percent[i,j]:.1f}%)' 
        for j in range(cm.shape[1])
    ] for i in range(cm.shape[0])])
    
    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', 
                xticklabels=['çœŸå®', 'ä¼ªé€ '],
                yticklabels=['çœŸå®', 'ä¼ªé€ '],
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    
    plt.title('å¢å¼ºæ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    stats_text = f'å‡†ç¡®ç‡: {accuracy:.3f}\nç²¾ç¡®ç‡: {precision:.3f}\nå¬å›ç‡: {recall:.3f}\nF1åˆ†æ•°: {f1:.3f}'
    plt.text(2.1, 0.5, stats_text, fontsize=10, 
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")

def plot_roc_pr_curves(targets, scores, save_path):
    """ç»˜åˆ¶ROCå’ŒPRæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(targets, scores)
    roc_auc = auc(fpr, tpr)
    
    ax1.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('å‡æ­£ç‡')
    ax1.set_ylabel('çœŸæ­£ç‡')
    ax1.set_title('ROCæ›²çº¿')
    ax1.legend(loc='lower right')
    ax1.grid(True, alpha=0.3)
    
    # PRæ›²çº¿
    precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)
    pr_auc = auc(recall_curve, precision_curve)
    
    ax2.plot(recall_curve, precision_curve, color='darkgreen', lw=2,
             label=f'PRæ›²çº¿ (AUC = {pr_auc:.4f})')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('å¬å›ç‡')
    ax2.set_ylabel('ç²¾ç¡®ç‡')
    ax2.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
    ax2.legend(loc='lower left')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ROC/PRæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")

def generate_class_imbalance_report(metrics):
    """ç”Ÿæˆè¯¦ç»†çš„ç±»åˆ«ä¸å¹³è¡¡åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“Š ç±»åˆ«ä¸å¹³è¡¡åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # æ•°æ®åˆ†å¸ƒ
    dist = metrics['class_distribution']
    print(f"\nğŸ“ˆ æ•°æ®åˆ†å¸ƒ:")
    print(f"  çœŸå®è§†é¢‘æ ·æœ¬: {dist['real_samples']}")
    print(f"  ä¼ªé€ è§†é¢‘æ ·æœ¬: {dist['fake_samples']}")
    print(f"  ä¸å¹³è¡¡æ¯”ä¾‹: {dist['imbalance_ratio']:.2f}:1 (ä¼ªé€ :çœŸå®)")
    
    # ç±»åˆ«ç‰¹å®šæ€§èƒ½
    print(f"\nğŸ¯ ç±»åˆ«ç‰¹å®šå‡†ç¡®ç‡:")
    print(f"  çœŸå®è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {metrics['real_accuracy']*100:.2f}%")
    print(f"  ä¼ªé€ è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {metrics['fake_accuracy']*100:.2f}%")
    
    # æ··æ·†çŸ©é˜µåˆ†æ
    tn, fp, fn, tp = metrics['tn'], metrics['fp'], metrics['fn'], metrics['tp']
    print(f"\nğŸ“‹ æ··æ·†çŸ©é˜µåˆ†æ:")
    print(f"  çœŸè´Ÿä¾‹ (TN): {tn} - æ­£ç¡®è¯†åˆ«çš„çœŸå®è§†é¢‘")
    print(f"  å‡æ­£ä¾‹ (FP): {fp} - è¯¯åˆ¤ä¸ºä¼ªé€ çš„çœŸå®è§†é¢‘")
    print(f"  å‡è´Ÿä¾‹ (FN): {fn} - è¯¯åˆ¤ä¸ºçœŸå®çš„ä¼ªé€ è§†é¢‘")
    print(f"  çœŸæ­£ä¾‹ (TP): {tp} - æ­£ç¡®è¯†åˆ«çš„ä¼ªé€ è§†é¢‘")
    
    # åå‘æ€§åˆ†æ
    total_predictions = tn + fp + fn + tp
    predicted_real = tn + fn
    predicted_fake = fp + tp
    
    print(f"\nâš–ï¸ æ¨¡å‹åå‘æ€§åˆ†æ:")
    print(f"  é¢„æµ‹ä¸ºçœŸå®çš„æ ·æœ¬: {predicted_real} ({predicted_real/total_predictions*100:.1f}%)")
    print(f"  é¢„æµ‹ä¸ºä¼ªé€ çš„æ ·æœ¬: {predicted_fake} ({predicted_fake/total_predictions*100:.1f}%)")
    
    # é—®é¢˜è¯Šæ–­
    print(f"\nğŸ” é—®é¢˜è¯Šæ–­:")
    if metrics['real_accuracy'] < 0.1:
        print("  âŒ ä¸¥é‡é—®é¢˜: æ¨¡å‹å‡ ä¹æ— æ³•è¯†åˆ«çœŸå®è§†é¢‘")
    elif metrics['real_accuracy'] < 0.5:
        print("  âš ï¸  é—®é¢˜: çœŸå®è§†é¢‘è¯†åˆ«èƒ½åŠ›è¾ƒå·®")
    else:
        print("  âœ… çœŸå®è§†é¢‘è¯†åˆ«èƒ½åŠ›æ­£å¸¸")
        
    if metrics['fake_accuracy'] > 0.9 and metrics['real_accuracy'] < 0.1:
        print("  âŒ ä¸¥é‡åå‘: æ¨¡å‹è¿‡åº¦åå‘é¢„æµ‹ä¼ªé€ è§†é¢‘")
    
    if metrics['auc_roc'] < 0.6:
        print("  âŒ AUC-ROCè¿‡ä½: æ¨¡å‹åˆ¤åˆ«èƒ½åŠ›æ¥è¿‘éšæœºçŒœæµ‹")
    
    # æ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    if dist['imbalance_ratio'] > 3.0:
        print("  1. å¢åŠ çœŸå®è§†é¢‘æ ·æœ¬æˆ–å‡å°‘ä¼ªé€ è§†é¢‘æ ·æœ¬")
        print("  2. ä½¿ç”¨æ›´å¼ºçš„ç±»åˆ«æƒé‡ (pos_weight > 3.0)")
        print("  3. è°ƒæ•´Focal Losså‚æ•° (é™ä½alpha, å¢åŠ gamma)")
    
    if metrics['real_accuracy'] < 0.3:
        print("  4. æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œç¡®ä¿çœŸå®è§†é¢‘æ ‡ç­¾æ­£ç¡®")
        print("  5. ä½¿ç”¨æˆæœ¬æ•æ„Ÿå­¦ä¹ æ–¹æ³•")
        print("  6. è€ƒè™‘ä½¿ç”¨SMOTEç­‰è¿‡é‡‡æ ·æŠ€æœ¯")
    
    if metrics['auc_roc'] < 0.6:
        print("  7. é‡æ–°è®¾è®¡æ¨¡å‹æ¶æ„")
        print("  8. å¢åŠ æ¨¡å‹å¤æ‚åº¦æˆ–ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹")
        print("  9. æ£€æŸ¥ç‰¹å¾æå–æ˜¯å¦æœ‰æ•ˆ")
    
    print("="*60)

print("âœ… è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–å®šä¹‰å®Œæˆ")