# Cell 12: è®­ç»ƒå¾ªç¯ - GPUä¼˜åŒ–ç‰ˆæœ¬

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")

# è®­ç»ƒå†å²è®°å½•
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []
best_val_loss = float('inf')
best_model_state = None

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    print(f"\n{'='*50}")
    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"{'='*50}")
    
    # è®­ç»ƒé˜¶æ®µ - ä½¿ç”¨æ··åˆç²¾åº¦
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, scaler)
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    
    # éªŒè¯é˜¶æ®µ - ä½¿ç”¨æ··åˆç²¾åº¦
    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device, scaler)
    val_losses.append(val_loss)
    val_accuracies.append(val_acc)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)
    current_lr = optimizer.param_groups[0]['lr']
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
    print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
    print(f"éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f}")
    print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
    
    # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        memory_reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"GPUå†…å­˜: {memory_allocated:.1f}GB / {memory_reserved:.1f}GB")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model_state = model.state_dict().copy()
        print(f"ğŸ¯ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    
    # æ—©åœæ£€æŸ¥
    if early_stopping(val_loss):
        print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
        break
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("\nâœ… è®­ç»ƒå®Œæˆ!")