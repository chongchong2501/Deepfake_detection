# Cell 5: æ¨¡å‹å®šä¹‰ - é›†æˆå¤šæ¨¡æ€ç‰¹å¾å’ŒEnsembleç­–ç•¥
class OptimizedDeepfakeDetector(nn.Module):
    """ä¼˜åŒ–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹å™¨ - é›†æˆå¤šæ¨¡æ€ç‰¹å¾å’ŒEnsembleç­–ç•¥"""
    
    def __init__(self, num_classes=1, dropout_rate=0.3, use_attention=True, 
                 use_multimodal=False, ensemble_mode=False):
        super(OptimizedDeepfakeDetector, self).__init__()
        
        self.use_attention = use_attention
        self.use_multimodal = use_multimodal
        self.ensemble_mode = ensemble_mode
        
        # ä¸»å¹²ç½‘ç»œ - ResNet50
        self.backbone = models.resnet50(pretrained=True)
        backbone_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()  # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
        
        # æ—¶åºç‰¹å¾æå–
        self.temporal_conv = nn.Sequential(
            nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool3d((1, 7, 7))
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        if use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=backbone_features, 
                num_heads=8, 
                dropout=dropout_rate,
                batch_first=True
            )
            self.attention_norm = nn.LayerNorm(backbone_features)
        
        # å¤šæ¨¡æ€ç‰¹å¾èåˆ
        if use_multimodal:
            # é¢‘åŸŸç‰¹å¾å¤„ç† - ä¿®æ­£è¾“å…¥ç»´åº¦
            self.fourier_fc = nn.Sequential(
                nn.Linear(5, 256),  # é¢‘åŸŸç‰¹å¾å®é™…ç»´åº¦ä¸º5 (mean, std, max, energy, entropy)
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(256, 128)
            )
            
            # å‹ç¼©ä¼ªå½±ç‰¹å¾å¤„ç† - ä¿®æ­£è¾“å…¥ç»´åº¦
            self.compression_fc = nn.Sequential(
                nn.Linear(32, 64),  # å‹ç¼©ç‰¹å¾æ‰©å±•ä¸º32ç»´
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(64, 32)
            )
            
            # æ—¶åºä¸€è‡´æ€§ç‰¹å¾å¤„ç†
            self.temporal_fc = nn.Sequential(
                nn.Linear(4, 64),  # æ—¶åºç‰¹å¾ç»´åº¦ä¸º4
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(64, 32)
            )
            
            # ç‰¹å¾èåˆå±‚ - åŠ¨æ€è®¡ç®—è¾“å…¥ç»´åº¦
            # åŸºç¡€ç‰¹å¾: backbone_features (2048)
            # é¢‘åŸŸç‰¹å¾: 128 (fourier_fcè¾“å‡º)
            # å‹ç¼©ç‰¹å¾: 32 (compression_fcè¾“å‡º)  
            # æ—¶åºç‰¹å¾: 32 (temporal_fcè¾“å‡º)
            fusion_dim = backbone_features + 128 + 32 + 32  # 2048 + 128 + 32 + 32 = 2240
            self.fusion_layer = nn.Sequential(
                nn.Linear(fusion_dim, 512),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(512, 256)
            )
            final_features = 256
        else:
            final_features = backbone_features
        
        # é›†æˆæ¨¡å¼çš„å¤šä¸ªåˆ†ç±»å¤´
        if ensemble_mode:
            # ä¸»åˆ†ç±»å™¨
            self.main_classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(final_features, 128),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(128, num_classes)
            )
            
            # è¾…åŠ©åˆ†ç±»å™¨1 - ä¸“æ³¨äºç©ºé—´ç‰¹å¾
            self.spatial_classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(final_features, 64),
                nn.ReLU(inplace=True),
                nn.Linear(64, num_classes)
            )
            
            # è¾…åŠ©åˆ†ç±»å™¨2 - ä¸“æ³¨äºæ—¶åºç‰¹å¾
            self.temporal_classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(final_features, 64),
                nn.ReLU(inplace=True),
                nn.Linear(64, num_classes)
            )
            
            # é›†æˆæƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
            self.ensemble_weights = nn.Parameter(torch.ones(3) / 3)
            
        else:
            # å•ä¸€åˆ†ç±»å™¨
            self.classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(final_features, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(256, 128),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(128, num_classes)
            )
            
            # æ·»åŠ å•ä¸€åˆ†ç±»å™¨ç”¨äºå¤„ç†åŸºç¡€ç‰¹å¾ï¼ˆå½“å¤šæ¨¡æ€ç‰¹å¾å¤„ç†å¤±è´¥æ—¶ï¼‰
            self.single_classifier = nn.Sequential(
                nn.Dropout(dropout_rate),
                nn.Linear(backbone_features, 128),  # ç›´æ¥å¤„ç†backboneç‰¹å¾
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate),
                nn.Linear(128, num_classes)
            )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   - æ³¨æ„åŠ›æœºåˆ¶: {'å¯ç”¨' if use_attention else 'ç¦ç”¨'}")
        print(f"   - å¤šæ¨¡æ€èåˆ: {'å¯ç”¨' if use_multimodal else 'ç¦ç”¨'}")
        print(f"   - é›†æˆæ¨¡å¼: {'å¯ç”¨' if ensemble_mode else 'ç¦ç”¨'}")

    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

    def forward(self, x, additional_features=None):
        """
        å‰å‘ä¼ æ’­
        Args:
            x: è§†é¢‘å¼ é‡ (B, T, C, H, W)
            additional_features: é¢å¤–ç‰¹å¾å­—å…¸
        """
        batch_size, num_frames, channels, height, width = x.shape
        
        # æå–æ¯å¸§çš„ç©ºé—´ç‰¹å¾
        x_reshaped = x.view(batch_size * num_frames, channels, height, width)
        spatial_features = self.backbone(x_reshaped)  # (B*T, features)
        spatial_features = spatial_features.view(batch_size, num_frames, -1)  # (B, T, features)
        
        # æ—¶åºç‰¹å¾èšåˆ
        if self.use_attention:
            # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆæ—¶åºç‰¹å¾
            attended_features, attention_weights = self.attention(
                spatial_features, spatial_features, spatial_features
            )
            attended_features = self.attention_norm(attended_features + spatial_features)
            # å…¨å±€å¹³å‡æ± åŒ–
            temporal_features = torch.mean(attended_features, dim=1)  # (B, features)
        else:
            # ç®€å•å¹³å‡æ± åŒ–
            temporal_features = torch.mean(spatial_features, dim=1)  # (B, features)
        
        # å¤šæ¨¡æ€ç‰¹å¾èåˆ
        if self.use_multimodal and additional_features is not None:
            fusion_features = [temporal_features]
            
            # å¤„ç†é¢‘åŸŸç‰¹å¾
            if 'fourier' in additional_features:
                try:
                    fourier_feat = additional_features['fourier']
                    if isinstance(fourier_feat, dict):
                        # å®‰å…¨åœ°æå–æ•°å€¼ç‰¹å¾
                        fourier_values = []
                        for value in fourier_feat.values():
                            if isinstance(value, (int, float)):
                                fourier_values.append(float(value))
                            elif isinstance(value, torch.Tensor):
                                if value.numel() == 1:
                                    fourier_values.append(float(value.item()))
                                else:
                                    fourier_values.append(float(value.mean().item()))
                            elif isinstance(value, np.ndarray):
                                if value.size == 1:
                                    fourier_values.append(float(value.item()))
                                else:
                                    fourier_values.append(float(value.mean()))
                            else:
                                fourier_values.append(0.0)  # é»˜è®¤å€¼
                        
                        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç‰¹å¾ç»´åº¦
                        if len(fourier_values) < 5:  # fourier_fcæœŸæœ›5ç»´è¾“å…¥
                            fourier_values.extend([0.0] * (5 - len(fourier_values)))
                        elif len(fourier_values) > 5:
                            fourier_values = fourier_values[:5]
                        
                        fourier_tensor = torch.tensor([fourier_values] * batch_size, 
                                                    dtype=torch.float32, 
                                                    device=temporal_features.device)
                    else:
                        # å¦‚æœå·²ç»æ˜¯å¼ é‡ï¼Œç¡®ä¿æ­£ç¡®çš„å½¢çŠ¶
                        if isinstance(fourier_feat, torch.Tensor):
                            fourier_tensor = fourier_feat.to(temporal_features.device)
                            if fourier_tensor.dim() == 1:
                                fourier_tensor = fourier_tensor.unsqueeze(0).repeat(batch_size, 1)
                        else:
                            # åˆ›å»ºé»˜è®¤å¼ é‡
                            fourier_tensor = torch.zeros(batch_size, 5, 
                                                        dtype=torch.float32, 
                                                        device=temporal_features.device)
                    
                    fourier_processed = self.fourier_fc(fourier_tensor)
                    fusion_features.append(fourier_processed)
                except Exception as e:
                    print(f"âš ï¸ é¢‘åŸŸç‰¹å¾å¤„ç†å¤±è´¥: {e}")
                    # ä½¿ç”¨é»˜è®¤ç‰¹å¾
                    fourier_tensor = torch.zeros(batch_size, 5, 
                                                dtype=torch.float32, 
                                                device=temporal_features.device)
                    fourier_processed = self.fourier_fc(fourier_tensor)
                    fusion_features.append(fourier_processed)
            
            # å¤„ç†å‹ç¼©ä¼ªå½±ç‰¹å¾
            if 'compression' in additional_features:
                try:
                    comp_feat = additional_features['compression']
                    if isinstance(comp_feat, dict):
                        # å®‰å…¨åœ°æå–å‹ç¼©ç‰¹å¾ - ä¿®æ­£ä¸º5ä¸ªç‰¹å¾
                        comp_values = []
                        for key in ['dct_mean', 'dct_std', 'dct_energy', 'high_freq_energy', 'edge_density']:
                            if key in comp_feat:
                                value = comp_feat[key]
                                if isinstance(value, (int, float)):
                                    comp_values.append(float(value))
                                elif isinstance(value, torch.Tensor):
                                    comp_values.append(float(value.item() if value.numel() == 1 else value.mean().item()))
                                elif isinstance(value, np.ndarray):
                                    comp_values.append(float(value.item() if value.size == 1 else value.mean()))
                                else:
                                    comp_values.append(0.0)
                            else:
                                comp_values.append(0.0)
                        
                        # æ‰©å±•åˆ°32ç»´ï¼šé‡å¤åŸºç¡€ç‰¹å¾å¹¶æ·»åŠ æ´¾ç”Ÿç‰¹å¾
                        extended_values = comp_values.copy()
                        # æ·»åŠ æ´¾ç”Ÿç‰¹å¾
                        extended_values.extend([
                            comp_values[0] * comp_values[1],  # mean * std
                            comp_values[2] / (comp_values[3] + 1e-8),  # energy ratio
                            comp_values[4] * comp_values[0],  # edge * mean
                            np.sqrt(abs(comp_values[2])),  # sqrt energy
                            comp_values[1] / (comp_values[0] + 1e-8),  # std/mean ratio
                        ])
                        # é‡å¤å¡«å……åˆ°32ç»´
                        while len(extended_values) < 32:
                            extended_values.extend(comp_values[:min(5, 32 - len(extended_values))])
                        
                        comp_tensor = torch.tensor([extended_values[:32]] * batch_size, 
                                                 dtype=torch.float32, 
                                                 device=temporal_features.device)
                    else:
                        if isinstance(comp_feat, torch.Tensor):
                            comp_tensor = comp_feat.to(temporal_features.device)
                            if comp_tensor.dim() == 1:
                                comp_tensor = comp_tensor.unsqueeze(0).repeat(batch_size, 1)
                            # ç¡®ä¿æ˜¯32ç»´
                            if comp_tensor.size(-1) < 32:
                                padding = torch.zeros(batch_size, 32 - comp_tensor.size(-1), 
                                                    dtype=torch.float32, 
                                                    device=temporal_features.device)
                                comp_tensor = torch.cat([comp_tensor, padding], dim=-1)
                            elif comp_tensor.size(-1) > 32:
                                comp_tensor = comp_tensor[:, :32]
                        else:
                            comp_tensor = torch.zeros(batch_size, 32, 
                                                    dtype=torch.float32, 
                                                    device=temporal_features.device)
                    
                    comp_processed = self.compression_fc(comp_tensor)
                    fusion_features.append(comp_processed)
                except Exception as e:
                    print(f"âš ï¸ å‹ç¼©ç‰¹å¾å¤„ç†å¤±è´¥: {e}")
                    comp_tensor = torch.zeros(batch_size, 32, 
                                            dtype=torch.float32, 
                                            device=temporal_features.device)
                    comp_processed = self.compression_fc(comp_tensor)
                    fusion_features.append(comp_processed)
            
            # å¤„ç†æ—¶åºä¸€è‡´æ€§ç‰¹å¾
            if 'temporal' in additional_features:
                try:
                    temp_feat = additional_features['temporal']
                    if isinstance(temp_feat, dict):
                        # å®‰å…¨åœ°æå–æ—¶åºç‰¹å¾
                        temp_values = []
                        for key in ['mean_frame_diff', 'std_frame_diff', 'max_frame_diff', 'temporal_smoothness']:
                            if key in temp_feat:
                                value = temp_feat[key]
                                if isinstance(value, (int, float)):
                                    temp_values.append(float(value))
                                elif isinstance(value, torch.Tensor):
                                    temp_values.append(float(value.item() if value.numel() == 1 else value.mean().item()))
                                elif isinstance(value, np.ndarray):
                                    temp_values.append(float(value.item() if value.size == 1 else value.mean()))
                                else:
                                    temp_values.append(0.0)
                            else:
                                temp_values.append(0.0)
                        
                        temp_tensor = torch.tensor([temp_values] * batch_size, 
                                                 dtype=torch.float32, 
                                                 device=temporal_features.device)
                    else:
                        if isinstance(temp_feat, torch.Tensor):
                            temp_tensor = temp_feat.to(temporal_features.device)
                            if temp_tensor.dim() == 1:
                                temp_tensor = temp_tensor.unsqueeze(0).repeat(batch_size, 1)
                        else:
                            temp_tensor = torch.zeros(batch_size, 4, 
                                                    dtype=torch.float32, 
                                                    device=temporal_features.device)
                    
                    temp_processed = self.temporal_fc(temp_tensor)
                    fusion_features.append(temp_processed)
                except Exception as e:
                    print(f"âš ï¸ æ—¶åºç‰¹å¾å¤„ç†å¤±è´¥: {e}")
                    temp_tensor = torch.zeros(batch_size, 4, 
                                            dtype=torch.float32, 
                                            device=temporal_features.device)
                    temp_processed = self.temporal_fc(temp_tensor)
                    fusion_features.append(temp_processed)
            
            # ç‰¹å¾èåˆ - ç¡®ä¿ç»´åº¦ä¸€è‡´æ€§
            if len(fusion_features) > 1:
                try:
                    # æ£€æŸ¥æ¯ä¸ªç‰¹å¾çš„ç»´åº¦
                    feature_dims = [f.shape[1] for f in fusion_features]
                    total_dim = sum(feature_dims)
                    expected_dim = self.fusion_layer[0].in_features
                    
                    if total_dim == expected_dim:
                        # ç»´åº¦åŒ¹é…ï¼Œç›´æ¥èåˆ
                        fused_features = torch.cat(fusion_features, dim=1)
                        final_features = self.fusion_layer(fused_features)
                    else:
                        # ç»´åº¦ä¸åŒ¹é…æ—¶è¿›è¡Œè°ƒæ•´ï¼ˆè¿™æ˜¯æ­£å¸¸çš„å¤šæ¨¡æ€ç‰¹å¾å¤„ç†ï¼‰
                        if total_dim < expected_dim:
                            # ç»´åº¦ä¸è¶³ï¼Œç”¨é›¶å¡«å……
                            padding_dim = expected_dim - total_dim
                            fused_features = torch.cat(fusion_features, dim=1)
                            padding = torch.zeros(batch_size, padding_dim, 
                                                dtype=fused_features.dtype, 
                                                device=fused_features.device)
                            fused_features = torch.cat([fused_features, padding], dim=1)
                            final_features = self.fusion_layer(fused_features)
                            # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                            if hasattr(self, 'debug_mode') and self.debug_mode:
                                print(f"ğŸ”§ ç‰¹å¾å¡«å……: {total_dim} -> {expected_dim}")
                        elif total_dim > expected_dim:
                            # ç»´åº¦è¿‡å¤šï¼Œæˆªæ–­åˆ°æœŸæœ›ç»´åº¦
                            fused_features = torch.cat(fusion_features, dim=1)
                            fused_features = fused_features[:, :expected_dim]
                            final_features = self.fusion_layer(fused_features)
                            # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¾“å‡ºè¯¦ç»†ä¿¡æ¯
                            if hasattr(self, 'debug_mode') and self.debug_mode:
                                print(f"ğŸ”§ ç‰¹å¾æˆªæ–­: {total_dim} -> {expected_dim}")
                        else:
                            # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
                            print(f"âš ï¸ ç‰¹å¾èåˆå¼‚å¸¸ï¼Œä½¿ç”¨åŸºç¡€ç‰¹å¾")
                            final_features = temporal_features
                            
                except Exception as e:
                    print(f"âš ï¸ ç‰¹å¾èåˆå¤±è´¥: {e}")
                    final_features = temporal_features
            else:
                final_features = temporal_features
        else:
            final_features = temporal_features
        
        # åˆ†ç±»é¢„æµ‹ - æ ¹æ®ç‰¹å¾ç»´åº¦é€‰æ‹©åˆé€‚çš„åˆ†ç±»å™¨
        if self.ensemble_mode:
            # é›†æˆé¢„æµ‹
            main_pred = self.main_classifier(final_features)
            spatial_pred = self.spatial_classifier(final_features)
            temporal_pred = self.temporal_classifier(final_features)
            
            # åŠ æƒèåˆ
            weights = F.softmax(self.ensemble_weights, dim=0)
            ensemble_pred = (weights[0] * main_pred + 
                           weights[1] * spatial_pred + 
                           weights[2] * temporal_pred)
            
            if self.training:
                # è®­ç»ƒæ—¶è¿”å›æ‰€æœ‰é¢„æµ‹ç”¨äºå¤šä»»åŠ¡å­¦ä¹ 
                return {
                    'main': main_pred,
                    'spatial': spatial_pred,
                    'temporal': temporal_pred,
                    'ensemble': ensemble_pred
                }
            else:
                # æ¨ç†æ—¶åªè¿”å›é›†æˆç»“æœ
                return ensemble_pred
        else:
            # æ£€æŸ¥ç‰¹å¾ç»´åº¦å¹¶é€‰æ‹©åˆé€‚çš„åˆ†ç±»å™¨
            feature_dim = final_features.shape[1]
            
            # è·å–åˆ†ç±»å™¨çš„è¾“å…¥ç»´åº¦
            classifier_input_dim = None
            single_classifier_input_dim = None
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªLinearå±‚æ¥è·å–è¾“å…¥ç»´åº¦
            for layer in self.classifier:
                if isinstance(layer, nn.Linear):
                    classifier_input_dim = layer.in_features
                    break
            
            for layer in self.single_classifier:
                if isinstance(layer, nn.Linear):
                    single_classifier_input_dim = layer.in_features
                    break
            
            # æ ¹æ®ç‰¹å¾ç»´åº¦é€‰æ‹©åˆé€‚çš„åˆ†ç±»å™¨
            if classifier_input_dim and feature_dim == classifier_input_dim:
                logits = self.classifier(final_features)
            elif single_classifier_input_dim and feature_dim == single_classifier_input_dim:
                logits = self.single_classifier(final_features)
            else:
                # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œå°è¯•ä½¿ç”¨å•ä¸€åˆ†ç±»å™¨ï¼ˆé€šå¸¸å¤„ç†åŸºç¡€ç‰¹å¾ï¼‰
                print(f"âš ï¸ ç‰¹å¾ç»´åº¦ {feature_dim} ä¸åŒ¹é…ä»»ä½•åˆ†ç±»å™¨ï¼Œä½¿ç”¨å•ä¸€åˆ†ç±»å™¨")
                logits = self.single_classifier(final_features)
            
            # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaNæˆ–æ— ç©·å€¼
            if torch.isnan(logits).any() or torch.isinf(logits).any():
                print("âš ï¸ æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Infï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤è¾“å‡º")
                # è¿”å›å®‰å…¨çš„é»˜è®¤è¾“å‡ºï¼ˆä¸­æ€§é¢„æµ‹ï¼‰
                batch_size = logits.shape[0]
                device = logits.device
                logits = torch.zeros(batch_size, 1, device=device, dtype=torch.float32)
            
            # é™åˆ¶logitsçš„æ•°å€¼èŒƒå›´ï¼Œé¿å…æç«¯å€¼
            logits = torch.clamp(logits, -10, 10)
            
            return logits

    def get_attention_weights(self, x):
        """è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        if not self.use_attention:
            return None
        
        batch_size, num_frames, channels, height, width = x.shape
        x_reshaped = x.view(batch_size * num_frames, channels, height, width)
        spatial_features = self.backbone(x_reshaped)
        spatial_features = spatial_features.view(batch_size, num_frames, -1)
        
        _, attention_weights = self.attention(
            spatial_features, spatial_features, spatial_features
        )
        
        return attention_weights

    def enable_ensemble_mode(self):
        """å¯ç”¨é›†æˆæ¨¡å¼"""
        self.ensemble_mode = True
        print("ğŸ¯ é›†æˆæ¨¡å¼å·²å¯ç”¨")

    def disable_ensemble_mode(self):
        """ç¦ç”¨é›†æˆæ¨¡å¼"""
        self.ensemble_mode = False
        print("ğŸ¯ é›†æˆæ¨¡å¼å·²ç¦ç”¨")

    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'use_attention': self.use_attention,
            'use_multimodal': self.use_multimodal,
            'ensemble_mode': self.ensemble_mode
        }

def create_ensemble_models(num_models=3, **kwargs):
    """åˆ›å»ºå¤šä¸ªæ¨¡å‹ç”¨äºé›†æˆå­¦ä¹ """
    models = []
    for i in range(num_models):
        # ä¸ºæ¯ä¸ªæ¨¡å‹ä½¿ç”¨ä¸åŒçš„é…ç½®
        model_kwargs = kwargs.copy()
        if i == 0:
            model_kwargs.update({'use_attention': True, 'dropout_rate': 0.3})
        elif i == 1:
            model_kwargs.update({'use_attention': False, 'dropout_rate': 0.4})
        else:
            model_kwargs.update({'use_attention': True, 'dropout_rate': 0.2})
        
        model = OptimizedDeepfakeDetector(**model_kwargs)
        models.append(model)
    
    print(f"âœ… åˆ›å»ºäº† {num_models} ä¸ªé›†æˆæ¨¡å‹")
    return models

print("âœ… ä¼˜åŒ–æ¨¡å‹å®šä¹‰å®Œæˆ")