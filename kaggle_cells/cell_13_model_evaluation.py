# Cell 13: æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æ

print("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
print("=" * 60)

# åŠ è½½æœ€ä½³æ¨¡å‹
print("ğŸ”„ åŠ è½½æœ€ä½³æ¨¡å‹...")
try:
    # ä½¿ç”¨weights_only=Falseæ¥å…¼å®¹æ—§ç‰ˆæœ¬çš„æ¨¡å‹æ–‡ä»¶
    checkpoint = torch.load('./models/best_model.pth', map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    best_epoch = checkpoint['epoch']
    best_val_acc = checkpoint['best_val_acc']
    best_val_auc = checkpoint['best_val_auc']
    
    print(f"âœ… æˆåŠŸåŠ è½½ç¬¬ {best_epoch+1} è½®çš„æœ€ä½³æ¨¡å‹")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")
except Exception as e:
    print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    print("ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œè¯„ä¼°")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
print("\nğŸ” åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
eval_results = evaluate_model_optimized(model, test_loader, criterion, device)

# è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡
print("\nğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
metrics = calculate_comprehensive_metrics(
    eval_results['predictions'], 
    eval_results['targets'], 
    eval_results['scores']
)

# æ‰“å°è¯¦ç»†ç»“æœ
print("\nğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ:")
print("=" * 50)
print(f"æµ‹è¯•æŸå¤±: {eval_results['loss']:.4f}")
print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
print(f"å¹³è¡¡å‡†ç¡®ç‡: {metrics['balanced_accuracy']:.4f} ({metrics['balanced_accuracy']*100:.2f}%)")
print(f"ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
print(f"å¬å›ç‡: {metrics['recall']:.4f}")
print(f"ç‰¹å¼‚æ€§: {metrics['specificity']:.4f}")
print(f"F1åˆ†æ•°: {metrics['f1']:.4f}")
print(f"AUC-ROC: {metrics['auc_roc']:.4f}")
print(f"AUC-PR: {metrics['auc_pr']:.4f}")
print(f"è´Ÿé¢„æµ‹å€¼: {metrics['npv']:.4f}")

# æ··æ·†çŸ©é˜µè¯¦ç»†ä¿¡æ¯
print("\nğŸ” æ··æ·†çŸ©é˜µåˆ†æ:")
print(f"çœŸè´Ÿä¾‹ (TN): {metrics['tn']}")
print(f"å‡æ­£ä¾‹ (FP): {metrics['fp']}")
print(f"å‡è´Ÿä¾‹ (FN): {metrics['fn']}")
print(f"çœŸæ­£ä¾‹ (TP): {metrics['tp']}")

# æ€§èƒ½åˆ†æ
print("\nâš¡ æ€§èƒ½åˆ†æ:")
print(f"å¹³å‡æ¨ç†æ—¶é—´: {eval_results['avg_inference_time']*1000:.2f} ms/batch")
print(f"æ€»æ¨ç†æ—¶é—´: {eval_results['total_inference_time']:.2f} ç§’")
print(f"æ¯ä¸ªæ ·æœ¬æ¨ç†æ—¶é—´: {eval_results['avg_inference_time']*1000/batch_size:.2f} ms")

# è®¡ç®—é¢å¤–æŒ‡æ ‡
total_samples = len(eval_results['targets'])
real_samples = np.sum(eval_results['targets'] == 0)
fake_samples = np.sum(eval_results['targets'] == 1)
real_accuracy = np.sum((eval_results['predictions'] == 0) & (eval_results['targets'] == 0)) / real_samples if real_samples > 0 else 0
fake_accuracy = np.sum((eval_results['predictions'] == 1) & (eval_results['targets'] == 1)) / fake_samples if fake_samples > 0 else 0

print("\nğŸ“‹ ç±»åˆ«ç‰¹å®šåˆ†æ:")
print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
print(f"çœŸå®è§†é¢‘æ ·æœ¬: {real_samples} ({real_samples/total_samples*100:.1f}%)")
print(f"ä¼ªé€ è§†é¢‘æ ·æœ¬: {fake_samples} ({fake_samples/total_samples*100:.1f}%)")
print(f"çœŸå®è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {real_accuracy:.4f} ({real_accuracy*100:.2f}%)")
print(f"ä¼ªé€ è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {fake_accuracy:.4f} ({fake_accuracy*100:.2f}%)")

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
print("\nğŸ“Š ç”Ÿæˆè¯„ä¼°å›¾è¡¨...")

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
os.makedirs('./results/evaluation', exist_ok=True)

# ç»˜åˆ¶å¢å¼ºæ··æ·†çŸ©é˜µ
plot_enhanced_confusion_matrix(
    metrics['confusion_matrix'], 
    './results/evaluation/confusion_matrix.png'
)

# ç»˜åˆ¶ROCå’ŒPRæ›²çº¿
plot_roc_pr_curves(
    eval_results['targets'], 
    eval_results['scores'], 
    './results/evaluation/roc_pr_curves.png'
)

# é¢„æµ‹åˆ†æ•°åˆ†å¸ƒå›¾
plt.figure(figsize=(12, 5))

# çœŸå®è§†é¢‘çš„é¢„æµ‹åˆ†æ•°åˆ†å¸ƒ
plt.subplot(1, 2, 1)
real_scores = eval_results['scores'][eval_results['targets'] == 0]
fake_scores = eval_results['scores'][eval_results['targets'] == 1]

plt.hist(real_scores, bins=30, alpha=0.7, label='çœŸå®è§†é¢‘', color='blue', density=True)
plt.hist(fake_scores, bins=30, alpha=0.7, label='ä¼ªé€ è§†é¢‘', color='red', density=True)
plt.xlabel('é¢„æµ‹åˆ†æ•°')
plt.ylabel('å¯†åº¦')
plt.title('é¢„æµ‹åˆ†æ•°åˆ†å¸ƒ')
plt.legend()
plt.grid(True, alpha=0.3)

# é¢„æµ‹åˆ†æ•°ç®±çº¿å›¾
plt.subplot(1, 2, 2)
scores_data = [real_scores, fake_scores]
labels = ['çœŸå®è§†é¢‘', 'ä¼ªé€ è§†é¢‘']
plt.boxplot(scores_data, labels=labels)
plt.ylabel('é¢„æµ‹åˆ†æ•°')
plt.title('é¢„æµ‹åˆ†æ•°ç®±çº¿å›¾')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('./results/evaluation/score_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… è¯„ä¼°å›¾è¡¨ç”Ÿæˆå®Œæˆ")

# ç”Ÿæˆè¯¦ç»†çš„ç±»åˆ«ä¸å¹³è¡¡åˆ†ææŠ¥å‘Š
generate_class_imbalance_report(metrics)

print("=" * 60)
print("ğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
print("ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° ./results/evaluation/ ç›®å½•")
print("\nğŸ’¡ å¦‚æœå‘ç°ä¸¥é‡çš„ç±»åˆ«åå‘é—®é¢˜ï¼Œè¯·å‚è€ƒä¸Šè¿°æ”¹è¿›å»ºè®®è¿›è¡Œä¼˜åŒ–")