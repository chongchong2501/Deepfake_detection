# Cell 7: è®­ç»ƒå‡½æ•° - é›†æˆå¤šä»»åŠ¡å­¦ä¹ å’Œé«˜çº§ä¼˜åŒ–ç­–ç•¥

import torch
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np

def train_epoch(model, train_loader, criterion, optimizer, device, scheduler=None, 
                use_amp=False, gradient_clip=1.0, ensemble_mode=False):
    """
    è®­ç»ƒä¸€ä¸ªepoch - æ”¯æŒé›†æˆå­¦ä¹ å’Œå¤šä»»åŠ¡å­¦ä¹ 
    
    Args:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è®¾å¤‡
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        gradient_clip: æ¢¯åº¦è£å‰ªé˜ˆå€¼
        ensemble_mode: æ˜¯å¦ä¸ºé›†æˆæ¨¡å¼
    """
    model.train()
    total_loss = 0.0
    total_samples = 0
    correct_predictions = 0
    
    # é›†æˆæ¨¡å¼çš„æŸå¤±ç»Ÿè®¡
    if ensemble_mode:
        ensemble_losses = {
            'main': 0.0,
            'spatial': 0.0,
            'temporal': 0.0,
            'ensemble': 0.0
        }
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    if use_amp:
        scaler = torch.cuda.amp.GradScaler()
    
    progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­", leave=False)
    
    for batch_idx, batch_data in enumerate(progress_bar):
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if len(batch_data) == 3:
            # åŒ…å«é¢å¤–ç‰¹å¾
            videos, labels, additional_features = batch_data
            videos = videos.to(device)
            labels = labels.to(device)
            
            # å¤„ç†é¢å¤–ç‰¹å¾
            if additional_features and isinstance(additional_features, dict):
                for key, value in additional_features.items():
                    if isinstance(value, torch.Tensor):
                        additional_features[key] = value.to(device)
        else:
            # æ ‡å‡†æ ¼å¼
            videos, labels = batch_data
            videos = videos.to(device)
            labels = labels.to(device)
            additional_features = None
        
        optimizer.zero_grad()
        
        try:
            if use_amp:
                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with torch.cuda.amp.autocast():
                    if additional_features is not None:
                        outputs = model(videos, additional_features)
                    else:
                        outputs = model(videos)
                    
                    # è®¡ç®—æŸå¤±
                    if ensemble_mode and isinstance(outputs, dict):
                        # é›†æˆæ¨¡å¼çš„å¤šä»»åŠ¡æŸå¤±
                        losses = {}
                        total_ensemble_loss = 0
                        
                        for key, pred in outputs.items():
                            if pred.dim() > 1:
                                pred = pred.squeeze(-1)
                            loss = criterion(pred, labels)
                            losses[key] = loss
                            
                            # ä¸åŒä»»åŠ¡çš„æƒé‡
                            if key == 'ensemble':
                                weight = 0.5  # é›†æˆé¢„æµ‹æƒé‡æœ€é«˜
                            elif key == 'main':
                                weight = 0.3
                            else:
                                weight = 0.1  # è¾…åŠ©ä»»åŠ¡æƒé‡è¾ƒä½
                            
                            total_ensemble_loss += weight * loss
                        
                        loss = total_ensemble_loss
                        
                        # æ›´æ–°é›†æˆæŸå¤±ç»Ÿè®¡
                        for key, l in losses.items():
                            ensemble_losses[key] += l.item()
                    else:
                        # æ ‡å‡†æ¨¡å¼
                        if outputs.dim() > 1:
                            outputs = outputs.squeeze(-1)
                        loss = criterion(outputs, labels)
                
                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                if gradient_clip > 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                
                scaler.step(optimizer)
                scaler.update()
            else:
                # æ ‡å‡†ç²¾åº¦è®­ç»ƒ
                if additional_features is not None:
                    outputs = model(videos, additional_features)
                else:
                    outputs = model(videos)
                
                # è®¡ç®—æŸå¤±
                if ensemble_mode and isinstance(outputs, dict):
                    # é›†æˆæ¨¡å¼çš„å¤šä»»åŠ¡æŸå¤±
                    losses = {}
                    total_ensemble_loss = 0
                    
                    for key, pred in outputs.items():
                        if pred.dim() > 1:
                            pred = pred.squeeze(-1)
                        loss_item = criterion(pred, labels)
                        losses[key] = loss_item
                        
                        # ä¸åŒä»»åŠ¡çš„æƒé‡
                        if key == 'ensemble':
                            weight = 0.5
                        elif key == 'main':
                            weight = 0.3
                        else:
                            weight = 0.1
                        
                        total_ensemble_loss += weight * loss_item
                    
                    loss = total_ensemble_loss
                    
                    # æ›´æ–°é›†æˆæŸå¤±ç»Ÿè®¡
                    for key, l in losses.items():
                        ensemble_losses[key] += l.item()
                    
                    # ä½¿ç”¨é›†æˆè¾“å‡ºè®¡ç®—å‡†ç¡®ç‡
                    pred_probs = torch.sigmoid(outputs['ensemble'])
                else:
                    # æ ‡å‡†æ¨¡å¼
                    if outputs.dim() > 1:
                        outputs = outputs.squeeze(-1)
                    loss = criterion(outputs, labels)
                    pred_probs = torch.sigmoid(outputs)
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if gradient_clip > 0:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
                
                optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            predictions = (pred_probs > 0.5).float()
            correct_predictions += (predictions == labels).sum().item()
            
            # æ›´æ–°ç»Ÿè®¡
            total_loss += loss.item()
            total_samples += labels.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            avg_loss = total_loss / (batch_idx + 1)
            accuracy = correct_predictions / total_samples
            
            if ensemble_mode:
                progress_bar.set_postfix({
                    'Loss': f'{avg_loss:.4f}',
                    'Acc': f'{accuracy:.4f}',
                    'Ensemble': f'{ensemble_losses["ensemble"]/(batch_idx+1):.4f}'
                })
            else:
                progress_bar.set_postfix({
                    'Loss': f'{avg_loss:.4f}',
                    'Acc': f'{accuracy:.4f}'
                })
            
        except Exception as e:
            print(f"âš ï¸ è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
            continue
    
    # å­¦ä¹ ç‡è°ƒåº¦
    if scheduler is not None:
        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
            scheduler.step(total_loss / len(train_loader))
        else:
            scheduler.step()
    
    # è¿”å›è®­ç»ƒç»“æœ
    avg_loss = total_loss / len(train_loader)
    accuracy = correct_predictions / total_samples
    
    results = {
        'loss': avg_loss,
        'accuracy': accuracy,
        'learning_rate': optimizer.param_groups[0]['lr']
    }
    
    if ensemble_mode:
        # æ·»åŠ é›†æˆæŸå¤±ç»Ÿè®¡
        for key in ensemble_losses:
            results[f'{key}_loss'] = ensemble_losses[key] / len(train_loader)
    
    return results

def validate_epoch(model, val_loader, criterion, device, ensemble_mode=False):
    """
    éªŒè¯ä¸€ä¸ªepoch - æ”¯æŒé›†æˆå­¦ä¹ è¯„ä¼°
    
    Args:
        model: æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡
        ensemble_mode: æ˜¯å¦ä¸ºé›†æˆæ¨¡å¼
    """
    model.eval()
    total_loss = 0.0
    total_samples = 0
    correct_predictions = 0
    all_predictions = []
    all_labels = []
    
    # é›†æˆæ¨¡å¼çš„æŸå¤±ç»Ÿè®¡
    if ensemble_mode:
        ensemble_losses = {
            'main': 0.0,
            'spatial': 0.0,
            'temporal': 0.0,
            'ensemble': 0.0
        }
        ensemble_predictions = {
            'main': [],
            'spatial': [],
            'temporal': [],
            'ensemble': []
        }
    
    progress_bar = tqdm(val_loader, desc="éªŒè¯ä¸­", leave=False)
    
    with torch.no_grad():
        for batch_idx, batch_data in enumerate(progress_bar):
            try:
                # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                if len(batch_data) == 3:
                    videos, labels, additional_features = batch_data
                    videos = videos.to(device)
                    labels = labels.to(device)
                    
                    # å¤„ç†é¢å¤–ç‰¹å¾
                    if additional_features and isinstance(additional_features, dict):
                        for key, value in additional_features.items():
                            if isinstance(value, torch.Tensor):
                                additional_features[key] = value.to(device)
                else:
                    videos, labels = batch_data
                    videos = videos.to(device)
                    labels = labels.to(device)
                    additional_features = None
                
                # å‰å‘ä¼ æ’­
                if additional_features is not None:
                    outputs = model(videos, additional_features)
                else:
                    outputs = model(videos)
                
                # è®¡ç®—æŸå¤±å’Œé¢„æµ‹
                if ensemble_mode and isinstance(outputs, dict):
                    # é›†æˆæ¨¡å¼
                    losses = {}
                    total_ensemble_loss = 0
                    
                    for key, pred in outputs.items():
                        if pred.dim() > 1:
                            pred = pred.squeeze(-1)
                        loss_item = criterion(pred, labels)
                        losses[key] = loss_item
                        
                        # æƒé‡ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
                        if key == 'ensemble':
                            weight = 0.5
                        elif key == 'main':
                            weight = 0.3
                        else:
                            weight = 0.1
                        
                        total_ensemble_loss += weight * loss_item
                        
                        # ä¿å­˜é¢„æµ‹ç»“æœ
                        pred_probs = torch.sigmoid(pred)
                        ensemble_predictions[key].extend(pred_probs.cpu().numpy())
                        ensemble_losses[key] += loss_item.item()
                    
                    loss = total_ensemble_loss
                    pred_probs = torch.sigmoid(outputs['ensemble'])
                else:
                    # æ ‡å‡†æ¨¡å¼
                    if outputs.dim() > 1:
                        outputs = outputs.squeeze(-1)
                    loss = criterion(outputs, labels)
                    pred_probs = torch.sigmoid(outputs)
                
                # è®¡ç®—å‡†ç¡®ç‡
                predictions = (pred_probs > 0.5).float()
                correct_predictions += (predictions == labels).sum().item()
                
                # ä¿å­˜é¢„æµ‹å’Œæ ‡ç­¾ç”¨äºè¯¦ç»†è¯„ä¼°
                all_predictions.extend(pred_probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                # æ›´æ–°ç»Ÿè®¡
                total_loss += loss.item()
                total_samples += labels.size(0)
                
                # æ›´æ–°è¿›åº¦æ¡
                avg_loss = total_loss / (batch_idx + 1)
                accuracy = correct_predictions / total_samples
                progress_bar.set_postfix({
                    'Val Loss': f'{avg_loss:.4f}',
                    'Val Acc': f'{accuracy:.4f}'
                })
                
            except Exception as e:
                print(f"âš ï¸ éªŒè¯æ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}")
                continue
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    avg_loss = total_loss / len(val_loader)
    accuracy = correct_predictions / total_samples
    
    # è®¡ç®—AUCç­‰é«˜çº§æŒ‡æ ‡
    try:
        from sklearn.metrics import roc_auc_score, precision_recall_fscore_support
        auc_score = roc_auc_score(all_labels, all_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, 
            np.array(all_predictions) > 0.5, 
            average='binary'
        )
    except Exception as e:
        print(f"âš ï¸ è®¡ç®—é«˜çº§æŒ‡æ ‡å¤±è´¥: {e}")
        auc_score = 0.0
        precision = recall = f1 = 0.0
    
    results = {
        'loss': avg_loss,
        'accuracy': accuracy,
        'auc': auc_score,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': all_predictions,
        'labels': all_labels
    }
    
    if ensemble_mode:
        # æ·»åŠ é›†æˆè¯„ä¼°ç»“æœ
        for key in ensemble_losses:
            results[f'{key}_loss'] = ensemble_losses[key] / len(val_loader)
            results[f'{key}_predictions'] = ensemble_predictions[key]
        
        # è®¡ç®—é›†æˆæ¨¡å‹çš„AUC
        try:
            ensemble_auc = roc_auc_score(all_labels, ensemble_predictions['ensemble'])
            results['ensemble_auc'] = ensemble_auc
        except:
            results['ensemble_auc'] = 0.0
    
    return results

def train_ensemble_models(models, train_loader, val_loader, criterion, optimizers, 
                         device, num_epochs=10, schedulers=None, use_amp=False):
    """
    è®­ç»ƒå¤šä¸ªæ¨¡å‹è¿›è¡Œé›†æˆå­¦ä¹ 
    
    Args:
        models: æ¨¡å‹åˆ—è¡¨
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizers: ä¼˜åŒ–å™¨åˆ—è¡¨
        device: è®¾å¤‡
        num_epochs: è®­ç»ƒè½®æ•°
        schedulers: å­¦ä¹ ç‡è°ƒåº¦å™¨åˆ—è¡¨
        use_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    """
    ensemble_results = []
    
    for i, (model, optimizer) in enumerate(zip(models, optimizers)):
        print(f"\nğŸš€ è®­ç»ƒé›†æˆæ¨¡å‹ {i+1}/{len(models)}")
        
        scheduler = schedulers[i] if schedulers else None
        model_results = {'train_history': [], 'val_history': []}
        
        best_val_auc = 0.0
        best_model_state = None
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒ
            train_results = train_epoch(
                model, train_loader, criterion, optimizer, device,
                scheduler=scheduler, use_amp=use_amp
            )
            
            # éªŒè¯
            val_results = validate_epoch(model, val_loader, criterion, device)
            
            # ä¿å­˜å†å²
            model_results['train_history'].append(train_results)
            model_results['val_history'].append(val_results)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_results['auc'] > best_val_auc:
                best_val_auc = val_results['auc']
                best_model_state = model.state_dict().copy()
            
            print(f"è®­ç»ƒ - Loss: {train_results['loss']:.4f}, Acc: {train_results['accuracy']:.4f}")
            print(f"éªŒè¯ - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.4f}, AUC: {val_results['auc']:.4f}")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        model_results['best_val_auc'] = best_val_auc
        ensemble_results.append(model_results)
        
        print(f"âœ… æ¨¡å‹ {i+1} è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}")
    
    return ensemble_results

def ensemble_predict(models, data_loader, device, weights=None):
    """
    ä½¿ç”¨å¤šä¸ªæ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹
    
    Args:
        models: æ¨¡å‹åˆ—è¡¨
        data_loader: æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        weights: æ¨¡å‹æƒé‡ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å¹³å‡æƒé‡ï¼‰
    """
    if weights is None:
        weights = [1.0 / len(models)] * len(models)
    
    all_predictions = []
    all_labels = []
    
    # è®¾ç½®æ‰€æœ‰æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    for model in models:
        model.eval()
    
    with torch.no_grad():
        for batch_data in tqdm(data_loader, desc="é›†æˆé¢„æµ‹ä¸­"):
            if len(batch_data) == 3:
                videos, labels, additional_features = batch_data
                videos = videos.to(device)
                labels = labels.to(device)
                
                if additional_features and isinstance(additional_features, dict):
                    for key, value in additional_features.items():
                        if isinstance(value, torch.Tensor):
                            additional_features[key] = value.to(device)
            else:
                videos, labels = batch_data
                videos = videos.to(device)
                labels = labels.to(device)
                additional_features = None
            
            # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
            batch_predictions = []
            for model in models:
                if additional_features is not None:
                    outputs = model(videos, additional_features)
                else:
                    outputs = model(videos)
                
                if isinstance(outputs, dict):
                    # é›†æˆæ¨¡å¼ï¼Œä½¿ç”¨ensembleè¾“å‡º
                    pred = outputs['ensemble']
                else:
                    pred = outputs
                
                if pred.dim() > 1:
                    pred = pred.squeeze(-1)
                
                pred_probs = torch.sigmoid(pred)
                batch_predictions.append(pred_probs.cpu().numpy())
            
            # åŠ æƒå¹³å‡
            ensemble_pred = np.average(batch_predictions, axis=0, weights=weights)
            all_predictions.extend(ensemble_pred)
            all_labels.extend(labels.cpu().numpy())
    
    return np.array(all_predictions), np.array(all_labels)

print("âœ… ä¼˜åŒ–è®­ç»ƒå‡½æ•°å®šä¹‰å®Œæˆ")