#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿå®‰è£…è„šæœ¬ - æœ¬åœ°RTX4070ç‰ˆæœ¬

è¿™ä¸ªè„šæœ¬å¸®åŠ©ç”¨æˆ·å¿«é€Ÿè®¾ç½®å’ŒéªŒè¯æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿçš„è¿è¡Œç¯å¢ƒã€‚

ä½¿ç”¨æ–¹æ³•:
    python setup.py [--check-only] [--install-deps]
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path
import importlib.util

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿç¯å¢ƒè®¾ç½®')
    parser.add_argument('--check-only', action='store_true', 
                       help='ä»…æ£€æŸ¥ç¯å¢ƒï¼Œä¸å®‰è£…ä¾èµ–')
    parser.add_argument('--install-deps', action='store_true',
                       help='è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–')
    parser.add_argument('--cuda-version', type=str, default='auto',
                       choices=['auto', '11.8', '12.1'],
                       help='æŒ‡å®šCUDAç‰ˆæœ¬ (é»˜è®¤: auto)')
    
    return parser.parse_args()

def print_section(title):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"ğŸ”§ {title}")
    print("="*60)

def print_step(step, description):
    """æ‰“å°æ­¥éª¤"""
    print(f"\nğŸ“‹ {step}: {description}")
    print("-" * 40)

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print_step("æ­¥éª¤ 1", "æ£€æŸ¥Pythonç‰ˆæœ¬")
    
    version = sys.version_info
    print(f"å½“å‰Pythonç‰ˆæœ¬: {version.major}.{version.minor}.{version.micro}")
    
    if version.major != 3 or version.minor < 8 or version.minor > 11:
        print("âŒ Pythonç‰ˆæœ¬ä¸å…¼å®¹")
        print("   è¦æ±‚: Python 3.8 - 3.11")
        print(f"   å½“å‰: Python {version.major}.{version.minor}.{version.micro}")
        return False
    else:
        print("âœ… Pythonç‰ˆæœ¬å…¼å®¹")
        return True

def check_cuda_availability():
    """æ£€æŸ¥CUDAå¯ç”¨æ€§"""
    print_step("æ­¥éª¤ 2", "æ£€æŸ¥CUDAç¯å¢ƒ")
    
    try:
        # æ£€æŸ¥nvidia-smi
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… NVIDIAé©±åŠ¨å·²å®‰è£…")
            
            # æå–CUDAç‰ˆæœ¬ä¿¡æ¯
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if 'CUDA Version:' in line:
                    cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                    print(f"   æ”¯æŒçš„CUDAç‰ˆæœ¬: {cuda_version}")
                    break
            
            return True
        else:
            print("âŒ NVIDIAé©±åŠ¨æœªå®‰è£…æˆ–ä¸å¯ç”¨")
            return False
    
    except FileNotFoundError:
        print("âŒ nvidia-smi å‘½ä»¤æœªæ‰¾åˆ°")
        print("   è¯·ç¡®ä¿å·²å®‰è£…NVIDIAé©±åŠ¨")
        return False
    except Exception as e:
        print(f"âŒ CUDAæ£€æŸ¥å¤±è´¥: {e}")
        return False

def check_package(package_name, import_name=None, version_attr=None):
    """æ£€æŸ¥PythonåŒ…æ˜¯å¦å·²å®‰è£…"""
    if import_name is None:
        import_name = package_name
    
    try:
        spec = importlib.util.find_spec(import_name)
        if spec is None:
            return False, None
        
        module = importlib.import_module(import_name)
        
        if version_attr:
            version = getattr(module, version_attr, 'Unknown')
        else:
            version = getattr(module, '__version__', 'Unknown')
        
        return True, version
    
    except ImportError:
        return False, None
    except Exception:
        return False, None

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    print_step("æ­¥éª¤ 3", "æ£€æŸ¥Pythonä¾èµ–")
    
    # æ ¸å¿ƒä¾èµ–
    core_deps = [
        ('torch', 'torch', '__version__'),
        ('torchvision', 'torchvision', '__version__'),
        ('opencv-python', 'cv2', '__version__'),
        ('numpy', 'numpy', '__version__'),
        ('pandas', 'pandas', '__version__'),
        ('scikit-learn', 'sklearn', '__version__'),
        ('matplotlib', 'matplotlib', '__version__'),
        ('seaborn', 'seaborn', '__version__'),
        ('tqdm', 'tqdm', '__version__'),
        ('pillow', 'PIL', '__version__'),
    ]
    
    # å¯é€‰ä¾èµ–
    optional_deps = [
        ('av', 'av', '__version__'),
        ('albumentations', 'albumentations', '__version__'),
        ('psutil', 'psutil', '__version__'),
    ]
    
    missing_core = []
    missing_optional = []
    
    print("æ ¸å¿ƒä¾èµ–:")
    for package_name, import_name, version_attr in core_deps:
        installed, version = check_package(package_name, import_name, version_attr)
        if installed:
            print(f"  âœ… {package_name}: {version}")
        else:
            print(f"  âŒ {package_name}: æœªå®‰è£…")
            missing_core.append(package_name)
    
    print("\nå¯é€‰ä¾èµ–:")
    for package_name, import_name, version_attr in optional_deps:
        installed, version = check_package(package_name, import_name, version_attr)
        if installed:
            print(f"  âœ… {package_name}: {version}")
        else:
            print(f"  âš ï¸ {package_name}: æœªå®‰è£… (å¯é€‰)")
            missing_optional.append(package_name)
    
    return missing_core, missing_optional

def check_pytorch_cuda():
    """æ£€æŸ¥PyTorch CUDAæ”¯æŒ"""
    print_step("æ­¥éª¤ 4", "æ£€æŸ¥PyTorch CUDAæ”¯æŒ")
    
    try:
        import torch
        
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
            
            return True
        else:
            print("âŒ PyTorch CUDAæ”¯æŒä¸å¯ç”¨")
            return False
    
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ PyTorch CUDAæ£€æŸ¥å¤±è´¥: {e}")
        return False

def test_gpu_performance():
    """æµ‹è¯•GPUæ€§èƒ½"""
    print_step("æ­¥éª¤ 5", "GPUæ€§èƒ½æµ‹è¯•")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
            return
        
        device = torch.device('cuda')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ç®€å•çš„çŸ©é˜µä¹˜æ³•æµ‹è¯•
        print("è¿›è¡Œç®€å•çš„GPUè®¡ç®—æµ‹è¯•...")
        
        size = 1000
        a = torch.randn(size, size, device=device)
        b = torch.randn(size, size, device=device)
        
        # é¢„çƒ­
        for _ in range(5):
            _ = torch.mm(a, b)
        
        torch.cuda.synchronize()
        
        # è®¡æ—¶æµ‹è¯•
        import time
        start_time = time.time()
        
        for _ in range(10):
            c = torch.mm(a, b)
        
        torch.cuda.synchronize()
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"âœ… GPUè®¡ç®—æµ‹è¯•å®Œæˆ")
        print(f"   çŸ©é˜µå¤§å°: {size}x{size}")
        print(f"   å¹³å‡è®¡ç®—æ—¶é—´: {avg_time*1000:.2f} ms")
        
        # å†…å­˜æµ‹è¯•
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        memory_reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"   GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
        
    except Exception as e:
        print(f"âŒ GPUæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

def install_dependencies(missing_core, missing_optional, cuda_version='auto'):
    """å®‰è£…ç¼ºå¤±çš„ä¾èµ–"""
    print_step("å®‰è£…ä¾èµ–", "è‡ªåŠ¨å®‰è£…ç¼ºå¤±çš„åŒ…")
    
    if not missing_core and not missing_optional:
        print("âœ… æ‰€æœ‰ä¾èµ–éƒ½å·²å®‰è£…")
        return True
    
    try:
        # å®‰è£…æ ¸å¿ƒä¾èµ–
        if missing_core:
            print("å®‰è£…æ ¸å¿ƒä¾èµ–...")
            
            # ç‰¹æ®Šå¤„ç†PyTorch
            if 'torch' in missing_core or 'torchvision' in missing_core:
                print("å®‰è£…PyTorch...")
                
                if cuda_version == 'auto':
                    # å°è¯•æ£€æµ‹CUDAç‰ˆæœ¬
                    try:
                        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
                        if 'CUDA Version: 12.' in result.stdout:
                            cuda_version = '12.1'
                        elif 'CUDA Version: 11.' in result.stdout:
                            cuda_version = '11.8'
                        else:
                            cuda_version = '11.8'  # é»˜è®¤
                    except:
                        cuda_version = '11.8'  # é»˜è®¤
                
                if cuda_version == '12.1':
                    torch_cmd = [
                        sys.executable, '-m', 'pip', 'install', 
                        'torch', 'torchvision', 'torchaudio',
                        '--index-url', 'https://download.pytorch.org/whl/cu121'
                    ]
                else:  # 11.8
                    torch_cmd = [
                        sys.executable, '-m', 'pip', 'install', 
                        'torch', 'torchvision', 'torchaudio',
                        '--index-url', 'https://download.pytorch.org/whl/cu118'
                    ]
                
                print(f"æ‰§è¡Œ: {' '.join(torch_cmd)}")
                result = subprocess.run(torch_cmd)
                
                if result.returncode != 0:
                    print("âŒ PyTorchå®‰è£…å¤±è´¥")
                    return False
                
                # ä»ç¼ºå¤±åˆ—è¡¨ä¸­ç§»é™¤
                missing_core = [pkg for pkg in missing_core if pkg not in ['torch', 'torchvision']]
            
            # å®‰è£…å…¶ä»–æ ¸å¿ƒä¾èµ–
            if missing_core:
                cmd = [sys.executable, '-m', 'pip', 'install'] + missing_core
                print(f"æ‰§è¡Œ: {' '.join(cmd)}")
                result = subprocess.run(cmd)
                
                if result.returncode != 0:
                    print("âŒ æ ¸å¿ƒä¾èµ–å®‰è£…å¤±è´¥")
                    return False
        
        # å®‰è£…å¯é€‰ä¾èµ–
        if missing_optional:
            print("å®‰è£…å¯é€‰ä¾èµ–...")
            cmd = [sys.executable, '-m', 'pip', 'install'] + missing_optional
            print(f"æ‰§è¡Œ: {' '.join(cmd)}")
            result = subprocess.run(cmd)
            
            if result.returncode != 0:
                print("âš ï¸ éƒ¨åˆ†å¯é€‰ä¾èµ–å®‰è£…å¤±è´¥ï¼ˆä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ï¼‰")
        
        print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ ä¾èµ–å®‰è£…å¤±è´¥: {e}")
        return False

def check_project_structure():
    """æ£€æŸ¥é¡¹ç›®ç»“æ„"""
    print_step("æ­¥éª¤ 6", "æ£€æŸ¥é¡¹ç›®ç»“æ„")
    
    required_files = [
        'src/__init__.py',
        'src/config.py',
        'src/data_processing.py',
        'src/dataset.py',
        'src/model.py',
        'src/training.py',
        'src/utils.py',
        'train.py',
        'evaluate.py',
        'inference.py',
        'requirements.txt',
        'README.md'
    ]
    
    missing_files = []
    
    for file_path in required_files:
        if Path(file_path).exists():
            print(f"  âœ… {file_path}")
        else:
            print(f"  âŒ {file_path}")
            missing_files.append(file_path)
    
    if missing_files:
        print(f"\nâš ï¸ ç¼ºå¤± {len(missing_files)} ä¸ªæ–‡ä»¶")
        return False
    else:
        print("\nâœ… é¡¹ç›®ç»“æ„å®Œæ•´")
        return True

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("="*60)
    print("ğŸš€ æ·±åº¦ä¼ªé€ æ£€æµ‹ç³»ç»Ÿç¯å¢ƒè®¾ç½®")
    print("="*60)
    print(f"æ¨¡å¼: {'ä»…æ£€æŸ¥' if args.check_only else 'æ£€æŸ¥å¹¶å®‰è£…'}")
    print(f"CUDAç‰ˆæœ¬: {args.cuda_version}")
    print("="*60)
    
    success = True
    
    # 1. æ£€æŸ¥Pythonç‰ˆæœ¬
    if not check_python_version():
        success = False
    
    # 2. æ£€æŸ¥CUDA
    cuda_available = check_cuda_availability()
    if not cuda_available:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆæ€§èƒ½è¾ƒä½ï¼‰")
    
    # 3. æ£€æŸ¥ä¾èµ–
    missing_core, missing_optional = check_dependencies()
    
    if missing_core:
        print(f"\nâŒ ç¼ºå¤±æ ¸å¿ƒä¾èµ–: {', '.join(missing_core)}")
        success = False
    
    # 4. æ£€æŸ¥PyTorch CUDA
    if cuda_available:
        pytorch_cuda = check_pytorch_cuda()
        if not pytorch_cuda:
            print("âš ï¸ PyTorch CUDAæ”¯æŒä¸å¯ç”¨")
    
    # 5. GPUæ€§èƒ½æµ‹è¯•
    if cuda_available:
        test_gpu_performance()
    
    # 6. æ£€æŸ¥é¡¹ç›®ç»“æ„
    if not check_project_structure():
        success = False
    
    # å®‰è£…ä¾èµ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not args.check_only and (missing_core or missing_optional):
        if install_dependencies(missing_core, missing_optional, args.cuda_version):
            print("\nğŸ”„ é‡æ–°æ£€æŸ¥ä¾èµ–...")
            missing_core, missing_optional = check_dependencies()
            if not missing_core:
                success = True
    
    # æœ€ç»ˆç»“æœ
    print_section("è®¾ç½®ç»“æœ")
    
    if success and not missing_core:
        print("ğŸ‰ ç¯å¢ƒè®¾ç½®æˆåŠŸï¼")
        print("\nğŸ“š æ¥ä¸‹æ¥ä½ å¯ä»¥ï¼š")
        print("   1. è¿è¡Œ python demo.py --quick è¿›è¡Œå¿«é€Ÿæ¼”ç¤º")
        print("   2. è¿è¡Œ python train.py å¼€å§‹è®­ç»ƒ")
        print("   3. æŸ¥çœ‹ README.md äº†è§£è¯¦ç»†ä½¿ç”¨æ–¹æ³•")
        return 0
    else:
        print("âŒ ç¯å¢ƒè®¾ç½®æœªå®Œæˆ")
        if missing_core:
            print(f"   ç¼ºå¤±æ ¸å¿ƒä¾èµ–: {', '.join(missing_core)}")
            print("   è¯·è¿è¡Œ: python setup.py --install-deps")
        return 1

if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)