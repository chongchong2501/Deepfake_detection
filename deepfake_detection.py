
import os
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import random
import warnings
import gc
import json
import time
import sys
from pathlib import Path
from datetime import datetime
warnings.filterwarnings('ignore')

# PyTorchç›¸å…³
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
import torchvision.models as models
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau
from torch.cuda.amp import GradScaler, autocast

# æœºå™¨å­¦ä¹ æŒ‡æ ‡
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, auc, precision_recall_curve, balanced_accuracy_score
)
from sklearn.model_selection import train_test_split

# æ•°æ®å¢å¼º
try:
    import albumentations as A
    from albumentations.pytorch import ToTensorV2
    ALBUMENTATIONS_AVAILABLE = True
except ImportError:
    ALBUMENTATIONS_AVAILABLE = False
    print("è­¦å‘Š: albumentationsæœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€æ•°æ®å¢å¼º")

# =============================================================================
# å…¨å±€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# æ£€æŸ¥GPUå¯ç”¨æ€§
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# åˆ›å»ºå¿…è¦çš„ç›®å½•
for dir_name in ['./data', './models', './logs', './results', './results/evaluation']:
    os.makedirs(dir_name, exist_ok=True)

# æ£€æŸ¥æ˜¯å¦åœ¨Kaggleç¯å¢ƒä¸­
IS_KAGGLE = os.path.exists('/kaggle')
BASE_DATA_DIR = '/kaggle/input/ff-c23/FaceForensics++_C23' if IS_KAGGLE else './FaceForensics++_C23'

print(f"ç¯å¢ƒ: {'Kaggle' if IS_KAGGLE else 'æœ¬åœ°'}")
print(f"æ•°æ®åŸºç¡€è·¯å¾„: {BASE_DATA_DIR}")
print("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")

# =============================================================================
# æ•°æ®å¤„ç†æ¨¡å—
# =============================================================================

def extract_frames_memory_efficient(video_path, max_frames=24, target_size=(160, 160),
                                   quality_threshold=30, skip_frames=2):
    """å†…å­˜å‹å¥½çš„å¸§æå–å‡½æ•°"""
    cap = cv2.VideoCapture(video_path)
    frames = []

    if not cap.isOpened():
        print(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        return frames

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames == 0:
        cap.release()
        return frames

    # å‡åŒ€é‡‡æ ·ç­–ç•¥
    if total_frames <= max_frames:
        frame_indices = list(range(0, total_frames, skip_frames))
    else:
        step = max(1, total_frames // max_frames)
        frame_indices = list(range(0, total_frames, step))[:max_frames]

    frame_count = 0
    for frame_idx in frame_indices:
        if frame_count >= max_frames:
            break

        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ç®€åŒ–è´¨é‡æ£€æµ‹
            gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
            quality = cv2.Laplacian(gray, cv2.CV_64F).var()

            if quality > quality_threshold:
                frame = cv2.resize(frame, target_size)
                frames.append(frame)
                frame_count += 1

    cap.release()

    # å¦‚æœå¸§æ•°ä¸è¶³ï¼Œé‡å¤æœ€åä¸€å¸§
    while len(frames) < max_frames and len(frames) > 0:
        frames.append(frames[-1].copy())

    return frames[:max_frames]

def process_videos_simple(base_data_dir, max_videos_per_class=80, max_frames=24):
    """ç®€åŒ–çš„è§†é¢‘å¤„ç†å‡½æ•°"""
    data_list = []
    fake_methods = ['Deepfakes', 'Face2Face', 'FaceShifter', 'FaceSwap', 'NeuralTextures']

    print("å¼€å§‹å¤„ç†çœŸå®è§†é¢‘...")
    # å¤„ç†çœŸå®è§†é¢‘
    original_dir = os.path.join(base_data_dir, 'original')
    if os.path.exists(original_dir):
        video_files = [f for f in os.listdir(original_dir)
                      if f.endswith(('.mp4', '.avi', '.mov'))]
        
        if len(video_files) > max_videos_per_class:
            video_files = random.sample(video_files, max_videos_per_class)

        print(f"æ‰¾åˆ° {len(video_files)} ä¸ªçœŸå®è§†é¢‘")

        for video_file in tqdm(video_files, desc="å¤„ç†çœŸå®è§†é¢‘"):
            try:
                video_path = os.path.join(original_dir, video_file)
                frames = extract_frames_memory_efficient(video_path, max_frames)
                
                if len(frames) >= max_frames // 2:  # è‡³å°‘è¦æœ‰ä¸€åŠçš„å¸§
                    data_list.append({
                        'video_path': video_path,
                        'frames': frames,
                        'label': 0,  # çœŸå®è§†é¢‘
                        'method': 'original'
                    })
            except Exception as e:
                print(f"å¤„ç†è§†é¢‘ {video_file} æ—¶å‡ºé”™: {e}")
                continue

    # å¤„ç†ä¼ªé€ è§†é¢‘
    print("å¼€å§‹å¤„ç†ä¼ªé€ è§†é¢‘...")
    for method in fake_methods:
        method_dir = os.path.join(base_data_dir, method)
        if os.path.exists(method_dir):
            video_files = [f for f in os.listdir(method_dir)
                          if f.endswith(('.mp4', '.avi', '.mov'))]
            
            if len(video_files) > max_videos_per_class:
                video_files = random.sample(video_files, max_videos_per_class)

            print(f"å¤„ç† {method}: {len(video_files)} ä¸ªè§†é¢‘")

            for video_file in tqdm(video_files, desc=f"å¤„ç†{method}"):
                try:
                    video_path = os.path.join(method_dir, video_file)
                    frames = extract_frames_memory_efficient(video_path, max_frames)
                    
                    if len(frames) >= max_frames // 2:
                        data_list.append({
                            'video_path': video_path,
                            'frames': frames,
                            'label': 1,  # ä¼ªé€ è§†é¢‘
                            'method': method
                        })
                except Exception as e:
                    print(f"å¤„ç†è§†é¢‘ {video_file} æ—¶å‡ºé”™: {e}")
                    continue

    print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(data_list)} ä¸ªè§†é¢‘")
    return data_list

def create_dataset_split(data_list, test_size=0.2, val_size=0.1):
    """åˆ›å»ºæ•°æ®é›†åˆ’åˆ†"""
    # åˆ†ç¦»çœŸå®å’Œä¼ªé€ æ•°æ®
    real_data = [item for item in data_list if item['label'] == 0]
    fake_data = [item for item in data_list if item['label'] == 1]
    
    print(f"çœŸå®è§†é¢‘: {len(real_data)} ä¸ª")
    print(f"ä¼ªé€ è§†é¢‘: {len(fake_data)} ä¸ª")
    
    # åˆ†åˆ«åˆ’åˆ†çœŸå®å’Œä¼ªé€ æ•°æ®
    real_train, real_temp = train_test_split(real_data, test_size=test_size+val_size, random_state=42)
    real_val, real_test = train_test_split(real_temp, test_size=test_size/(test_size+val_size), random_state=42)
    
    fake_train, fake_temp = train_test_split(fake_data, test_size=test_size+val_size, random_state=42)
    fake_val, fake_test = train_test_split(fake_temp, test_size=test_size/(test_size+val_size), random_state=42)
    
    # åˆå¹¶æ•°æ®
    train_data = real_train + fake_train
    val_data = real_val + fake_val
    test_data = real_test + fake_test
    
    # æ‰“ä¹±æ•°æ®
    random.shuffle(train_data)
    random.shuffle(val_data)
    random.shuffle(test_data)
    
    return train_data, val_data, test_data

def save_dataset_to_csv(data_list, filename):
    """å°†æ•°æ®é›†ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
    df_data = []
    for item in data_list:
        df_data.append({
            'video_path': item['video_path'],
            'label': item['label'],
            'method': item['method'],
            'num_frames': len(item['frames'])
        })
    
    df = pd.DataFrame(df_data)
    df.to_csv(filename, index=False)
    print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {filename}")
    return df

# =============================================================================
# æ•°æ®é›†ç±»å®šä¹‰
# =============================================================================

class DeepfakeVideoDataset(Dataset):
    """æ·±åº¦ä¼ªé€ è§†é¢‘æ•°æ®é›†ç±»"""
    
    def __init__(self, csv_file=None, data_list=None, transform=None, max_frames=24):
        if csv_file is not None:
            self.df = pd.read_csv(csv_file)
            self.data_list = None
        elif data_list is not None:
            self.data_list = data_list
            self.df = None
        else:
            raise ValueError("å¿…é¡»æä¾›csv_fileæˆ–data_list")
            
        self.transform = transform
        self.max_frames = max_frames
    
    def __len__(self):
        if self.df is not None:
            return len(self.df)
        return len(self.data_list)
    
    def __getitem__(self, idx):
        if self.data_list is not None:
            # ç›´æ¥ä»å†…å­˜ä¸­çš„æ•°æ®åˆ—è¡¨è·å–
            item = self.data_list[idx]
            frames = item['frames']
            label = item['label']
        else:
            # ä»CSVæ–‡ä»¶è·å–è·¯å¾„å¹¶é‡æ–°æå–å¸§
            row = self.df.iloc[idx]
            video_path = row['video_path']
            label = row['label']
            frames = extract_frames_memory_efficient(video_path, self.max_frames)
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§
        if len(frames) == 0:
            # åˆ›å»ºé»‘è‰²å¸§ä½œä¸ºfallback
            frames = [np.zeros((160, 160, 3), dtype=np.uint8) for _ in range(self.max_frames)]
        
        while len(frames) < self.max_frames:
            frames.append(frames[-1].copy() if frames else np.zeros((160, 160, 3), dtype=np.uint8))
        
        frames = frames[:self.max_frames]
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            frames = [self.transform(frame) for frame in frames]
        else:
            # é»˜è®¤å˜æ¢
            frames = [torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0 for frame in frames]
        
        # å †å å¸§ (T, C, H, W)
        video_tensor = torch.stack(frames)
        label_tensor = torch.tensor(label, dtype=torch.float32)
        
        return video_tensor, label_tensor

# =============================================================================
# æ¨¡å‹å®šä¹‰
# =============================================================================

class OptimizedDeepfakeDetector(nn.Module):
    """ä¼˜åŒ–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹"""
    
    def __init__(self, backbone='resnet50', hidden_dim=512, num_layers=2, 
                 dropout=0.3, use_attention=True):
        super(OptimizedDeepfakeDetector, self).__init__()
        
        self.use_attention = use_attention
        
        # ç‰¹å¾æå–å™¨
        if backbone == 'resnet50':
            self.backbone = models.resnet50(pretrained=True)
            feature_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        elif backbone == 'resnet18':
            self.backbone = models.resnet18(pretrained=True)
            feature_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„backbone: {backbone}")
        
        # æ—¶åºå»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=feature_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        lstm_output_dim = hidden_dim * 2  # åŒå‘LSTM
        
        # æ³¨æ„åŠ›æœºåˆ¶
        if self.use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=lstm_output_dim,
                num_heads=8,
                dropout=dropout,
                batch_first=True
            )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(lstm_output_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # x shape: (batch_size, num_frames, channels, height, width)
        batch_size, num_frames = x.shape[:2]
        
        # é‡å¡‘ä¸º (batch_size * num_frames, channels, height, width)
        x = x.view(-1, *x.shape[2:])
        
        # ç‰¹å¾æå–
        features = self.backbone(x)  # (batch_size * num_frames, feature_dim)
        
        # é‡å¡‘å›æ—¶åºæ ¼å¼
        features = features.view(batch_size, num_frames, -1)
        
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(features)  # (batch_size, num_frames, hidden_dim*2)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = None
        if self.use_attention:
            attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)
            # å…¨å±€å¹³å‡æ± åŒ–
            pooled = attended_out.mean(dim=1)  # (batch_size, hidden_dim*2)
        else:
            # ç®€å•çš„å…¨å±€å¹³å‡æ± åŒ–
            pooled = lstm_out.mean(dim=1)
        
        # åˆ†ç±»
        output = self.classifier(pooled)
        
        return output.squeeze(-1), attention_weights

# =============================================================================
# æŸå¤±å‡½æ•°å’Œå·¥å…·ç±»
# =============================================================================

class FocalLoss(nn.Module):
    """ç„¦ç‚¹æŸå¤±å‡½æ•° - è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜"""
    
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = nn.BCELoss(reduction='none')(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1

        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False

    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()

# =============================================================================
# æ•°æ®å˜æ¢
# =============================================================================

def get_transforms(mode='train', image_size=160):
    """è·å–æ•°æ®å˜æ¢"""
    if mode == 'train':
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((int(image_size * 1.1), int(image_size * 1.1))),
            transforms.RandomCrop((image_size, image_size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.RandomRotation(degrees=10),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))
        ])
    else:
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

# =============================================================================
# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# =============================================================================

def train_epoch(model, train_loader, criterion, optimizer, device, scaler=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []

    pbar = tqdm(train_loader, desc='Training', leave=False)

    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()

        if scaler is not None:
            with autocast():
                output, _ = model(data)
                loss = criterion(output, target)
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            output, _ = model(data)
            loss = criterion(output, target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

        total_loss += loss.item()
        predicted = (output > 0.5).float()
        total += target.size(0)
        correct += (predicted == target).sum().item()

        all_preds.extend(output.detach().cpu().numpy())
        all_targets.extend(target.detach().cpu().numpy())

        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total

    try:
        auc_score = roc_auc_score(all_targets, all_preds)
    except:
        auc_score = 0.0

    return avg_loss, accuracy, auc_score

def validate_epoch(model, val_loader, criterion, device):
    """éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        pbar = tqdm(val_loader, desc='Validation', leave=False)

        for data, target in pbar:
            data, target = data.to(device), target.to(device)
            output, _ = model(data)
            loss = criterion(output, target)

            total_loss += loss.item()
            predicted = (output > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()

            all_preds.extend(output.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })

    avg_loss = total_loss / len(val_loader)
    accuracy = 100. * correct / total

    try:
        auc_score = roc_auc_score(all_targets, all_preds)
    except:
        auc_score = 0.0

    return avg_loss, accuracy, auc_score

# =============================================================================
# è¯„ä¼°å‡½æ•°
# =============================================================================

def evaluate_model_optimized(model, test_loader, criterion, device):
    """ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°å‡½æ•°"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_targets = []
    all_scores = []
    
    inference_times = []
    
    print("ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(tqdm(test_loader, desc="è¯„ä¼°è¿›åº¦")):
            data, target = data.to(device), target.to(device)
            
            # è®°å½•æ¨ç†æ—¶é—´
            start_time = time.time()
            output, attention_weights = model(data)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output, target)
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            predictions = (output > 0.5).float()
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_scores.extend(output.cpu().numpy())
    
    avg_loss = total_loss / len(test_loader)
    avg_inference_time = np.mean(inference_times)
    total_inference_time = np.sum(inference_times)
    
    print(f"âœ… è¯„ä¼°å®Œæˆ")
    print(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time*1000:.2f} ms/batch")
    
    return {
        'loss': avg_loss,
        'predictions': np.array(all_predictions),
        'targets': np.array(all_targets),
        'scores': np.array(all_scores),
        'avg_inference_time': avg_inference_time,
        'total_inference_time': total_inference_time
    }

def calculate_comprehensive_metrics(predictions, targets, scores):
    """è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(targets, predictions)
    balanced_acc = balanced_accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, zero_division=0)
    recall = recall_score(targets, predictions, zero_division=0)
    f1 = f1_score(targets, predictions, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(targets, predictions)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
    
    # ç‰¹å¼‚æ€§å’Œè´Ÿé¢„æµ‹å€¼
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0
    
    # AUCæŒ‡æ ‡
    try:
        auc_roc = roc_auc_score(targets, scores)
    except:
        auc_roc = 0.0
    
    try:
        precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)
        auc_pr = auc(recall_curve, precision_curve)
    except:
        auc_pr = 0.0
    
    return {
        'accuracy': accuracy,
        'balanced_accuracy': balanced_acc,
        'precision': precision,
        'recall': recall,
        'specificity': specificity,
        'f1': f1,
        'auc_roc': auc_roc,
        'auc_pr': auc_pr,
        'npv': npv,
        'confusion_matrix': cm,
        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp
    }

def plot_enhanced_confusion_matrix(cm, save_path):
    """ç»˜åˆ¶å¢å¼ºçš„æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(10, 8))
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ ‡ç­¾
    labels = np.array([[
        f'{cm[i,j]}\n({cm_percent[i,j]:.1f}%)' 
        for j in range(cm.shape[1])
    ] for i in range(cm.shape[0])])
    
    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', 
                xticklabels=['çœŸå®', 'ä¼ªé€ '],
                yticklabels=['çœŸå®', 'ä¼ªé€ '],
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    
    plt.title('å¢å¼ºæ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    stats_text = f'å‡†ç¡®ç‡: {accuracy:.3f}\nç²¾ç¡®ç‡: {precision:.3f}\nå¬å›ç‡: {recall:.3f}\nF1åˆ†æ•°: {f1:.3f}'
    plt.text(2.1, 0.5, stats_text, fontsize=10, 
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")

def plot_roc_pr_curves(targets, scores, save_path):
    """ç»˜åˆ¶ROCå’ŒPRæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(targets, scores)
    roc_auc = auc(fpr, tpr)
    
    ax1.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('å‡æ­£ç‡')
    ax1.set_ylabel('çœŸæ­£ç‡')
    ax1.set_title('ROCæ›²çº¿')
    ax1.legend(loc='lower right')
    ax1.grid(True, alpha=0.3)
    
    # PRæ›²çº¿
    precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)
    pr_auc = auc(recall_curve, precision_curve)
    
    ax2.plot(recall_curve, precision_curve, color='darkgreen', lw=2,
             label=f'PRæ›²çº¿ (AUC = {pr_auc:.4f})')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('å¬å›ç‡')
    ax2.set_ylabel('ç²¾ç¡®ç‡')
    ax2.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
    ax2.legend(loc='lower left')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ROC/PRæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹"""
    print("ğŸš€ æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹å¯åŠ¨...")
    print("=" * 60)
    
    # å¦‚æœéœ€è¦å¤„ç†æ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
    if not os.path.exists('./data/train.csv'):
        print("ğŸ“ å¼€å§‹æ•°æ®å¤„ç†...")
        data_list = process_videos_simple(BASE_DATA_DIR, max_videos_per_class=50, max_frames=24)
        
        if len(data_list) == 0:
            print("âŒ æœªæ‰¾åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„")
            return
        
        train_data, val_data, test_data = create_dataset_split(data_list)
        
        # ä¿å­˜æ•°æ®é›†
        save_dataset_to_csv(train_data, './data/train.csv')
        save_dataset_to_csv(val_data, './data/val.csv')
        save_dataset_to_csv(test_data, './data/test.csv')
        
        print(f"è®­ç»ƒé›†: {len(train_data)} ä¸ªæ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_data)} ä¸ªæ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(test_data)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # ä½¿ç”¨CSVæ–‡ä»¶åˆ›å»ºæ•°æ®é›†ï¼ˆèŠ‚çœå†…å­˜ï¼‰
    train_transform = get_transforms('train')
    val_transform = get_transforms('val')
    
    train_dataset = DeepfakeVideoDataset('./data/train.csv', transform=train_transform)
    val_dataset = DeepfakeVideoDataset('./data/val.csv', transform=val_transform)
    test_dataset = DeepfakeVideoDataset('./data/test.csv', transform=val_transform)
    
    batch_size = 8 if torch.cuda.is_available() else 4
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    print(f"æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
    model = OptimizedDeepfakeDetector(
        backbone='resnet18',  # ä½¿ç”¨æ›´è½»é‡çš„backbone
        hidden_dim=256,
        num_layers=1,
        dropout=0.3,
        use_attention=True
    ).to(device)
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = FocalLoss(alpha=1, gamma=2)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)
    early_stopping = EarlyStopping(patience=5)
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = GradScaler() if torch.cuda.is_available() else None
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    num_epochs = 20
    best_val_acc = 0
    
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print("-" * 40)
        
        # è®­ç»ƒ
        train_loss, train_acc, train_auc = train_epoch(
            model, train_loader, criterion, optimizer, device, scaler
        )
        
        # éªŒè¯
        val_loss, val_acc, val_auc = validate_epoch(
            model, val_loader, criterion, device
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        print(f"è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.2f}%, AUC: {train_auc:.4f}")
        print(f"éªŒè¯ - æŸå¤±: {val_loss:.4f}, å‡†ç¡®ç‡: {val_acc:.2f}%, AUC: {val_auc:.4f}")
        print(f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_acc': best_val_acc,
                'val_loss': val_loss
            }, './models/best_model.pth')
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%)")
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss, model):
            print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    print("\nğŸ“Š åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    checkpoint = torch.load('./models/best_model.pth', map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # è¯„ä¼°æ¨¡å‹
    print("\nğŸ” å¼€å§‹æœ€ç»ˆè¯„ä¼°...")
    eval_results = evaluate_model_optimized(model, test_loader, criterion, device)
    metrics = calculate_comprehensive_metrics(
        eval_results['predictions'], 
        eval_results['targets'], 
        eval_results['scores']
    )
    
    # æ‰“å°ç»“æœ
    print("\nğŸ“ˆ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
    print(f"æµ‹è¯•æŸå¤±: {eval_results['loss']:.4f}")
    print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    print(f"å¹³è¡¡å‡†ç¡®ç‡: {metrics['balanced_accuracy']:.4f}")
    print(f"ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
    print(f"å¬å›ç‡: {metrics['recall']:.4f}")
    print(f"F1åˆ†æ•°: {metrics['f1']:.4f}")
    print(f"AUC-ROC: {metrics['auc_roc']:.4f}")
    print(f"AUC-PR: {metrics['auc_pr']:.4f}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ“Š ç”Ÿæˆè¯„ä¼°å›¾è¡¨...")
    plot_enhanced_confusion_matrix(
        metrics['confusion_matrix'], 
        './results/evaluation/confusion_matrix.png'
    )
    plot_roc_pr_curves(
        eval_results['targets'], 
        eval_results['scores'], 
        './results/evaluation/roc_pr_curves.png'
    )
    
    # ä¿å­˜ç»“æœ
    results = {
        'timestamp': datetime.now().isoformat(),
        'model_config': {
            'backbone': 'resnet18',
            'hidden_dim': 256,
            'num_layers': 1,
            'dropout': 0.3,
            'use_attention': True
        },
        'training_config': {
            'num_epochs': num_epochs,
            'batch_size': batch_size,
            'learning_rate': 1e-4,
            'weight_decay': 1e-4
        },
        'metrics': {
            'test_loss': float(eval_results['loss']),
            'accuracy': float(metrics['accuracy']),
            'balanced_accuracy': float(metrics['balanced_accuracy']),
            'precision': float(metrics['precision']),
            'recall': float(metrics['recall']),
            'f1_score': float(metrics['f1']),
            'auc_roc': float(metrics['auc_roc']),
            'auc_pr': float(metrics['auc_pr']),
            'specificity': float(metrics['specificity']),
            'npv': float(metrics['npv'])
        },
        'performance': {
            'avg_inference_time_ms': eval_results['avg_inference_time'] * 1000,
            'total_inference_time': eval_results['total_inference_time']
        }
    }
    
    with open('./results/evaluation/final_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("\nğŸ‰ è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")
    print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ° ./results/evaluation/ ç›®å½•")
    print("=" * 60)

if __name__ == "__main__":
    main()