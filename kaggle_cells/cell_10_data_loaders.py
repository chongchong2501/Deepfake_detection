# Cell 10: æ•°æ®åŠ è½½å™¨åˆ›å»º
def create_data_loaders(train_df, val_df, test_df, batch_size=16, num_workers=2, 
                       balance_classes=True, oversample_minority=True):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨ - å¢å¼ºç±»åˆ«å¹³è¡¡
    
    Args:
        balance_classes: æ˜¯å¦å¹³è¡¡ç±»åˆ«
        oversample_minority: æ˜¯å¦å¯¹å°‘æ•°ç±»è¿›è¡Œè¿‡é‡‡æ ·
    """
    print("ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    
    # è·å–æ•°æ®å˜æ¢ - ä¿®å¤ç‰ˆæœ¬
    train_transform = get_transforms(mode='train')
    val_transform = get_transforms(mode='val')
    
    # åˆ›å»ºæ•°æ®é›† - å¯ç”¨å¤šæ¨¡æ€ç‰¹å¾æå–
    train_dataset = DeepfakeVideoDataset(
        data_list=train_df.to_dict('records'), 
        transform=train_transform,
        extract_fourier=True,  # å¯ç”¨é¢‘åŸŸç‰¹å¾
        extract_compression=True  # å¯ç”¨å‹ç¼©ç‰¹å¾
    )
    val_dataset = DeepfakeVideoDataset(
        data_list=val_df.to_dict('records'), 
        transform=val_transform,
        extract_fourier=True,  # å¯ç”¨é¢‘åŸŸç‰¹å¾
        extract_compression=True  # å¯ç”¨å‹ç¼©ç‰¹å¾
    )
    test_dataset = DeepfakeVideoDataset(
        data_list=test_df.to_dict('records'), 
        transform=val_transform,
        extract_fourier=True,  # å¯ç”¨é¢‘åŸŸç‰¹å¾
        extract_compression=True  # å¯ç”¨å‹ç¼©ç‰¹å¾
    )
    
    # åˆ†æç±»åˆ«åˆ†å¸ƒ
    train_labels = train_df['label'].values
    real_count = np.sum(train_labels == 0)
    fake_count = np.sum(train_labels == 1)
    total_count = len(train_labels)
    
    print(f"ğŸ“ˆ è®­ç»ƒæ•°æ®åˆ†å¸ƒ:")
    print(f"   - çœŸå®è§†é¢‘: {real_count} ({real_count/total_count*100:.1f}%)")
    print(f"   - ä¼ªé€ è§†é¢‘: {fake_count} ({fake_count/total_count*100:.1f}%)")
    print(f"   - ä¸å¹³è¡¡æ¯”ä¾‹: {max(real_count, fake_count)/min(real_count, fake_count):.2f}:1")
    
    # åˆ›å»ºé‡‡æ ·å™¨
    train_sampler = None
    if balance_classes and abs(real_count - fake_count) > total_count * 0.1:  # å¦‚æœä¸å¹³è¡¡è¶…è¿‡10%
        print("âš–ï¸ æ£€æµ‹åˆ°ç±»åˆ«ä¸å¹³è¡¡ï¼Œåº”ç”¨å¹³è¡¡é‡‡æ ·...")
        
        if oversample_minority:
            # è¿‡é‡‡æ ·å°‘æ•°ç±»
            from torch.utils.data import WeightedRandomSampler
            class_counts = [real_count, fake_count]
            class_weights = [1.0 / count for count in class_counts]
            sample_weights = [class_weights[label] for label in train_labels]
            
            train_sampler = WeightedRandomSampler(
                weights=sample_weights,
                num_samples=len(sample_weights),
                replacement=True
            )
            print(f"   âœ… ä½¿ç”¨åŠ æƒéšæœºé‡‡æ ·å™¨")
        else:
            # ä¸‹é‡‡æ ·å¤šæ•°ç±»
            from torch.utils.data import Subset
            
            real_indices = np.where(train_labels == 0)[0]
            fake_indices = np.where(train_labels == 1)[0]
            
            min_count = min(real_count, fake_count)
            balanced_real_indices = np.random.choice(real_indices, min_count, replace=False)
            balanced_fake_indices = np.random.choice(fake_indices, min_count, replace=False)
            
            balanced_indices = np.concatenate([balanced_real_indices, balanced_fake_indices])
            np.random.shuffle(balanced_indices)
            
            train_dataset = Subset(train_dataset, balanced_indices)
            print(f"   âœ… ä¸‹é‡‡æ ·åˆ°å¹³è¡¡æ•°æ®é›†: {len(balanced_indices)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä¿®å¤å¤šè¿›ç¨‹åºåˆ—åŒ–é—®é¢˜
    # åœ¨Jupyter/Kaggleç¯å¢ƒä¸­ï¼Œä½¿ç”¨num_workers=0é¿å…åºåˆ—åŒ–é—®é¢˜
    safe_num_workers = 0  # å¼ºåˆ¶ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        sampler=train_sampler,
        shuffle=(train_sampler is None),  # å¦‚æœæœ‰é‡‡æ ·å™¨å°±ä¸shuffle
        num_workers=safe_num_workers,
        pin_memory=True,
        drop_last=True,  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
        persistent_workers=False  # å•è¿›ç¨‹æ¨¡å¼ä¸‹ä¸éœ€è¦
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=safe_num_workers,
        pin_memory=True,
        persistent_workers=False  # å•è¿›ç¨‹æ¨¡å¼ä¸‹ä¸éœ€è¦
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=safe_num_workers,
        pin_memory=True,
        persistent_workers=False  # å•è¿›ç¨‹æ¨¡å¼ä¸‹ä¸éœ€è¦
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    print(f"   - è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"   - éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    print(f"   - æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    print(f"   - å·¥ä½œè¿›ç¨‹: {safe_num_workers} (å•è¿›ç¨‹æ¨¡å¼ï¼Œé¿å…åºåˆ—åŒ–é—®é¢˜)")
    
    return train_loader, val_loader, test_loader

# æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åŠ è½½æ•°æ®
if os.path.exists('./data/train.csv') and os.path.exists('./data/val.csv') and os.path.exists('./data/test.csv'):
    print("ğŸ“Š åŠ è½½ç°æœ‰æ•°æ®é›†...")
    train_df = pd.read_csv('./data/train.csv')
    val_df = pd.read_csv('./data/val.csv')
    test_df = pd.read_csv('./data/test.csv')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨å®ä¾‹ - ä½¿ç”¨å…¨å±€é…ç½®å‚æ•°ï¼Œä½†å¼ºåˆ¶å•è¿›ç¨‹æ¨¡å¼
    print("ğŸ”„ æ­£åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_data_loaders(
        train_df=train_df, 
        val_df=val_df, 
        test_df=test_df,
        batch_size=2,  # 
        num_workers=0,  # å¼ºåˆ¶ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼é¿å…åºåˆ—åŒ–é—®é¢˜
        balance_classes=True,
        oversample_minority=True
    )
    
    print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼")
else:
    print("âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®å‡†å¤‡æ­¥éª¤ï¼ˆcell_09ï¼‰")
    train_loader = val_loader = test_loader = None