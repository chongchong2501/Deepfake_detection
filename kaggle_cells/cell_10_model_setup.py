# Cell 10: æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒé…ç½® 
print("ğŸ¤– åˆ›å»ºå’Œé…ç½®æ¨¡å‹...")

# è®­ç»ƒé…ç½®å‚æ•° - ç®€åŒ–ä¼˜åŒ–ç‰ˆæœ¬
batch_size = 8  # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§

# åˆ›å»ºç®€åŒ–æ¨¡å‹ - ä¸“æ³¨åŸºç¡€ç‰¹å¾
model = OptimizedDeepfakeDetector(
    num_classes=1,
    dropout_rate=0.1,  # å¤§å¹…é™ä½dropoutç‡ï¼Œå‡å°‘æ­£åˆ™åŒ–
    use_attention=False,  # ç¦ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œç®€åŒ–æ¨¡å‹
    use_multimodal=False,  # ç¦ç”¨å¤šæ¨¡æ€ç‰¹å¾èåˆ
    ensemble_mode=False   # å•æ¨¡å‹æ¨¡å¼
).to(device)

# å¤šGPUå¹¶è¡Œæ”¯æŒ
if torch.cuda.is_available() and torch.cuda.device_count() > 1:
    print(f"ğŸš€ å¯ç”¨å¤šGPUå¹¶è¡Œè®­ç»ƒï¼Œä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPU")
    model = nn.DataParallel(model)
    # è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥å……åˆ†åˆ©ç”¨å¤šGPU
    effective_batch_size = batch_size * torch.cuda.device_count()
    print(f"ğŸ“¦ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size} (å•GPU: {batch_size})")
else:
    print("ğŸ“ å•GPUè®­ç»ƒæ¨¡å¼")

print(f"âœ… æ¨¡å‹å·²åˆ›å»ºå¹¶ç§»åŠ¨åˆ° {device}")
print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# ä¼˜åŒ–GPUå†…å­˜é…ç½® - æ›´ä¿å®ˆçš„å†…å­˜ä½¿ç”¨é¿å…OOM
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.6)  # é™ä½åˆ°60%é¿å…å†…å­˜æº¢å‡º
    torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print(f"ğŸ”§ å†…å­˜ä½¿ç”¨é™åˆ¶: 60%")

# æŸå¤±å‡½æ•° - ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡
# è®¡ç®—ç±»åˆ«æƒé‡ - ä¿®å¤ç‰ˆæœ¬
if 'train_loader' in globals() and train_loader is not None:
    # ä»train_loaderè·å–æ•°æ®é›†
    train_dataset = train_loader.dataset
    
    if hasattr(train_dataset, 'real_count') and hasattr(train_dataset, 'fake_count'):
        # ä½¿ç”¨é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯
        real_count = train_dataset.real_count
        fake_count = train_dataset.fake_count
    else:
        # å›é€€æ–¹æ¡ˆï¼šæ‰‹åŠ¨è®¡ç®—
        if hasattr(train_dataset, 'data_list') and train_dataset.data_list is not None:
            real_count = sum(1 for item in train_dataset.data_list if item['label'] == 0)
            fake_count = sum(1 for item in train_dataset.data_list if item['label'] == 1)
        elif hasattr(train_dataset, 'df') and train_dataset.df is not None:
            real_count = len(train_dataset.df[train_dataset.df['label'] == 0])
            fake_count = len(train_dataset.df[train_dataset.df['label'] == 1])
        else:
            # é»˜è®¤å€¼
            real_count = 1
            fake_count = 1
            print("âš ï¸ æ— æ³•è·å–ç±»åˆ«åˆ†å¸ƒï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
else:
    # å¦‚æœæ²¡æœ‰train_loaderï¼Œä½¿ç”¨é»˜è®¤å€¼
    real_count = 1
    fake_count = 1
    print("âš ï¸ train_loaderæœªå®šä¹‰ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«æƒé‡")

# ç¡®ä¿è®¡æ•°ä¸ä¸ºé›¶
real_count = max(real_count, 1)
fake_count = max(fake_count, 1)

pos_weight = torch.tensor([real_count / fake_count], device=device)

print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ - çœŸå®: {real_count}, ä¼ªé€ : {fake_count}")
print(f"âš–ï¸ æ­£æ ·æœ¬æƒé‡: {pos_weight.item():.2f}")

# ä½¿ç”¨FocalLosså¤„ç†ç±»åˆ«ä¸å¹³è¡¡ - ä¼˜åŒ–ç‰ˆæœ¬
criterion = FocalLoss(
    alpha=0.75,  # å¢åŠ alphaå€¼ï¼Œæ›´å¤šå…³æ³¨çœŸå®è§†é¢‘(å°‘æ•°ç±»)
    gamma=1.5,   # é™ä½gammaå€¼ï¼Œå‡å°‘å¯¹å›°éš¾æ ·æœ¬çš„è¿‡åº¦å…³æ³¨
    pos_weight=pos_weight,
    reduction='mean'
)

# ä¼˜åŒ–å™¨é…ç½® - æé«˜å­¦ä¹ ç‡
optimizer = optim.AdamW(
    model.parameters(),
    lr=5e-4,  # æé«˜å­¦ä¹ ç‡5å€ï¼ŒåŠ å¿«æ”¶æ•›
    weight_decay=0.005,  # é™ä½æƒé‡è¡°å‡ï¼Œå‡å°‘è¿‡åº¦æ­£åˆ™åŒ–
    betas=(0.9, 0.999),
    eps=1e-8
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ›´æ¿€è¿›çš„å­¦ä¹ ç‡ç­–ç•¥
scheduler = CosineAnnealingWarmRestarts(
    optimizer,
    T_0=5,  # å¢åŠ é‡å¯å‘¨æœŸï¼Œè®©æ¨¡å‹æœ‰æ›´å¤šæ—¶é—´å­¦ä¹ 
    T_mult=1,  # ä¿æŒå‘¨æœŸä¸å˜
    eta_min=1e-5  # æé«˜æœ€å°å­¦ä¹ ç‡
)

# æ—©åœæœºåˆ¶ - æ›´å®½æ¾çš„ç›‘æ§
early_stopping = EarlyStopping(
    patience=10,  # å¢åŠ è€å¿ƒå€¼ï¼Œç»™æ¨¡å‹æ›´å¤šå­¦ä¹ æ—¶é—´
    min_delta=0.005,  # é™ä½æœ€å°æ”¹è¿›é˜ˆå€¼
    restore_best_weights=True
)

# æ··åˆç²¾åº¦è®­ç»ƒ - æš‚æ—¶ç¦ç”¨ä»¥è§£å†³NaNé—®é¢˜
use_amp = False  # å¼ºåˆ¶ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š
scaler = None
print("ğŸ“ ä½¿ç”¨FP32è®­ç»ƒ (è§£å†³NaNé—®é¢˜)")

# è®­ç»ƒé…ç½® - ç®€åŒ–ä¼˜åŒ–ç‰ˆæœ¬
num_epochs = 50  # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œç»™æ¨¡å‹æ›´å¤šå­¦ä¹ æœºä¼š
print(f"ğŸ¯ ç®€åŒ–è®­ç»ƒé…ç½®:")
print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"  - åˆå§‹å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
print(f"  - æƒé‡è¡°å‡: {optimizer.param_groups[0]['weight_decay']:.3f}")
print(f"  - æ—©åœè€å¿ƒå€¼: {early_stopping.patience}")
print(f"  - æ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}")
print(f"  - æ¨¡å‹å¤æ‚åº¦: ç®€åŒ–ç‰ˆæœ¬ (æ— æ³¨æ„åŠ›æœºåˆ¶)")

print("âœ… ç®€åŒ–æ¨¡å‹å’Œè®­ç»ƒé…ç½®å®Œæˆ")