# Cell 11: æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒé…ç½® - Kaggle T4 GPUä¼˜åŒ–ç‰ˆæœ¬

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from torch.cuda.amp import GradScaler

print("ğŸ¤– åˆ›å»ºå’Œé…ç½®æ¨¡å‹...")

# åˆ›å»ºæ¨¡å‹ - é’ˆå¯¹Kaggle T4 GPUä¼˜åŒ–
model = OptimizedDeepfakeDetector(
    num_classes=1,
    dropout_rate=0.3,
    use_attention=True,
    use_multimodal=True,  # å¯ç”¨å¤šæ¨¡æ€ç‰¹å¾èåˆ
    ensemble_mode=False   # å•æ¨¡å‹æ¨¡å¼
).to(device)

print(f"âœ… æ¨¡å‹å·²åˆ›å»ºå¹¶ç§»åŠ¨åˆ° {device}")
print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# ä¼˜åŒ–GPUå†…å­˜é…ç½® - åŒT4 GPUé…ç½®
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.8)  # åŒT4å¯ä»¥ä½¿ç”¨æ›´å¤šå†…å­˜
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

# æŸå¤±å‡½æ•° - ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡
# è®¡ç®—ç±»åˆ«æƒé‡ - ä¿®å¤ç‰ˆæœ¬
if hasattr(train_dataset, 'real_count') and hasattr(train_dataset, 'fake_count'):
    # ä½¿ç”¨é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯
    real_count = train_dataset.real_count
    fake_count = train_dataset.fake_count
else:
    # å›é€€æ–¹æ¡ˆï¼šæ‰‹åŠ¨è®¡ç®—
    if hasattr(train_dataset, 'data_list') and train_dataset.data_list is not None:
        real_count = sum(1 for item in train_dataset.data_list if item['label'] == 0)
        fake_count = sum(1 for item in train_dataset.data_list if item['label'] == 1)
    elif hasattr(train_dataset, 'df') and train_dataset.df is not None:
        real_count = len(train_dataset.df[train_dataset.df['label'] == 0])
        fake_count = len(train_dataset.df[train_dataset.df['label'] == 1])
    else:
        # é»˜è®¤å€¼
        real_count = 1
        fake_count = 1
        print("âš ï¸ æ— æ³•è·å–ç±»åˆ«åˆ†å¸ƒï¼Œä½¿ç”¨é»˜è®¤æƒé‡")

# ç¡®ä¿è®¡æ•°ä¸ä¸ºé›¶
real_count = max(real_count, 1)
fake_count = max(fake_count, 1)

pos_weight = torch.tensor([real_count / fake_count], device=device)

print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ - çœŸå®: {real_count}, ä¼ªé€ : {fake_count}")
print(f"âš–ï¸ æ­£æ ·æœ¬æƒé‡: {pos_weight.item():.2f}")

# ä½¿ç”¨FocalLosså¤„ç†ç±»åˆ«ä¸å¹³è¡¡
criterion = FocalLoss(
    alpha=0.25,
    gamma=2.0,  # é™ä½gammaå€¼ï¼Œå‡å°‘å¯¹å›°éš¾æ ·æœ¬çš„è¿‡åº¦å…³æ³¨
    pos_weight=pos_weight,
    reduction='mean'
)

# ä¼˜åŒ–å™¨é…ç½® - ä½¿ç”¨AdamWå’Œå­¦ä¹ ç‡è°ƒåº¦
optimizer = optim.AdamW(
    model.parameters(),
    lr=2e-4,  # æé«˜åˆå§‹å­¦ä¹ ç‡
    weight_decay=1e-4,  # å¢åŠ æƒé‡è¡°å‡
    betas=(0.9, 0.999),
    eps=1e-8
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½¿ç”¨ä½™å¼¦é€€ç«
scheduler = CosineAnnealingWarmRestarts(
    optimizer,
    T_0=10,  # åˆå§‹é‡å¯å‘¨æœŸ
    T_mult=2,  # å‘¨æœŸå€å¢å› å­
    eta_min=1e-6  # æœ€å°å­¦ä¹ ç‡
)

# æ—©åœæœºåˆ¶ - åŒT4 GPUé…ç½®
early_stopping = EarlyStopping(
    patience=8,  # é€‚ä¸­çš„è€å¿ƒå€¼ï¼Œé€‚åˆåŒT4è®­ç»ƒ
    min_delta=0.001,
    restore_best_weights=True
)

# æ··åˆç²¾åº¦è®­ç»ƒ - ä»…åœ¨æ”¯æŒçš„GPUä¸Šå¯ç”¨
use_amp = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7
if use_amp:
    scaler = GradScaler()
    print("âœ… å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
else:
    scaler = None
    print("ğŸ“ ä½¿ç”¨FP32è®­ç»ƒ (å…¼å®¹æ€§æ¨¡å¼)")

# è®­ç»ƒé…ç½® - åŒT4 GPUä¼˜åŒ–
num_epochs = 15  # é€‚ä¸­çš„è®­ç»ƒè½®æ•°ï¼Œé€‚åˆåŒT4é…ç½®
print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"  - åˆå§‹å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
print(f"  - æƒé‡è¡°å‡: {optimizer.param_groups[0]['weight_decay']:.2e}")
print(f"  - æ—©åœè€å¿ƒå€¼: {early_stopping.patience}")
print(f"  - æ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}")

print("âœ… æ¨¡å‹å’Œè®­ç»ƒé…ç½®å®Œæˆ")