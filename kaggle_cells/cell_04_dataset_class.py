# Cell 4: æ•°æ®é›†ç±»å®šä¹‰

class DeepfakeVideoDataset(Dataset):
    """æ·±åº¦ä¼ªé€ è§†é¢‘æ•°æ®é›†ç±» - é›†æˆMTCNNå’Œå¤šæ¨¡æ€ç‰¹å¾"""
    
    def __init__(self, csv_file=None, data_list=None, transform=None, max_frames=16, 
                 gpu_preprocessing=False, cache_frames=False, use_mtcnn=True, 
                 extract_fourier=False, extract_compression=False):
        if csv_file is not None:
            try:
                self.df = pd.read_csv(csv_file)
                self.data_list = None
                print(f"âœ… æˆåŠŸåŠ è½½CSVæ–‡ä»¶: {csv_file}")
            except FileNotFoundError:
                print(f"âš ï¸ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}ï¼Œåˆ›å»ºç©ºæ•°æ®é›†")
                self.df = pd.DataFrame(columns=['video_path', 'label'])
                self.data_list = None
        elif data_list is not None:
            self.data_list = data_list
            self.df = None
        else:
            raise ValueError("å¿…é¡»æä¾›csv_fileæˆ–data_list")
            
        self.transform = transform
        self.max_frames = max_frames
        self.gpu_preprocessing = gpu_preprocessing and torch.cuda.is_available()
        self.cache_frames = cache_frames
        self.use_mtcnn = use_mtcnn and globals().get('MTCNN_AVAILABLE', False)
        self.extract_fourier = extract_fourier and globals().get('SCIPY_AVAILABLE', False)
        self.extract_compression = extract_compression
        
        # ä¼˜åŒ–ç¼“å­˜ç³»ç»Ÿ - ä½¿ç”¨LRUç¼“å­˜
        if cache_frames:
            from functools import lru_cache
            self.frame_cache = {}
            self.cache_hits = 0
            self.cache_misses = 0
            self.max_cache_size = 100  # é™åˆ¶ç¼“å­˜å¤§å°
        else:
            self.frame_cache = None
        
        # é¢„è®¡ç®—æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        self._compute_dataset_stats()
        
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {len(self)} ä¸ªæ ·æœ¬")
        if self.gpu_preprocessing:
            print("ğŸš€ å¯ç”¨GPUé¢„å¤„ç†")
        if self.use_mtcnn:
            print("ğŸ‘ï¸ å¯ç”¨MTCNNäººè„¸æ£€æµ‹")
        if self.extract_fourier:
            print("ğŸ“Š å¯ç”¨é¢‘åŸŸç‰¹å¾æå–")
        if self.extract_compression:
            print("ğŸ” å¯ç”¨å‹ç¼©ä¼ªå½±åˆ†æ")

    def _compute_dataset_stats(self):
        """é¢„è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.df is not None and len(self.df) > 0:
                self.real_count = len(self.df[self.df['label'] == 0])
                self.fake_count = len(self.df[self.df['label'] == 1])
            elif self.data_list is not None:
                self.real_count = sum(1 for item in self.data_list if item['label'] == 0)
                self.fake_count = sum(1 for item in self.data_list if item['label'] == 1)
            else:
                self.real_count = 0
                self.fake_count = 0
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—æ•°æ®ç»Ÿè®¡æ—¶å‡ºé”™: {e}")
            self.real_count = 0
            self.fake_count = 0
        
        print(f"ğŸ“Š æ•°æ®åˆ†å¸ƒ: çœŸå®={self.real_count}, ä¼ªé€ ={self.fake_count}")

    def __len__(self):
        if self.df is not None:
            return len(self.df)
        return len(self.data_list) if self.data_list else 0

    def __getitem__(self, idx):
        try:
            if self.data_list is not None:
                item = self.data_list[idx]
                video_path = item['video_path']
                frames = item.get('frames', None)
                label = item['label']
            else:
                row = self.df.iloc[idx]
                video_path = row['video_path']
                label = row['label']
                frames = None

            # å¦‚æœæ²¡æœ‰é¢„æå–çš„å¸§ï¼Œåˆ™å®æ—¶æå–
            if frames is None:
                try:
                    frames = extract_frames_memory_efficient(
                        video_path, 
                        max_frames=self.max_frames,
                        use_mtcnn=self.use_mtcnn
                    )
                except Exception as e:
                    print(f"âš ï¸ å®æ—¶å¸§æå–å¤±è´¥: {e}")
                    frames = self._create_default_frames()
            
            # å¦‚æœä»ç„¶æ²¡æœ‰å¸§ï¼Œåˆ›å»ºé»˜è®¤å¸§
            if not frames:
                frames = self._create_default_frames()
            
            # ç¡®ä¿å¸§æ•°ä¸€è‡´
            while len(frames) < self.max_frames:
                frames.append(frames[-1] if frames else np.zeros((224, 224, 3), dtype=np.uint8))
            frames = frames[:self.max_frames]

            # æå–å¤šæ¨¡æ€ç‰¹å¾
            additional_features = self._extract_additional_features(frames)

            # å§‹ç»ˆä½¿ç”¨CPUå¤„ç†è·¯å¾„ç¡®ä¿ç¨³å®šæ€§
            video_tensor = torch.stack([
                torch.from_numpy(frame).permute(2, 0, 1) for frame in frames
            ]).float() / 255.0  # (T, C, H, W)

            # åº”ç”¨å˜æ¢
            if self.transform:
                try:
                    transformed_frames = []
                    for frame in frames:  # ç›´æ¥ä½¿ç”¨åŸå§‹numpyå¸§
                        # ç¡®ä¿frameæ˜¯numpyæ•°ç»„ä¸”ä¸ºuint8ç±»å‹
                        if isinstance(frame, np.ndarray):
                            if frame.dtype != np.uint8:
                                frame = (frame * 255).astype(np.uint8) if frame.max() <= 1.0 else frame.astype(np.uint8)
                            # è½¬æ¢ä¸ºPIL Image
                            frame_pil = Image.fromarray(frame)
                            transformed_frame = self.transform(frame_pil)
                            
                            # æ£€æŸ¥å˜æ¢åæ˜¯å¦æœ‰NaNæˆ–æ— ç©·å€¼
                            if torch.isnan(transformed_frame).any() or torch.isinf(transformed_frame).any():
                                print(f"âš ï¸ æ£€æµ‹åˆ°NaN/Infå€¼ï¼Œä½¿ç”¨é»˜è®¤å¸§")
                                default_frame = np.zeros((224, 224, 3), dtype=np.uint8)
                                frame_pil = Image.fromarray(default_frame)
                                transformed_frame = self.transform(frame_pil)
                            
                            transformed_frames.append(transformed_frame)
                        else:
                            # å¦‚æœä¸æ˜¯numpyæ•°ç»„ï¼Œåˆ›å»ºé»˜è®¤å¸§
                            default_frame = np.zeros((224, 224, 3), dtype=np.uint8)
                            frame_pil = Image.fromarray(default_frame)
                            transformed_frame = self.transform(frame_pil)
                            transformed_frames.append(transformed_frame)
                    video_tensor = torch.stack(transformed_frames)
                    
                    # æœ€ç»ˆæ£€æŸ¥æ•´ä¸ªè§†é¢‘å¼ é‡
                    if torch.isnan(video_tensor).any() or torch.isinf(video_tensor).any():
                        print(f"âš ï¸ è§†é¢‘å¼ é‡åŒ…å«NaN/Infï¼Œä½¿ç”¨å®‰å…¨çš„é»˜è®¤å¤„ç†")
                        # å›é€€åˆ°åŸå§‹å¤„ç†æ–¹å¼
                        video_tensor = torch.stack([
                            torch.from_numpy(frame).permute(2, 0, 1) for frame in frames
                        ]).float() / 255.0
                        # æ‰‹åŠ¨åº”ç”¨æ ‡å‡†åŒ–ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ•°å€¼
                        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
                        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
                        video_tensor = torch.clamp((video_tensor - mean) / std, -10, 10)  # é™åˆ¶æ•°å€¼èŒƒå›´
                        
                except Exception as e:
                    print(f"âš ï¸ æ•°æ®å˜æ¢å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                    # å›é€€åˆ°åŸå§‹å¤„ç†æ–¹å¼ï¼Œä½†ä¸å†åº”ç”¨æ ‡å‡†åŒ–ï¼ˆå› ä¸ºtransformä¸­å·²åŒ…å«ï¼‰
                    video_tensor = torch.stack([
                        torch.from_numpy(frame).permute(2, 0, 1) for frame in frames
                    ]).float() / 255.0
                    # æ‰‹åŠ¨åº”ç”¨æ ‡å‡†åŒ–ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ•°å€¼
                    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
                    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
                    video_tensor = torch.clamp((video_tensor - mean) / std, -10, 10)  # é™åˆ¶æ•°å€¼èŒƒå›´
            else:
                # å¦‚æœæ²¡æœ‰å˜æ¢ï¼Œåº”ç”¨é»˜è®¤æ ‡å‡†åŒ–ï¼Œä½¿ç”¨æ›´å®‰å…¨çš„æ•°å€¼å¤„ç†
                mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
                video_tensor = torch.clamp((video_tensor - mean) / std, -10, 10)  # é™åˆ¶æ•°å€¼èŒƒå›´

            label_tensor = torch.tensor(label, dtype=torch.float32)
            
            # è¿”å›æ•°æ®å’Œé¢å¤–ç‰¹å¾
            if additional_features:
                return video_tensor, label_tensor, additional_features
            else:
                return video_tensor, label_tensor
            
        except Exception as e:
            print(f"âš ï¸ è·å–æ•°æ®é¡¹ {idx} æ—¶å‡ºé”™: {e}")
            # è¿”å›é»˜è®¤æ•°æ®
            return self._get_default_item()

    def _extract_additional_features(self, frames):
        """æå–é¢å¤–çš„å¤šæ¨¡æ€ç‰¹å¾"""
        features = {}
        
        try:
            if self.extract_fourier:
                # æå–é¢‘åŸŸç‰¹å¾ï¼ˆä½¿ç”¨ä¸­é—´å¸§ï¼‰
                mid_frame = frames[len(frames) // 2]
                try:
                    # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
                    if 'extract_fourier_features' in globals():
                        fourier_features = extract_fourier_features(mid_frame)
                        if fourier_features:
                            features['fourier'] = fourier_features
                    else:
                        # å¦‚æœå‡½æ•°ä¸å­˜åœ¨ï¼Œåˆ›å»ºç®€å•çš„é¢‘åŸŸç‰¹å¾æ›¿ä»£
                        # ä½¿ç”¨FFTè®¡ç®—ç®€å•çš„é¢‘åŸŸç»Ÿè®¡
                        gray_frame = np.mean(mid_frame, axis=2)
                        fft = np.fft.fft2(gray_frame)
                        fft_magnitude = np.abs(fft)
                        features['fourier'] = {
                            'mean_magnitude': float(np.mean(fft_magnitude)),
                            'std_magnitude': float(np.std(fft_magnitude)),
                            'max_magnitude': float(np.max(fft_magnitude))
                        }
                except Exception as e:
                    print(f"âš ï¸ é¢‘åŸŸç‰¹å¾æå–å¤±è´¥: {e}")
                    # è·³è¿‡é¢‘åŸŸç‰¹å¾
            
            if self.extract_compression:
                # æå–å‹ç¼©ä¼ªå½±ç‰¹å¾
                compression_features = []
                for frame in frames[::4]:  # æ¯4å¸§é‡‡æ ·ä¸€æ¬¡
                    try:
                        # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨
                        if 'analyze_compression_artifacts' in globals():
                            comp_feat = analyze_compression_artifacts(frame)
                            if comp_feat:
                                compression_features.append(comp_feat)
                        else:
                            # å¦‚æœå‡½æ•°ä¸å­˜åœ¨ï¼Œåˆ›å»ºç®€å•çš„å‹ç¼©ç‰¹å¾æ›¿ä»£
                            # ä½¿ç”¨DCTå’Œè¾¹ç¼˜æ£€æµ‹çš„ç®€å•å®ç°
                            gray_frame = np.mean(frame, axis=2)
                            # ç®€å•çš„DCTèƒ½é‡è®¡ç®—
                            dct_energy = float(np.var(gray_frame))
                            # ç®€å•çš„è¾¹ç¼˜å¯†åº¦è®¡ç®—
                            edges = np.abs(np.gradient(gray_frame.astype(float)))
                            edge_density = float(np.mean(edges[0]**2 + edges[1]**2))
                            
                            comp_feat = {
                                'dct_energy': dct_energy,
                                'edge_density': edge_density,
                                'dct_mean': dct_energy,
                                'high_freq_energy': dct_energy * 0.1
                            }
                            compression_features.append(comp_feat)
                    except Exception as e:
                        print(f"âš ï¸ å‹ç¼©ç‰¹å¾æå–å¤±è´¥: {e}")
                        continue
                
                if compression_features:
                    # èšåˆå‹ç¼©ç‰¹å¾ - ä½¿ç”¨ä¸æ¨¡å‹æœŸæœ›åŒ¹é…çš„é”®å
                    features['compression'] = {
                        'dct_mean': np.mean([f.get('dct_mean', f.get('dct_energy', 0)) for f in compression_features]),
                        'dct_std': np.std([f.get('dct_mean', f.get('dct_energy', 0)) for f in compression_features]),
                        'dct_energy': np.mean([f.get('dct_energy', 0) for f in compression_features]),
                        'high_freq_energy': np.mean([f.get('high_freq_energy', f.get('dct_energy', 0) * 0.1) for f in compression_features]),
                        'edge_density': np.mean([f.get('edge_density', 0) for f in compression_features])
                    }
            
            # è®¡ç®—æ—¶åºä¸€è‡´æ€§ç‰¹å¾
            if len(frames) > 1:
                temporal_features = self._compute_temporal_consistency(frames)
                if temporal_features:
                    features['temporal'] = temporal_features
            
            return features if features else None
            
        except Exception as e:
            print(f"âš ï¸ æå–é¢å¤–ç‰¹å¾å¤±è´¥: {e}")
            return None

    def _compute_temporal_consistency(self, frames):
        """è®¡ç®—æ—¶åºä¸€è‡´æ€§ç‰¹å¾"""
        try:
            # è®¡ç®—ç›¸é‚»å¸§ä¹‹é—´çš„å·®å¼‚
            frame_diffs = []
            for i in range(len(frames) - 1):
                diff = np.mean(np.abs(frames[i+1].astype(float) - frames[i].astype(float)))
                frame_diffs.append(diff)
            
            if frame_diffs:
                return {
                    'mean_frame_diff': np.mean(frame_diffs),
                    'std_frame_diff': np.std(frame_diffs),
                    'max_frame_diff': np.max(frame_diffs),
                    'temporal_smoothness': 1.0 / (1.0 + np.std(frame_diffs))
                }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—æ—¶åºç‰¹å¾å¤±è´¥: {e}")
            return None

    def _create_default_frames(self):
        """åˆ›å»ºé»˜è®¤å¸§æ•°æ®"""
        # åˆ›å»ºéšæœºå™ªå£°å¸§è€Œä¸æ˜¯å…¨é›¶å¸§ï¼Œä½¿è®­ç»ƒæ›´æœ‰æ„ä¹‰
        frames = []
        for i in range(self.max_frames):
            # åˆ›å»ºå¸¦æœ‰è½»å¾®éšæœºå™ªå£°çš„å¸§
            frame = np.random.randint(0, 50, (224, 224, 3), dtype=np.uint8)
            frames.append(frame)
        return frames

    def _get_default_item(self):
        """è·å–é»˜è®¤æ•°æ®é¡¹ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰"""
        frames = self._create_default_frames()
        video_tensor = torch.stack([
            torch.from_numpy(frame).permute(2, 0, 1) for frame in frames
        ]).float() / 255.0
        
        # æ ‡å‡†åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        video_tensor = (video_tensor - mean) / std
        
        label_tensor = torch.tensor(0.0, dtype=torch.float32)
        return video_tensor, label_tensor

    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.frame_cache is not None:
            total_requests = self.cache_hits + self.cache_misses
            hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
            return {
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'hit_rate': hit_rate,
                'cache_size': len(self.frame_cache)
            }
        return None

    def enable_ensemble_mode(self):
        """å¯ç”¨é›†æˆæ¨¡å¼ï¼Œæå–æ‰€æœ‰å¯ç”¨ç‰¹å¾"""
        self.extract_fourier = globals().get('SCIPY_AVAILABLE', False)
        self.extract_compression = True
        self.use_mtcnn = globals().get('MTCNN_AVAILABLE', False)
        print("ğŸ¯ å¯ç”¨é›†æˆæ¨¡å¼ï¼šæ‰€æœ‰ç‰¹å¾æå–å·²æ¿€æ´»")

print("âœ… æ•°æ®é›†ç±»å®šä¹‰å®Œæˆ")