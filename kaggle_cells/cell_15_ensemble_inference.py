# Cell 15: é›†æˆæ¨ç†è„šæœ¬ - æ•´åˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½ï¼ˆè®­ç»ƒå®Œæˆåä½¿ç”¨ï¼‰

import torch
import torch.nn as nn
import numpy as np
import cv2
from pathlib import Path
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class EnsembleDeepfakeDetector:
    """é›†æˆæ·±åº¦ä¼ªé€ æ£€æµ‹å™¨ - æ•´åˆæ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½"""
    
    def __init__(self, model_paths=None, device=None, use_mtcnn=True, 
                 extract_fourier=True, extract_compression=True):
        """
        åˆå§‹åŒ–é›†æˆæ£€æµ‹å™¨
        
        Args:
            model_paths: æ¨¡å‹æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            device: è®¡ç®—è®¾å¤‡
            use_mtcnn: æ˜¯å¦ä½¿ç”¨MTCNNäººè„¸æ£€æµ‹
            extract_fourier: æ˜¯å¦æå–é¢‘åŸŸç‰¹å¾
            extract_compression: æ˜¯å¦æå–å‹ç¼©ä¼ªå½±ç‰¹å¾
        """
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.use_mtcnn = use_mtcnn and globals().get('MTCNN_AVAILABLE', False)
        self.extract_fourier = extract_fourier and globals().get('SCIPY_AVAILABLE', False)
        self.extract_compression = extract_compression
        
        # åˆå§‹åŒ–MTCNN
        if self.use_mtcnn:
            try:
                from facenet_pytorch import MTCNN
                self.mtcnn = MTCNN(
                    image_size=224,
                    margin=20,
                    min_face_size=50,
                    thresholds=[0.6, 0.7, 0.7],
                    factor=0.709,
                    post_process=True,
                    device=self.device
                )
                print("âœ… MTCNNäººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ MTCNNåˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_mtcnn = False
                self.mtcnn = None
        else:
            self.mtcnn = None
        
        # åŠ è½½æ¨¡å‹
        self.models = []
        if model_paths:
            self.load_models(model_paths)
        
        print(f"ğŸš€ é›†æˆæ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - MTCNN: {'å¯ç”¨' if self.use_mtcnn else 'ç¦ç”¨'}")
        print(f"   - é¢‘åŸŸåˆ†æ: {'å¯ç”¨' if self.extract_fourier else 'ç¦ç”¨'}")
        print(f"   - å‹ç¼©åˆ†æ: {'å¯ç”¨' if self.extract_compression else 'ç¦ç”¨'}")

    def load_models(self, model_paths):
        """åŠ è½½å¤šä¸ªæ¨¡å‹ç”¨äºé›†æˆ"""
        from .cell_05_model_definition import OptimizedDeepfakeDetector
        
        for i, model_path in enumerate(model_paths):
            try:
                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model = OptimizedDeepfakeDetector(
                    use_attention=True,
                    use_multimodal=True,
                    ensemble_mode=True
                )
                
                # åŠ è½½æƒé‡
                if Path(model_path).exists():
                    checkpoint = torch.load(model_path, map_location=self.device)
                    if 'model_state_dict' in checkpoint:
                        model.load_state_dict(checkpoint['model_state_dict'])
                    else:
                        model.load_state_dict(checkpoint)
                    
                    model.to(self.device)
                    model.eval()
                    self.models.append(model)
                    print(f"âœ… æ¨¡å‹ {i+1} åŠ è½½æˆåŠŸ: {model_path}")
                else:
                    print(f"âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                    
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ¨¡å‹ {model_path} å¤±è´¥: {e}")
        
        print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(self.models)} ä¸ªæ¨¡å‹ç”¨äºé›†æˆ")

    def extract_frames_from_video(self, video_path, max_frames=16, target_size=(224, 224)):
        """ä»è§†é¢‘ä¸­æå–å¸§"""
        try:
            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # è®¡ç®—é‡‡æ ·é—´éš”
            if total_frames <= max_frames:
                frame_indices = list(range(total_frames))
            else:
                frame_indices = np.linspace(0, total_frames-1, max_frames, dtype=int)
            
            frames = []
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if ret:
                    # è½¬æ¢é¢œè‰²ç©ºé—´
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    
                    # MTCNNäººè„¸æ£€æµ‹å’Œè£å‰ª
                    if self.use_mtcnn and self.mtcnn is not None:
                        try:
                            # æ£€æµ‹äººè„¸
                            face = self.mtcnn(frame)
                            if face is not None:
                                # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è°ƒæ•´å¤§å°
                                face_np = face.permute(1, 2, 0).cpu().numpy()
                                face_np = (face_np * 255).astype(np.uint8)
                                frame = cv2.resize(face_np, target_size)
                            else:
                                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œä½¿ç”¨åŸå§‹å¸§
                                frame = cv2.resize(frame, target_size)
                        except Exception as e:
                            print(f"âš ï¸ MTCNNå¤„ç†å¤±è´¥: {e}")
                            frame = cv2.resize(frame, target_size)
                    else:
                        # ç›´æ¥è°ƒæ•´å¤§å°
                        frame = cv2.resize(frame, target_size)
                    
                    frames.append(frame)
            
            cap.release()
            
            # ç¡®ä¿å¸§æ•°ä¸€è‡´
            while len(frames) < max_frames:
                frames.append(frames[-1] if frames else np.zeros((*target_size, 3), dtype=np.uint8))
            
            return frames[:max_frames]
            
        except Exception as e:
            print(f"âš ï¸ æå–è§†é¢‘å¸§å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å¸§
            return [np.zeros((*target_size, 3), dtype=np.uint8) for _ in range(max_frames)]

    def extract_additional_features(self, frames):
        """æå–é¢å¤–çš„å¤šæ¨¡æ€ç‰¹å¾"""
        features = {}
        
        try:
            # é¢‘åŸŸç‰¹å¾æå–
            if self.extract_fourier and len(frames) > 0:
                mid_frame = frames[len(frames) // 2]
                from .cell_03_data_processing import extract_fourier_features
                fourier_features = extract_fourier_features(mid_frame)
                if fourier_features:
                    features['fourier'] = fourier_features
            
            # å‹ç¼©ä¼ªå½±ç‰¹å¾æå–
            if self.extract_compression and len(frames) > 0:
                compression_features = []
                for frame in frames[::4]:  # æ¯4å¸§é‡‡æ ·ä¸€æ¬¡
                    from .cell_03_data_processing import analyze_compression_artifacts
                    comp_feat = analyze_compression_artifacts(frame)
                    if comp_feat:
                        compression_features.append(comp_feat)
                
                if compression_features:
                    features['compression'] = {
                        'mean_dct_energy': np.mean([f['dct_energy'] for f in compression_features]),
                        'mean_edge_density': np.mean([f['edge_density'] for f in compression_features]),
                        'std_dct_energy': np.std([f['dct_energy'] for f in compression_features])
                    }
            
            # æ—¶åºä¸€è‡´æ€§ç‰¹å¾
            if len(frames) > 1:
                frame_diffs = []
                for i in range(len(frames) - 1):
                    diff = np.mean(np.abs(frames[i+1].astype(float) - frames[i].astype(float)))
                    frame_diffs.append(diff)
                
                if frame_diffs:
                    features['temporal'] = {
                        'mean_frame_diff': np.mean(frame_diffs),
                        'std_frame_diff': np.std(frame_diffs),
                        'max_frame_diff': np.max(frame_diffs),
                        'temporal_smoothness': 1.0 / (1.0 + np.std(frame_diffs))
                    }
            
            return features if features else None
            
        except Exception as e:
            print(f"âš ï¸ æå–é¢å¤–ç‰¹å¾å¤±è´¥: {e}")
            return None

    def preprocess_frames(self, frames):
        """é¢„å¤„ç†å¸§æ•°æ®"""
        try:
            # è½¬æ¢ä¸ºå¼ é‡
            video_tensor = torch.stack([
                torch.from_numpy(frame).permute(2, 0, 1) for frame in frames
            ]).float() / 255.0  # (T, C, H, W)
            
            # æ ‡å‡†åŒ–
            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
            video_tensor = (video_tensor - mean) / std
            
            # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
            video_tensor = video_tensor.unsqueeze(0)  # (1, T, C, H, W)
            
            return video_tensor.to(self.device)
            
        except Exception as e:
            print(f"âš ï¸ é¢„å¤„ç†å¤±è´¥: {e}")
            return None

    def predict_single_video(self, video_path, return_details=False):
        """å¯¹å•ä¸ªè§†é¢‘è¿›è¡Œé¢„æµ‹"""
        try:
            # æå–å¸§
            frames = self.extract_frames_from_video(video_path)
            if not frames:
                return {'error': 'æ— æ³•æå–è§†é¢‘å¸§'}
            
            # æå–é¢å¤–ç‰¹å¾
            additional_features = self.extract_additional_features(frames)
            
            # é¢„å¤„ç†
            video_tensor = self.preprocess_frames(frames)
            if video_tensor is None:
                return {'error': 'é¢„å¤„ç†å¤±è´¥'}
            
            # é›†æˆé¢„æµ‹
            predictions = []
            model_outputs = []
            
            with torch.no_grad():
                for i, model in enumerate(self.models):
                    try:
                        if additional_features:
                            # å¤„ç†é¢å¤–ç‰¹å¾ä¸ºå¼ é‡æ ¼å¼
                            processed_features = {}
                            for key, value in additional_features.items():
                                if isinstance(value, dict):
                                    # å°†å­—å…¸è½¬æ¢ä¸ºå¼ é‡
                                    if key == 'fourier':
                                        tensor_value = torch.tensor(
                                            list(value.values()), dtype=torch.float32
                                        ).unsqueeze(0).to(self.device)
                                    elif key == 'compression':
                                        tensor_value = torch.tensor([
                                            value['mean_dct_energy'],
                                            value['mean_edge_density'],
                                            value['std_dct_energy']
                                        ], dtype=torch.float32).unsqueeze(0).to(self.device)
                                    elif key == 'temporal':
                                        tensor_value = torch.tensor([
                                            value['mean_frame_diff'],
                                            value['std_frame_diff'],
                                            value['max_frame_diff'],
                                            value['temporal_smoothness']
                                        ], dtype=torch.float32).unsqueeze(0).to(self.device)
                                    else:
                                        tensor_value = torch.tensor(value).to(self.device)
                                    
                                    processed_features[key] = tensor_value
                            
                            outputs = model(video_tensor, processed_features)
                        else:
                            outputs = model(video_tensor)
                        
                        # å¤„ç†è¾“å‡º
                        if isinstance(outputs, dict):
                            # é›†æˆæ¨¡å¼ï¼Œä½¿ç”¨ensembleè¾“å‡º
                            pred = outputs['ensemble']
                            model_outputs.append({
                                'main': torch.sigmoid(outputs['main']).item(),
                                'spatial': torch.sigmoid(outputs['spatial']).item(),
                                'temporal': torch.sigmoid(outputs['temporal']).item(),
                                'ensemble': torch.sigmoid(outputs['ensemble']).item()
                            })
                        else:
                            pred = outputs
                            model_outputs.append({'prediction': torch.sigmoid(pred).item()})
                        
                        if pred.dim() > 1:
                            pred = pred.squeeze(-1)
                        
                        pred_prob = torch.sigmoid(pred).item()
                        predictions.append(pred_prob)
                        
                    except Exception as e:
                        print(f"âš ï¸ æ¨¡å‹ {i+1} é¢„æµ‹å¤±è´¥: {e}")
                        continue
            
            if not predictions:
                return {'error': 'æ‰€æœ‰æ¨¡å‹é¢„æµ‹å¤±è´¥'}
            
            # è®¡ç®—é›†æˆç»“æœ
            ensemble_prob = np.mean(predictions)
            ensemble_pred = 'FAKE' if ensemble_prob > 0.5 else 'REAL'
            confidence = max(ensemble_prob, 1 - ensemble_prob)
            
            result = {
                'prediction': ensemble_pred,
                'probability': ensemble_prob,
                'confidence': confidence,
                'individual_predictions': predictions,
                'num_models': len(predictions)
            }
            
            if return_details:
                result.update({
                    'model_outputs': model_outputs,
                    'additional_features': additional_features,
                    'num_frames': len(frames)
                })
            
            return result
            
        except Exception as e:
            return {'error': f'é¢„æµ‹å¤±è´¥: {str(e)}'}

    def predict_batch(self, video_paths, show_progress=True):
        """æ‰¹é‡é¢„æµ‹å¤šä¸ªè§†é¢‘"""
        results = {}
        
        iterator = tqdm(video_paths, desc="æ‰¹é‡é¢„æµ‹ä¸­") if show_progress else video_paths
        
        for video_path in iterator:
            try:
                result = self.predict_single_video(video_path)
                results[str(video_path)] = result
            except Exception as e:
                results[str(video_path)] = {'error': f'å¤„ç†å¤±è´¥: {str(e)}'}
        
        return results

    def visualize_prediction(self, video_path, save_path=None):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        try:
            # è·å–é¢„æµ‹ç»“æœ
            result = self.predict_single_video(video_path, return_details=True)
            
            if 'error' in result:
                print(f"âŒ é¢„æµ‹å¤±è´¥: {result['error']}")
                return
            
            # æå–å¸§ç”¨äºå¯è§†åŒ–
            frames = self.extract_frames_from_video(video_path, max_frames=8)
            
            # åˆ›å»ºå¯è§†åŒ–
            fig, axes = plt.subplots(2, 4, figsize=(16, 8))
            fig.suptitle(f'æ·±åº¦ä¼ªé€ æ£€æµ‹ç»“æœ\né¢„æµ‹: {result["prediction"]} (ç½®ä¿¡åº¦: {result["confidence"]:.3f})', 
                        fontsize=16, fontweight='bold')
            
            # æ˜¾ç¤ºå¸§
            for i, frame in enumerate(frames):
                row = i // 4
                col = i % 4
                axes[row, col].imshow(frame)
                axes[row, col].set_title(f'å¸§ {i+1}')
                axes[row, col].axis('off')
            
            # æ·»åŠ é¢„æµ‹ä¿¡æ¯
            info_text = f"""
            è§†é¢‘è·¯å¾„: {Path(video_path).name}
            é¢„æµ‹ç»“æœ: {result['prediction']}
            æ¦‚ç‡: {result['probability']:.4f}
            ç½®ä¿¡åº¦: {result['confidence']:.4f}
            ä½¿ç”¨æ¨¡å‹æ•°: {result['num_models']}
            æå–å¸§æ•°: {result.get('num_frames', 'N/A')}
            """
            
            fig.text(0.02, 0.02, info_text, fontsize=10, verticalalignment='bottom',
                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {save_path}")
            
            plt.show()
            
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")

    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            'num_models': len(self.models),
            'device': str(self.device),
            'use_mtcnn': self.use_mtcnn,
            'extract_fourier': self.extract_fourier,
            'extract_compression': self.extract_compression,
            'model_details': []
        }
        
        for i, model in enumerate(self.models):
            try:
                model_info = model.get_model_info()
                info['model_details'].append({
                    'model_id': i+1,
                    **model_info
                })
            except:
                info['model_details'].append({
                    'model_id': i+1,
                    'error': 'æ— æ³•è·å–æ¨¡å‹ä¿¡æ¯'
                })
        
        return info

# ä¾¿æ·å‡½æ•°
def create_ensemble_detector(model_paths=None, **kwargs):
    """åˆ›å»ºé›†æˆæ£€æµ‹å™¨çš„ä¾¿æ·å‡½æ•°"""
    return EnsembleDeepfakeDetector(model_paths=model_paths, **kwargs)

def quick_predict(video_path, model_paths=None):
    """å¿«é€Ÿé¢„æµ‹å•ä¸ªè§†é¢‘"""
    detector = create_ensemble_detector(model_paths)
    return detector.predict_single_video(video_path)

print("âœ… é›†æˆæ¨ç†è„šæœ¬å®šä¹‰å®Œæˆ")