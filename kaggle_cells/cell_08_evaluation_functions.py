# Cell 8: è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–

def evaluate_model_optimized(model, test_loader, criterion, device):
    """ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°å‡½æ•°"""
    model.eval()
    total_loss = 0.0
    all_predictions = []
    all_targets = []
    all_scores = []
    
    inference_times = []
    
    print("ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    with torch.no_grad():
        for batch_idx, (data, target) in enumerate(tqdm(test_loader, desc="è¯„ä¼°è¿›åº¦")):
            data, target = data.to(device), target.to(device)
            
            # è®°å½•æ¨ç†æ—¶é—´
            start_time = time.time()
            output, attention_weights = model(data)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(output, target)
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœ (åº”ç”¨ sigmoid è·å¾—æ¦‚ç‡)
            probs = torch.sigmoid(output)
            predictions = (probs > 0.5).float()
            all_predictions.extend(predictions.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_scores.extend(probs.cpu().numpy())
    
    avg_loss = total_loss / len(test_loader)
    avg_inference_time = np.mean(inference_times)
    total_inference_time = np.sum(inference_times)
    
    print(f"âœ… è¯„ä¼°å®Œæˆ")
    print(f"å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time*1000:.2f} ms/batch")
    
    return {
        'loss': avg_loss,
        'predictions': np.array(all_predictions),
        'targets': np.array(all_targets),
        'scores': np.array(all_scores),
        'avg_inference_time': avg_inference_time,
        'total_inference_time': total_inference_time
    }

def calculate_comprehensive_metrics(predictions, targets, scores):
    """è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(targets, predictions)
    balanced_acc = balanced_accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, zero_division=0)
    recall = recall_score(targets, predictions, zero_division=0)
    f1 = f1_score(targets, predictions, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(targets, predictions)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
    
    # ç‰¹å¼‚æ€§å’Œè´Ÿé¢„æµ‹å€¼
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0
    
    # AUCæŒ‡æ ‡
    try:
        auc_roc = roc_auc_score(targets, scores)
    except:
        auc_roc = 0.0
    
    try:
        precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)
        auc_pr = auc(recall_curve, precision_curve)
    except:
        auc_pr = 0.0
    
    return {
        'accuracy': accuracy,
        'balanced_accuracy': balanced_acc,
        'precision': precision,
        'recall': recall,
        'specificity': specificity,
        'f1': f1,
        'auc_roc': auc_roc,
        'auc_pr': auc_pr,
        'npv': npv,
        'confusion_matrix': cm,
        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp
    }

def plot_enhanced_confusion_matrix(cm, save_path):
    """ç»˜åˆ¶å¢å¼ºçš„æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(10, 8))
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ ‡ç­¾
    labels = np.array([[
        f'{cm[i,j]}\n({cm_percent[i,j]:.1f}%)' 
        for j in range(cm.shape[1])
    ] for i in range(cm.shape[0])])
    
    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', 
                xticklabels=['çœŸå®', 'ä¼ªé€ '],
                yticklabels=['çœŸå®', 'ä¼ªé€ '],
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    
    plt.title('å¢å¼ºæ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    tn, fp, fn, tp = cm.ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    stats_text = f'å‡†ç¡®ç‡: {accuracy:.3f}\nç²¾ç¡®ç‡: {precision:.3f}\nå¬å›ç‡: {recall:.3f}\nF1åˆ†æ•°: {f1:.3f}'
    plt.text(2.1, 0.5, stats_text, fontsize=10, 
             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")

def plot_roc_pr_curves(targets, scores, save_path):
    """ç»˜åˆ¶ROCå’ŒPRæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(targets, scores)
    roc_auc = auc(fpr, tpr)
    
    ax1.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('å‡æ­£ç‡')
    ax1.set_ylabel('çœŸæ­£ç‡')
    ax1.set_title('ROCæ›²çº¿')
    ax1.legend(loc='lower right')
    ax1.grid(True, alpha=0.3)
    
    # PRæ›²çº¿
    precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)
    pr_auc = auc(recall_curve, precision_curve)
    
    ax2.plot(recall_curve, precision_curve, color='darkgreen', lw=2,
             label=f'PRæ›²çº¿ (AUC = {pr_auc:.4f})')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('å¬å›ç‡')
    ax2.set_ylabel('ç²¾ç¡®ç‡')
    ax2.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
    ax2.legend(loc='lower left')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ROC/PRæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")

print("âœ… è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–å®šä¹‰å®Œæˆ")