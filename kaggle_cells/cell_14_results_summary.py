# Cell 14: ç»“æœä¿å­˜å’Œæ€»ç»“

print("ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...")
print("=" * 60)

# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
os.makedirs('./results', exist_ok=True)

# å‡†å¤‡ä¿å­˜çš„ç»“æœæ•°æ®
results_summary = {
    'experiment_info': {
        'timestamp': datetime.now().isoformat(),
        'model_architecture': 'OptimizedDeepfakeDetector',
        'backbone': 'resnet50',
        'total_epochs': len(train_history['train_loss']),
        'early_stopping': True
    },
    'dataset_info': {
        'train_samples': len(train_loader.dataset) if train_loader else 0,
        'val_samples': len(val_loader.dataset) if val_loader else 0,
        'test_samples': len(test_loader.dataset) if test_loader else 0,
        'batch_size': batch_size
    },
    'training_config': {
        'optimizer': 'AdamW',
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'loss_function': 'FocalLoss',
        'scheduler': 'OneCycleLR',
        'early_stopping_patience': 7
    },
    'final_metrics': {
        'test_loss': float(eval_results['loss']),
        'accuracy': float(metrics['accuracy']),
        'precision': float(metrics['precision']),
        'recall': float(metrics['recall']),
        'f1_score': float(metrics['f1']),
        'auc_roc': float(metrics['auc_roc'])
    },
    'confusion_matrix': {
        'tn': int(metrics['tn']),
        'fp': int(metrics['fp']),
        'fn': int(metrics['fn']),
        'tp': int(metrics['tp'])
    },
    'training_history': {
        'train_loss': [float(x) for x in train_history['train_loss']],
        'train_acc': [float(x) for x in train_history['train_acc']],
        'val_loss': [float(x) for x in train_history['val_loss']],
        'val_acc': [float(x) for x in train_history['val_acc']],
        'val_auc': [float(x) for x in train_history['val_auc']],
        'val_precision': [float(x) for x in train_history.get('val_precision', [])],
        'val_recall': [float(x) for x in train_history.get('val_recall', [])],
        'val_f1': [float(x) for x in train_history.get('val_f1', [])]
    },
    'class_specific_metrics': {
        'real_video_accuracy': float(real_accuracy),
        'fake_video_accuracy': float(fake_accuracy),
        'real_samples_count': int(real_samples),
        'fake_samples_count': int(fake_samples)
    }
}

# ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
results_file = './results/experiment_results.json'
with open(results_file, 'w', encoding='utf-8') as f:
    json.dump(results_summary, f, indent=2, ensure_ascii=False)

print(f"âœ… å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}")

# ä¿å­˜è®­ç»ƒå†å²åˆ°CSV
history_df = pd.DataFrame(train_history)
history_df.to_csv('./results/training_history.csv', index=False)
print("âœ… è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: ./results/training_history.csv")

# ä¿å­˜é¢„æµ‹ç»“æœ
predictions_df = pd.DataFrame({
    'true_label': eval_results['targets'],
    'predicted_label': eval_results['predictions'],
    'prediction_score': eval_results['scores']
})
predictions_df.to_csv('./results/test_predictions.csv', index=False)
print("âœ… æµ‹è¯•é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: ./results/test_predictions.csv")

# ç”Ÿæˆå®éªŒæŠ¥å‘Š
print("\nğŸ“‹ ç”Ÿæˆå®éªŒæŠ¥å‘Š...")
report = f"""
æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹å®éªŒæŠ¥å‘Š
{'='*50}

å®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ¨¡å‹æ¶æ„: OptimizedDeepfakeDetector (ResNet50 + LSTM + Attention)

æ•°æ®é›†ä¿¡æ¯:
- è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset) if train_loader else 0:,}
- éªŒè¯æ ·æœ¬: {len(val_loader.dataset) if val_loader else 0:,}
- æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset) if test_loader else 0:,}
- æ‰¹æ¬¡å¤§å°: {batch_size}

è®­ç»ƒé…ç½®:
- ä¼˜åŒ–å™¨: AdamW (lr=1e-4, weight_decay=1e-4)
- æŸå¤±å‡½æ•°: Focal Loss
- å­¦ä¹ ç‡è°ƒåº¦: OneCycleLR
- æ—©åœæœºåˆ¶: patience=7

æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:
- å‡†ç¡®ç‡: {metrics['accuracy']*100:.2f}%
- ç²¾ç¡®ç‡: {metrics['precision']:.4f}
- å¬å›ç‡: {metrics['recall']:.4f}
- F1åˆ†æ•°: {metrics['f1']:.4f}
- AUC-ROC: {metrics['auc_roc']:.4f}

æ··æ·†çŸ©é˜µ:
- çœŸè´Ÿä¾‹ (TN): {metrics['tn']}
- å‡æ­£ä¾‹ (FP): {metrics['fp']}
- å‡è´Ÿä¾‹ (FN): {metrics['fn']}
- çœŸæ­£ä¾‹ (TP): {metrics['tp']}

ç±»åˆ«ç‰¹å®šæ€§èƒ½:
- çœŸå®è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {real_accuracy*100:.2f}%
- ä¼ªé€ è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {fake_accuracy*100:.2f}%

è®­ç»ƒæ€»ç»“:
- è®­ç»ƒè½®æ•°: {len(train_history['train_loss'])}
- æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(train_history['val_acc']):.2f}%
- æœ€ä½³éªŒè¯AUC: {max(train_history['val_auc']):.4f}

æ–‡ä»¶è¾“å‡º:
- æ¨¡å‹æƒé‡: ./models/best_model.pth
- å®éªŒç»“æœ: ./results/experiment_results.json
- è®­ç»ƒå†å²: ./results/training_history.csv
- é¢„æµ‹ç»“æœ: ./results/test_predictions.csv

{'='*50}
å®éªŒå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

# ä¿å­˜æŠ¥å‘Š
with open('./results/experiment_report.txt', 'w', encoding='utf-8') as f:
    f.write(report)

print("âœ… å®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: ./results/experiment_report.txt")

# æ‰“å°æœ€ç»ˆæ€»ç»“
print("\n" + "="*60)
print("ğŸ‰ æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")
print("="*60)
print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {metrics['accuracy']*100:.2f}%")
print(f"ğŸ“Š AUC-ROCåˆ†æ•°: {metrics['auc_roc']:.4f}")
print(f"ğŸ“Š F1åˆ†æ•°: {metrics['f1']:.4f}")
print("\nğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° ./results/ ç›®å½•")
print("ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° ./models/best_model.pth")
print("\nâœ¨ å®éªŒæˆåŠŸå®Œæˆï¼")
print("="*60)

# æ˜¾ç¤ºæ–‡ä»¶ç»“æ„
print("\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶ç»“æ„:")
print("""
./models/
  â””â”€â”€ best_model.pth
./results/
  â”œâ”€â”€ experiment_results.json
  â”œâ”€â”€ experiment_report.txt
  â”œâ”€â”€ training_history.csv
  â””â”€â”€ test_predictions.csv
""")

print("\nğŸš€ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†:")
print("""
# åŠ è½½æ¨¡å‹
model = OptimizedDeepfakeDetector(...)
checkpoint = torch.load('./models/best_model.pth', weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
""")

print("\nâœ… è®­ç»ƒå®Œæˆï¼")