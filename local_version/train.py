#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬ - æœ¬åœ°RTX4070ç‰ˆæœ¬

ä½¿ç”¨æ–¹æ³•:
    python train.py [--config CONFIG_FILE] [--resume CHECKPOINT_PATH]
"""

import os
import sys
import argparse
import json
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / 'src'))

from config import config
from data_processing import prepare_data
from dataset import create_data_loaders
from model import create_model
from utils import FocalLoss, EarlyStopping, set_random_seed, format_time
from training import Trainer
from memory_manager import MemoryManager, print_memory_info, get_memory_suggestions

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--data-dir', type=str, default=None, help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default=None, help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=None, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=None, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=None, help='å­¦ä¹ ç‡')
    parser.add_argument('--no-cuda', action='store_true', help='ç¦ç”¨CUDA')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    return parser.parse_args()

def setup_directories(output_dir):
    """è®¾ç½®è¾“å‡ºç›®å½•ï¼ˆå·²ç”± config.setup_environment() å¤„ç†ï¼Œæ­¤å‡½æ•°ä¿æŒå…¼å®¹æ€§ï¼‰"""
    output_dir = Path(output_dir)
    
    # æ³¨æ„ï¼šç›®å½•åˆ›å»ºå·²ç”± config.setup_environment() ç»Ÿä¸€å¤„ç†
    # è¿™é‡Œåªç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼ˆé˜²æ­¢é…ç½®æœªæ­£ç¡®è°ƒç”¨çš„æƒ…å†µï¼‰
    output_dir.mkdir(parents=True, exist_ok=True)
    
    return output_dir

def save_config(config_dict, output_dir):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    config_path = output_dir / 'config.json'
    
    # è½¬æ¢é…ç½®ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    serializable_config = {}
    for key, value in config_dict.items():
        if isinstance(value, Path):
            serializable_config[key] = str(value)
        elif hasattr(value, '__dict__'):
            serializable_config[key] = str(value)
        else:
            serializable_config[key] = value
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_config, f, indent=2, ensure_ascii=False)
    
    print(f"é…ç½®å·²ä¿å­˜åˆ°: {config_path}")

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_random_seed()
    
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆåœ¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ä¹‹å‰ï¼‰
    if args.config:
        config.load_config(args.config)
    else:
        config.load_config()  # ä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶
    
    # æ›´æ–°é…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æ›´é«˜ï¼‰
    if args.data_dir:
        config.DATA_DIR = Path(args.data_dir)
    if args.output_dir:
        config.OUTPUT_DIR = Path(args.output_dir)
    if args.epochs:
        config.NUM_EPOCHS = args.epochs
    if args.batch_size:
        config.BATCH_SIZE = args.batch_size
    if args.lr:
        config.LEARNING_RATE = args.lr
    if args.no_cuda:
        config.USE_CUDA = False
    
    # è®¾ç½®è°ƒè¯•æ¨¡å¼
    if args.debug:
        config.NUM_EPOCHS = 2
        config.BATCH_SIZE = min(config.BATCH_SIZE, 4)
        print("ğŸ› è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    # è®¾ç½®ç¯å¢ƒ
    config.setup_environment()
    device = config.get_device()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = setup_directories(config.OUTPUT_DIR)
    
    # ä¿å­˜é…ç½®
    save_config(vars(config), output_dir)
    
    print("="*60)
    print("ğŸš€ æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
    print("="*60)
    config.print_config()
    print("="*60)
    
    # æ˜¾ç¤ºå†…å­˜çŠ¶æ€å’Œä¼˜åŒ–å»ºè®®
    print("\nğŸ“Š è®­ç»ƒå‰å†…å­˜çŠ¶æ€:")
    print_memory_info()
    suggestions = get_memory_suggestions()
    if len(suggestions) > 1 or "å†…å­˜ä½¿ç”¨çŠ¶å†µè‰¯å¥½" not in suggestions[0]:
        print("\nğŸ’¡ å†…å­˜ä¼˜åŒ–å»ºè®®:")
        for suggestion in suggestions:
            print(f"   {suggestion}")
    
    # æ•°æ®å‡†å¤‡
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
    train_data, val_data, test_data = prepare_data(
        data_dir=config.DATA_DIR,
        max_videos_per_class=None,  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
        force_reprocess=False
    )
    
    print(f"è®­ç»ƒé›†: {len(train_data)} ä¸ªæ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_data)} ä¸ªæ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(test_data)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_data_loaders(
        train_data, val_data, test_data
    )
    
    print(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}")
    print(f"éªŒè¯æ‰¹æ¬¡: {len(val_loader)}")
    print(f"æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    model = create_model()
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = FocalLoss()
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = CosineAnnealingLR(
        optimizer,
        T_max=config.NUM_EPOCHS,
        eta_min=config.LEARNING_RATE * 0.01
    )
    
    # åˆ›å»ºæ—©åœæœºåˆ¶
    early_stopping = EarlyStopping(
        patience=config.EARLY_STOPPING_PATIENCE,
        min_delta=config.EARLY_STOPPING_MIN_DELTA
    )
    
    # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
    start_epoch = 0
    if args.resume:
        print(f"\nğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: {args.resume}")
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint.get('epoch', 0) + 1
        print(f"ä»ç¬¬ {start_epoch} è½®å¼€å§‹ç»§ç»­è®­ç»ƒ")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        early_stopping=early_stopping
    )
    
    # å¼€å§‹è®­ç»ƒï¼ˆä½¿ç”¨å†…å­˜ç®¡ç†ï¼‰
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒï¼ˆå¸¦æ™ºèƒ½å†…å­˜ç®¡ç†ï¼‰...")
    try:
        # ä½¿ç”¨å¸¦å†…å­˜ç®¡ç†çš„è®­ç»ƒæ–¹æ³•
        history = trainer.start_training_with_memory_management(
            num_epochs=config.NUM_EPOCHS - start_epoch,
            save_dir=output_dir / 'models'
        )
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = output_dir / 'results' / 'training_history.json'
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“Š è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_path}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = output_dir / 'models' / 'final_model.pth'
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'config': vars(config),
            'history': history
        }, final_model_path)
        print(f"ğŸ¯ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
        
        # æ˜¾ç¤ºè®­ç»ƒåçš„å†…å­˜çŠ¶æ€
        print("\nğŸ“Š è®­ç»ƒåå†…å­˜çŠ¶æ€:")
        print_memory_info()
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
        interrupted_model_path = output_dir / 'models' / 'interrupted_model.pth'
        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'config': vars(config),
            'history': trainer.history
        }, interrupted_model_path)
        print(f"ğŸ’¾ ä¸­æ–­æ—¶çš„æ¨¡å‹å·²ä¿å­˜åˆ°: {interrupted_model_path}")
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)