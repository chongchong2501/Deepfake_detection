#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å†…å­˜ç®¡ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ–°çš„å†…å­˜ç®¡ç†ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import time
import sys
import os

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def test_memory_manager_basic():
    """æµ‹è¯•åŸºç¡€å†…å­˜ç®¡ç†åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€å†…å­˜ç®¡ç†åŠŸèƒ½...")
    
    try:
        from memory_manager import MemoryManager, print_memory_info, get_memory_suggestions
        
        # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
        manager = MemoryManager(
            gpu_memory_threshold=0.8,
            cpu_memory_threshold=0.8,
            auto_cleanup_interval=5.0,
            enable_monitoring=False  # æµ‹è¯•æ—¶ç¦ç”¨è‡ªåŠ¨ç›‘æ§
        )
        
        print("âœ… å†…å­˜ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å†…å­˜ç»Ÿè®¡
        stats = manager.get_memory_stats()
        print(f"âœ… å†…å­˜ç»Ÿè®¡è·å–æˆåŠŸ: GPU {stats.gpu_memory_percent*100:.1f}%, CPU {stats.cpu_memory_percent*100:.1f}%")
        
        # æµ‹è¯•å†…å­˜æ¸…ç†
        freed = manager.cleanup_gpu_memory()
        print(f"âœ… GPUå†…å­˜æ¸…ç†æˆåŠŸ: é‡Šæ”¾äº† {freed:.2f}GB")
        
        # æµ‹è¯•ä¼˜åŒ–å»ºè®®
        suggestions = manager.get_optimization_suggestions()
        print(f"âœ… ä¼˜åŒ–å»ºè®®è·å–æˆåŠŸ: {len(suggestions)} æ¡å»ºè®®")
        
        # æµ‹è¯•å†…å­˜æŠ¥å‘Š
        print("\nğŸ“Š å†…å­˜æŠ¥å‘Šæµ‹è¯•:")
        manager.print_memory_report()
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_manager_with_tensors():
    """æµ‹è¯•å†…å­˜ç®¡ç†å™¨åœ¨å¤„ç†å¼ é‡æ—¶çš„è¡¨ç°"""
    print("\nğŸ§ª æµ‹è¯•å¼ é‡å†…å­˜ç®¡ç†...")
    
    if not torch.cuda.is_available():
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå¼ é‡æµ‹è¯•")
        return True
    
    try:
        from memory_manager import MemoryManager, auto_memory_management
        
        @auto_memory_management(cleanup_interval=3)
        def create_large_tensors(size):
            """åˆ›å»ºå¤§å¼ é‡çš„æµ‹è¯•å‡½æ•°"""
            tensors = []
            for i in range(5):
                tensor = torch.randn(size, size, device='cuda')
                tensors.append(tensor)
            return tensors
        
        # åˆ›å»ºå†…å­˜ç®¡ç†å™¨
        with MemoryManager() as manager:
            print("âœ… ä¸Šä¸‹æ–‡ç®¡ç†å™¨å·¥ä½œæ­£å¸¸")
            
            # åˆ›å»ºä¸€äº›å¤§å¼ é‡æ¥æµ‹è¯•å†…å­˜ç®¡ç†
            print("åˆ›å»ºå¤§å¼ é‡è¿›è¡Œæµ‹è¯•...")
            
            for i in range(10):
                tensors = create_large_tensors(1000)
                if i % 3 == 0:
                    print(f"  ç¬¬ {i+1} è½®: åˆ›å»ºäº† {len(tensors)} ä¸ªå¼ é‡")
                
                # æ‰‹åŠ¨æ¸…ç†ä¸€äº›å¼ é‡
                del tensors
            
            print("âœ… å¼ é‡å†…å­˜ç®¡ç†æµ‹è¯•å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¼ é‡å†…å­˜ç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_optimization_config():
    """æµ‹è¯•å†…å­˜ä¼˜åŒ–é…ç½®æ–‡ä»¶"""
    print("\nğŸ§ª æµ‹è¯•å†…å­˜ä¼˜åŒ–é…ç½®...")
    
    try:
        from config import Config
        
        # æµ‹è¯•åŠ è½½å†…å­˜ä¼˜åŒ–é…ç½®
        config_path = "configs/memory_optimized.yaml"
        if os.path.exists(config_path):
            Config.load_config(config_path)
            print(f"âœ… å†…å­˜ä¼˜åŒ–é…ç½®åŠ è½½æˆåŠŸ")
            print(f"   batch_size: {getattr(Config, 'BATCH_SIZE', 'N/A')}")
            print(f"   max_frames: {getattr(Config, 'MAX_FRAMES', 'N/A')}")
            print(f"   backbone: {getattr(Config, 'BACKBONE', 'N/A')}")
            print(f"   pin_memory: {getattr(Config, 'PIN_MEMORY', 'N/A')}")
            return True
        else:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
            
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_integration_with_existing_code():
    """æµ‹è¯•ä¸ç°æœ‰ä»£ç çš„é›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•ä¸ç°æœ‰ä»£ç é›†æˆ...")
    
    try:
        # æµ‹è¯•utilså‡½æ•°çš„å…¼å®¹æ€§
        from utils import print_gpu_memory_info, cleanup_gpu_memory, PerformanceMonitor
        
        print("æµ‹è¯•å¢å¼ºçš„utilså‡½æ•°...")
        
        # æµ‹è¯•å†…å­˜ä¿¡æ¯æ‰“å°
        print_gpu_memory_info()
        print("âœ… print_gpu_memory_info å·¥ä½œæ­£å¸¸")
        
        # æµ‹è¯•å†…å­˜æ¸…ç†
        freed = cleanup_gpu_memory()
        if isinstance(freed, dict):
            gpu_freed = freed.get('gpu_freed', 0)
            print(f"âœ… cleanup_gpu_memory å·¥ä½œæ­£å¸¸: é‡Šæ”¾äº† {gpu_freed:.2f}GB")
        else:
            print(f"âœ… cleanup_gpu_memory å·¥ä½œæ­£å¸¸: é‡Šæ”¾äº† {freed:.2f}GB")
        
        # æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨
        monitor = PerformanceMonitor()
        monitor.update()
        stats = monitor.get_stats()
        suggestions = monitor.get_memory_suggestions()
        
        print(f"âœ… PerformanceMonitor å·¥ä½œæ­£å¸¸: {len(suggestions)} æ¡å»ºè®®")
        
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶"""
    print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†æœºåˆ¶...")
    
    try:
        from memory_manager import MemoryManager
        
        # æµ‹è¯•æ— æ•ˆå‚æ•°
        try:
            manager = MemoryManager(gpu_memory_threshold=2.0)  # æ— æ•ˆé˜ˆå€¼
            print("âš ï¸ åº”è¯¥æ£€æµ‹åˆ°æ— æ•ˆå‚æ•°")
        except:
            print("âœ… æ— æ•ˆå‚æ•°æ£€æµ‹æ­£å¸¸")
        
        # æµ‹è¯•å†…å­˜ä¸è¶³æ¨¡æ‹Ÿ
        if torch.cuda.is_available():
            try:
                # å°è¯•åˆ†é…è¶…å¤§å¼ é‡æ¥è§¦å‘å†…å­˜ä¸è¶³
                huge_tensor = torch.randn(50000, 50000, device='cuda')
                print("âš ï¸ åº”è¯¥è§¦å‘å†…å­˜ä¸è¶³é”™è¯¯")
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print("âœ… å†…å­˜ä¸è¶³é”™è¯¯å¤„ç†æ­£å¸¸")
                else:
                    print(f"âœ… å…¶ä»–CUDAé”™è¯¯å¤„ç†æ­£å¸¸: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å†…å­˜ç®¡ç†åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    
    print("=" * 50)
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("åŸºç¡€åŠŸèƒ½æµ‹è¯•", test_memory_manager_basic),
        ("å¼ é‡å†…å­˜ç®¡ç†æµ‹è¯•", test_memory_manager_with_tensors),
        ("é…ç½®æ–‡ä»¶æµ‹è¯•", test_memory_optimization_config),
        ("ä»£ç é›†æˆæµ‹è¯•", test_integration_with_existing_code),
        ("é”™è¯¯å¤„ç†æµ‹è¯•", test_error_handling)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} é€šè¿‡")
            else:
                print(f"âŒ {test_name} å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} å¼‚å¸¸: {e}")
    
    # æµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å†…å­˜ç®¡ç†ç³»ç»Ÿå·¥ä½œæ­£å¸¸ã€‚")
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   1. ä½¿ç”¨ 'python train.py --config configs/memory_optimized.yaml' è¿›è¡Œè®­ç»ƒ")
        print("   2. æŸ¥çœ‹ MEMORY_OPTIMIZATION_GUIDE.md äº†è§£è¯¦ç»†ä½¿ç”¨æ–¹æ³•")
        print("   3. æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´é…ç½®å‚æ•°")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜ã€‚")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)