# Cell 12: è®­ç»ƒå¾ªç¯ - GPUä¼˜åŒ–ç‰ˆæœ¬
# æ‰€æœ‰importè¯­å¥å·²ç§»è‡³cell_01_imports_and_setup.py

# ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs('./models', exist_ok=True)

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
print(f"ğŸ“Š è®­ç»ƒé…ç½®: {len(train_loader)} ä¸ªè®­ç»ƒæ‰¹æ¬¡, {len(val_loader)} ä¸ªéªŒè¯æ‰¹æ¬¡")
print(f"ğŸ¯ æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
print(f"ğŸ’¾ è®¾å¤‡: {device}")
print(f"ğŸ”§ æ•°æ®åŠ è½½ä¼˜åŒ–: {num_workers} workers, prefetch={prefetch_factor}, persistent={persistent_workers}")
print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}, å¸§ç¼“å­˜: {'å¯ç”¨' if train_dataset.cache_frames else 'ç¦ç”¨'}")

# æ€§èƒ½ç›‘æ§
if torch.cuda.is_available():
    print(f"ğŸ® GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    torch.cuda.reset_peak_memory_stats()

# è®­ç»ƒå†å²è®°å½•
train_history = {
    'train_loss': [],
    'val_loss': [],
    'train_acc': [],
    'val_acc': [],
    'train_auc': [],
    'val_auc': [],
    'lr': [],
    'gpu_memory': [],
    'epoch_time': []
}
best_val_loss = float('inf')
best_val_acc = 0.0
best_val_auc = 0.0
best_model_state = None

# è®­ç»ƒå¾ªç¯
print("\nğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯...")
for epoch in range(num_epochs):
    epoch_start_time = time.time()
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1}/{num_epochs} - {datetime.now().strftime('%H:%M:%S')}")
    print(f"{'='*60}")
    
    # è®­ç»ƒé˜¶æ®µ - ä½¿ç”¨æ··åˆç²¾åº¦
    print(f"ğŸ“š å¼€å§‹è®­ç»ƒç¬¬ {epoch+1} è½®...")
    train_start = time.time()
    train_loss, train_acc, train_auc = train_epoch(model, train_loader, criterion, optimizer, device, scaler)
    train_time = time.time() - train_start
    
    # éªŒè¯é˜¶æ®µ - ä½¿ç”¨æ··åˆç²¾åº¦
    print(f"ğŸ” å¼€å§‹éªŒè¯ç¬¬ {epoch+1} è½®...")
    val_start = time.time()
    val_loss, val_acc, val_auc = validate_epoch(model, val_loader, criterion, device, scaler)
    val_time = time.time() - val_start
    
    # è®°å½•å†å²
    train_history['train_loss'].append(train_loss)
    train_history['train_acc'].append(train_acc)
    train_history['train_auc'].append(train_auc)
    train_history['val_loss'].append(val_loss)
    train_history['val_acc'].append(val_acc)
    train_history['val_auc'].append(val_auc)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)
    current_lr = optimizer.param_groups[0]['lr']
    train_history['lr'].append(current_lr)
    
    # è®¡ç®—epochæ—¶é—´
    epoch_time = time.time() - epoch_start_time
    train_history['epoch_time'].append(epoch_time)
    
    # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
    gpu_memory = 0
    if torch.cuda.is_available():
        memory_allocated = torch.cuda.memory_allocated() / 1024**3
        memory_reserved = torch.cuda.memory_reserved() / 1024**3
        memory_peak = torch.cuda.max_memory_allocated() / 1024**3
        gpu_memory = memory_allocated
        train_history['gpu_memory'].append(gpu_memory)
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
    print(f"è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.2f}%, AUC={train_auc:.4f} (ç”¨æ—¶: {train_time:.1f}s)")
    print(f"éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.2f}%, AUC={val_auc:.4f} (ç”¨æ—¶: {val_time:.1f}s)")
    print(f"å­¦ä¹ ç‡: {current_lr:.2e}, Epochç”¨æ—¶: {epoch_time:.1f}s")
    
    if torch.cuda.is_available():
        print(f"GPUå†…å­˜: {memory_allocated:.1f}GB/{memory_reserved:.1f}GB (å³°å€¼: {memory_peak:.1f}GB)")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºå¤šä¸ªæŒ‡æ ‡ï¼‰
    is_best = False
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_val_acc = val_acc
        best_val_auc = val_auc
        best_model_state = model.state_dict().copy()
        is_best = True
        print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! Loss: {best_val_loss:.4f}, Acc: {best_val_acc:.2f}%, AUC: {best_val_auc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°æ–‡ä»¶
        torch.save({
            'epoch': epoch,
            'model_state_dict': best_model_state,
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_val_loss': best_val_loss,
            'best_val_acc': best_val_acc,
            'best_val_auc': best_val_auc,
            'train_history': train_history
        }, './models/best_model.pth')
        print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° ./models/best_model.pth")
    
    # æ—©åœæ£€æŸ¥
    if early_stopping(val_loss, model):
        print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
        print(f"æœ€ä½³æ€§èƒ½: Loss={best_val_loss:.4f}, Acc={best_val_acc:.2f}%, AUC={best_val_auc:.4f}")
        break
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("\nâœ… è®­ç»ƒå®Œæˆ!")
print(f"ğŸ† æœ€ç»ˆæœ€ä½³æ€§èƒ½: Loss={best_val_loss:.4f}, Acc={best_val_acc:.2f}%, AUC={best_val_auc:.4f}")
print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {sum(train_history['epoch_time']):.1f}ç§’")
if torch.cuda.is_available():
    print(f"ğŸ’¾ å³°å€¼GPUå†…å­˜ä½¿ç”¨: {torch.cuda.max_memory_allocated() / 1024**3:.1f}GB")