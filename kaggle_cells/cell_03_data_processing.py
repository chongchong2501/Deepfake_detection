# Cell 3: æ•°æ®å¤„ç†å‡½æ•°
def extract_frames_gpu_accelerated(video_path, max_frames=16, target_size=(224, 224),
                                  quality_threshold=20, use_gpu=True, use_mtcnn=True):
    """GPUåŠ é€Ÿçš„å¸§æå–å‡½æ•° - é›†æˆMTCNNäººè„¸æ£€æµ‹"""
    try:
        # æ£€æŸ¥PyAVæ˜¯å¦å¯ç”¨
        if not globals().get('PYAV_AVAILABLE', False):
            print(f"PyAVä¸å¯ç”¨ï¼Œä½¿ç”¨CPUå›é€€å¤„ç†: {video_path}")
            return extract_frames_cpu_fallback(video_path, max_frames, target_size, quality_threshold, use_mtcnn)
            
        # è®¾å¤‡é€‰æ‹© - ä¼˜å…ˆä½¿ç”¨GPU
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ä½¿ç”¨torchvisionçš„GPUåŠ é€Ÿè§†é¢‘è¯»å–
        if not use_gpu:
            device = torch.device('cpu')
            
        # è¯»å–è§†é¢‘ï¼ˆtorchvisionè‡ªåŠ¨å¤„ç†è§£ç ï¼‰
        try:
            video_tensor, audio, info = read_video(video_path, pts_unit='sec')
            # video_tensor shape: (T, H, W, C)
        except Exception as e:
            print(f"GPUè§†é¢‘è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return extract_frames_cpu_fallback(video_path, max_frames, target_size, quality_threshold, use_mtcnn)
        
        if video_tensor.size(0) == 0:
            return []
            
        # ç§»åŠ¨åˆ°GPUè¿›è¡Œå¤„ç†
        video_tensor = video_tensor.to(device, non_blocking=True)
        total_frames = video_tensor.size(0)
        
        # æ™ºèƒ½å¸§é‡‡æ ·ç­–ç•¥
        if total_frames <= max_frames:
            frame_indices = torch.arange(0, total_frames, device=device)
        else:
            # å‡åŒ€é‡‡æ ·
            step = total_frames / max_frames
            frame_indices = torch.arange(0, total_frames, step, device=device).long()[:max_frames]
        
        # æ‰¹é‡æå–å¸§
        selected_frames = video_tensor[frame_indices]  # (max_frames, H, W, C)
        
        # GPUä¸Šè¿›è¡Œè´¨é‡æ£€æµ‹ï¼ˆä½¿ç”¨Sobelç®—å­ä»£æ›¿Laplacianï¼‰
        if quality_threshold > 0:
            # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œè´¨é‡æ£€æµ‹ï¼ˆå…ˆè½¬æ¢ä¸ºfloatç±»å‹ï¼‰
            gray_frames = selected_frames.float().mean(dim=-1, keepdim=True)  # (T, H, W, 1)
            gray_frames = gray_frames.permute(0, 3, 1, 2)  # (T, 1, H, W)
            
            # ä½¿ç”¨Sobelç®—å­è®¡ç®—å›¾åƒè´¨é‡
            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                 dtype=torch.float32, device=device).view(1, 1, 3, 3)
            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                 dtype=torch.float32, device=device).view(1, 1, 3, 3)
            
            grad_x = F.conv2d(gray_frames, sobel_x, padding=1)
            grad_y = F.conv2d(gray_frames, sobel_y, padding=1)
            quality_scores = (grad_x.pow(2) + grad_y.pow(2)).mean(dim=[1, 2, 3])
            
            # è¿‡æ»¤ä½è´¨é‡å¸§
            quality_mask = quality_scores > quality_threshold
            if quality_mask.sum() > 0:
                selected_frames = selected_frames[quality_mask]
            
        # GPUä¸Šè¿›è¡Œå°ºå¯¸è°ƒæ•´
        selected_frames = selected_frames.permute(0, 3, 1, 2).float()  # (T, C, H, W)
        if selected_frames.size(-1) != target_size[0] or selected_frames.size(-2) != target_size[1]:
            selected_frames = F.interpolate(selected_frames, size=target_size, 
                                          mode='bilinear', align_corners=False)
        
        # ç¡®ä¿å¸§æ•°è¶³å¤Ÿ
        current_frames = selected_frames.size(0)
        if current_frames < max_frames:
            # é‡å¤æœ€åä¸€å¸§
            if current_frames > 0:
                last_frame = selected_frames[-1:].repeat(max_frames - current_frames, 1, 1, 1)
                selected_frames = torch.cat([selected_frames, last_frame], dim=0)
            else:
                # åˆ›å»ºé»‘è‰²å¸§
                selected_frames = torch.zeros(max_frames, 3, target_size[0], target_size[1], 
                                            device=device, dtype=torch.float32)
        
        # é™åˆ¶åˆ°æœ€å¤§å¸§æ•°
        selected_frames = selected_frames[:max_frames]
        
        # è½¬æ¢å›CPU numpyæ ¼å¼ï¼ˆä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç ï¼‰
        frames_cpu = selected_frames.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)
        frames_list = [frame for frame in frames_cpu]
        
        # åº”ç”¨MTCNNäººè„¸æ£€æµ‹å’Œè£å‰ª
        if use_mtcnn and globals().get('MTCNN_AVAILABLE', False):
            frames_list = apply_mtcnn_face_detection(frames_list, target_size)
        
        return frames_list
        
    except Exception as e:
        print(f"GPUå¸§æå–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
        return extract_frames_cpu_fallback(video_path, max_frames, target_size, quality_threshold, use_mtcnn)

def apply_mtcnn_face_detection(frames, target_size=(224, 224)):
    """ä½¿ç”¨MTCNNè¿›è¡Œäººè„¸æ£€æµ‹å’Œè£å‰ª - å…¼å®¹æ–°ç‰ˆæœ¬API"""
    try:
        # æ–°ç‰ˆæœ¬MTCNNæ„é€ å‡½æ•°ä¸éœ€è¦å‚æ•°
        detector = MTCNN()
        processed_frames = []
        
        for frame in frames:
            # MTCNNéœ€è¦RGBæ ¼å¼
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) if len(frame.shape) == 3 else frame
            
            # æ£€æµ‹äººè„¸ - æ–°ç‰ˆæœ¬APIåœ¨detect_facesæ–¹æ³•ä¸­ä¼ é€’å‚æ•°
            results = detector.detect_faces(
                frame_rgb,
                min_face_size=40,  # æœ€å°äººè„¸å°ºå¯¸
                threshold_pnet=0.6,  # PNeté˜ˆå€¼
                threshold_rnet=0.7,  # RNeté˜ˆå€¼  
                threshold_onet=0.8   # ONeté˜ˆå€¼
            )
            
            if results and len(results) > 0:
                # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„äººè„¸
                best_face = max(results, key=lambda x: x['confidence'])
                
                if best_face['confidence'] > 0.9:  # é«˜ç½®ä¿¡åº¦é˜ˆå€¼
                    # æå–äººè„¸åŒºåŸŸ
                    x, y, w, h = best_face['box']
                    
                    # æ‰©å±•è¾¹ç•Œæ¡†ä»¥åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡
                    margin = 0.2
                    x_margin = int(w * margin)
                    y_margin = int(h * margin)
                    
                    x1 = max(0, x - x_margin)
                    y1 = max(0, y - y_margin)
                    x2 = min(frame_rgb.shape[1], x + w + x_margin)
                    y2 = min(frame_rgb.shape[0], y + h + y_margin)
                    
                    # è£å‰ªäººè„¸
                    face_crop = frame_rgb[y1:y2, x1:x2]
                    
                    # ä½¿ç”¨ç»Ÿä¸€çš„å¸§å¤„ç†å‡½æ•°
                    processed_frame = resize_and_validate_frame(face_crop, target_size, 0)  # MTCNNä¸éœ€è¦é¢å¤–è´¨é‡æ£€æŸ¥
                    if processed_frame is None:
                        processed_frames.append(cv2.resize(face_crop, target_size))  # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå¸§
                    else:
                        processed_frames.append(processed_frame)
                else:
                    # ç½®ä¿¡åº¦ä¸å¤Ÿï¼Œä½¿ç”¨åŸå§‹å¸§
                    processed_frames.append(cv2.resize(frame_rgb, target_size))
            else:
                # æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸ï¼Œä½¿ç”¨åŸå§‹å¸§
                processed_frames.append(cv2.resize(frame_rgb, target_size))
        
        return processed_frames
        
    except Exception as e:
        print(f"MTCNNäººè„¸æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å¸§: {e}")
        return [cv2.resize(frame, target_size) for frame in frames]

def resize_and_validate_frame(frame, target_size, quality_threshold=20):
    """ç»Ÿä¸€çš„å¸§å¤„ç†å‡½æ•°ï¼šè°ƒæ•´å¤§å°å¹¶éªŒè¯è´¨é‡"""
    if frame is None:
        return None
    
    # è°ƒæ•´å°ºå¯¸
    resized_frame = cv2.resize(frame, target_size)
    
    # è´¨é‡æ£€æŸ¥
    if quality_threshold > 0:
        # è®¡ç®—å›¾åƒçš„æ–¹å·®ä½œä¸ºè´¨é‡æŒ‡æ ‡
        gray = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2GRAY) if len(resized_frame.shape) == 3 else resized_frame
        variance = cv2.Laplacian(gray, cv2.CV_64F).var()
        if variance < quality_threshold:
            return None
    
    return resized_frame

def extract_frames_cpu_fallback(video_path, max_frames=16, target_size=(224, 224), quality_threshold=20, use_mtcnn=True):
    """CPUå›é€€çš„å¸§æå–å‡½æ•° - é›†æˆMTCNN"""
    cap = cv2.VideoCapture(video_path)
    frames = []

    if not cap.isOpened():
        print(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        return frames

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames == 0:
        cap.release()
        return frames

    # å‡åŒ€é‡‡æ ·ç­–ç•¥
    if total_frames <= max_frames:
        frame_indices = list(range(0, total_frames, max(1, total_frames // max_frames)))
    else:
        step = max(1, total_frames // max_frames)
        frame_indices = list(range(0, total_frames, step))[:max_frames]

    frame_count = 0
    for frame_idx in frame_indices:
        if frame_count >= max_frames:
            break

        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨ç»Ÿä¸€çš„å¸§å¤„ç†å‡½æ•°
            processed_frame = resize_and_validate_frame(frame, target_size, quality_threshold)
            if processed_frame is None:
                continue
            frame = processed_frame
            frames.append(frame)
            frame_count += 1

    cap.release()

    # å¦‚æœå¸§æ•°ä¸è¶³ï¼Œé‡å¤æœ€åä¸€å¸§
    while len(frames) < max_frames and len(frames) > 0:
        frames.append(frames[-1].copy())

    # åº”ç”¨MTCNNäººè„¸æ£€æµ‹
    if use_mtcnn and globals().get('MTCNN_AVAILABLE', False):
        frames = apply_mtcnn_face_detection(frames, target_size)

    return frames[:max_frames]

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸå‡½æ•°åï¼Œä½†ç§»é™¤å†—ä½™å‚æ•°
def extract_frames_memory_efficient(video_path, max_frames=16, target_size=(224, 224),
                                   quality_threshold=20, use_mtcnn=True):
    """å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œä¼˜å…ˆä½¿ç”¨GPUåŠ é€Ÿï¼Œé›†æˆMTCNN
    æ³¨æ„ï¼šskip_frameså‚æ•°å·²ç§»é™¤ï¼Œå› ä¸ºGPUç‰ˆæœ¬ä½¿ç”¨æ›´æ™ºèƒ½çš„é‡‡æ ·ç­–ç•¥
    """
    return extract_frames_gpu_accelerated(video_path, max_frames, target_size, quality_threshold, use_mtcnn=use_mtcnn)

def process_videos_simple(base_data_dir, max_videos_per_class=60, max_frames=16, max_real=None, max_fake=None):
    """ç®€åŒ–çš„è§†é¢‘å¤„ç†å‡½æ•° - ä¼˜åŒ–å‡è§†é¢‘å¹³å‡åˆ†é…"""
    # æ‰“å°è®¾å¤‡ä¿¡æ¯ï¼ˆåªæ‰“å°ä¸€æ¬¡ï¼‰
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± æ•°æ®å¤„ç†ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å‘åå…¼å®¹ï¼šå¦‚æœæŒ‡å®šäº†æ–°å‚æ•°ï¼Œä½¿ç”¨æ–°å‚æ•°ï¼›å¦åˆ™ä½¿ç”¨æ—§å‚æ•°
    if max_real is None:
        max_real = max_videos_per_class
    if max_fake is None:
        max_fake = max_videos_per_class
    
    data_list = []
    fake_methods = ['Deepfakes', 'Face2Face', 'FaceShifter', 'FaceSwap', 'NeuralTextures']

    print("å¼€å§‹å¤„ç†çœŸå®è§†é¢‘...")
    # å¤„ç†çœŸå®è§†é¢‘
    original_dir = os.path.join(base_data_dir, 'original')
    if os.path.exists(original_dir):
        video_files = [f for f in os.listdir(original_dir)
                      if f.endswith(('.mp4', '.avi', '.mov'))]
        
        if len(video_files) > max_real:
            video_files = random.sample(video_files, max_real)

        print(f"æ‰¾åˆ° {len(video_files)} ä¸ªçœŸå®è§†é¢‘")

        for video_file in tqdm(video_files, desc="å¤„ç†çœŸå®è§†é¢‘"):
            try:
                video_path = os.path.join(original_dir, video_file)
                frames = extract_frames_memory_efficient(video_path, max_frames)
                
                if len(frames) >= max_frames // 2:  # è‡³å°‘è¦æœ‰ä¸€åŠçš„å¸§
                    data_list.append({
                        'video_path': video_path,
                        'frames': frames,
                        'label': 0,  # çœŸå®è§†é¢‘
                        'method': 'original'
                    })
            except Exception as e:
                print(f"å¤„ç†è§†é¢‘ {video_file} æ—¶å‡ºé”™: {e}")
                continue

    # å¤„ç†ä¼ªé€ è§†é¢‘ - å¹³å‡åˆ†é…ç­–ç•¥
    print("å¼€å§‹å¤„ç†ä¼ªé€ è§†é¢‘...")
    
    # ç»Ÿè®¡æ¯ç§æ–¹æ³•çš„å¯ç”¨è§†é¢‘æ•°é‡
    method_videos = {}
    total_available_fake = 0
    
    for method in fake_methods:
        method_dir = os.path.join(base_data_dir, method)
        if os.path.exists(method_dir):
            videos = [os.path.join(method_dir, f) for f in os.listdir(method_dir) 
                     if f.endswith(('.mp4', '.avi', '.mov'))]
            method_videos[method] = videos
            total_available_fake += len(videos)
            print(f"  {method}: {len(videos)} ä¸ªè§†é¢‘")
        else:
            method_videos[method] = []
            print(f"  {method}: ç›®å½•ä¸å­˜åœ¨")
    
    print(f"æ€»å…±å¯ç”¨å‡è§†é¢‘: {total_available_fake} ä¸ª")
    
    # è®¡ç®—æ¯ç§æ–¹æ³•åº”è¯¥é‡‡æ ·çš„è§†é¢‘æ•°é‡ï¼ˆå¹³å‡åˆ†é…ï¼‰
    available_methods = [method for method in fake_methods if len(method_videos[method]) > 0]
    if not available_methods:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å‡è§†é¢‘æ–¹æ³•")
        return data_list
    
    videos_per_method = max_fake // len(available_methods)
    remaining_videos = max_fake % len(available_methods)
    
    print(f"å¹³å‡åˆ†é…ç­–ç•¥: æ¯ç§æ–¹æ³• {videos_per_method} ä¸ªè§†é¢‘")
    if remaining_videos > 0:
        print(f"å‰©ä½™ {remaining_videos} ä¸ªè§†é¢‘å°†åˆ†é…ç»™å‰ {remaining_videos} ç§æ–¹æ³•")
    
    # ä¸ºæ¯ç§æ–¹æ³•é‡‡æ ·è§†é¢‘
    selected_fake_videos = []
    for i, method in enumerate(available_methods):
        # è®¡ç®—å½“å‰æ–¹æ³•åº”è¯¥é‡‡æ ·çš„æ•°é‡
        current_method_quota = videos_per_method
        if i < remaining_videos:  # å‰å‡ ç§æ–¹æ³•å¤šåˆ†é…ä¸€ä¸ª
            current_method_quota += 1
        
        available_videos = method_videos[method]
        
        # å¦‚æœå¯ç”¨è§†é¢‘æ•°é‡å°‘äºé…é¢ï¼Œå…¨éƒ¨ä½¿ç”¨
        if len(available_videos) <= current_method_quota:
            method_selected = available_videos
            print(f"  {method}: ä½¿ç”¨å…¨éƒ¨ {len(method_selected)} ä¸ªè§†é¢‘")
        else:
            # éšæœºé‡‡æ ·æŒ‡å®šæ•°é‡
            method_selected = random.sample(available_videos, current_method_quota)
            print(f"  {method}: é‡‡æ · {len(method_selected)} ä¸ªè§†é¢‘")
        
        selected_fake_videos.extend([(v, method) for v in method_selected])
    
    print(f"æ€»å…±é€‰æ‹© {len(selected_fake_videos)} ä¸ªå‡è§†é¢‘è¿›è¡Œå¤„ç†")
    
    # æ‰“ä¹±é€‰æ‹©çš„å‡è§†é¢‘é¡ºåº
    random.shuffle(selected_fake_videos)
    
    # å¤„ç†é€‰æ‹©çš„å‡è§†é¢‘
    for video_path, method in tqdm(selected_fake_videos, desc="å¤„ç†ä¼ªé€ è§†é¢‘"):
        try:
            frames = extract_frames_memory_efficient(video_path, max_frames)
            
            if len(frames) >= max_frames // 2:
                data_list.append({
                    'video_path': video_path,
                    'frames': frames,
                    'label': 1,  # ä¼ªé€ è§†é¢‘
                    'method': method
                })
        except Exception as e:
            print(f"å¤„ç†è§†é¢‘ {os.path.basename(video_path)} æ—¶å‡ºé”™: {e}")
            continue

    # ç»Ÿè®¡æœ€ç»ˆç»“æœ
    method_counts = {}
    for item in data_list:
        if item['label'] == 1:  # åªç»Ÿè®¡å‡è§†é¢‘
            method = item['method']
            method_counts[method] = method_counts.get(method, 0) + 1
    
    print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(data_list)} ä¸ªè§†é¢‘")
    print("å‡è§†é¢‘æ–¹æ³•åˆ†å¸ƒ:")
    for method, count in method_counts.items():
        print(f"  {method}: {count} ä¸ªè§†é¢‘")
    
    return data_list

def create_dataset_split(data_list, test_size=0.2, val_size=0.1):
    """åˆ›å»ºæ•°æ®é›†åˆ’åˆ†"""
    # åˆ†ç¦»çœŸå®å’Œä¼ªé€ æ•°æ®
    real_data = [item for item in data_list if item['label'] == 0]
    fake_data = [item for item in data_list if item['label'] == 1]
    
    print(f"çœŸå®è§†é¢‘: {len(real_data)} ä¸ª")
    print(f"ä¼ªé€ è§†é¢‘: {len(fake_data)} ä¸ª")
    
    # åˆ†åˆ«åˆ’åˆ†çœŸå®å’Œä¼ªé€ æ•°æ®
    real_train, real_temp = train_test_split(real_data, test_size=test_size+val_size, random_state=42)
    real_val, real_test = train_test_split(real_temp, test_size=test_size/(test_size+val_size), random_state=42)
    
    fake_train, fake_temp = train_test_split(fake_data, test_size=test_size+val_size, random_state=42)
    fake_val, fake_test = train_test_split(fake_temp, test_size=test_size/(test_size+val_size), random_state=42)
    
    # åˆå¹¶æ•°æ®
    train_data = real_train + fake_train
    val_data = real_val + fake_val
    test_data = real_test + fake_test
    
    # æ‰“ä¹±æ•°æ®
    random.shuffle(train_data)
    random.shuffle(val_data)
    random.shuffle(test_data)
    
    return train_data, val_data, test_data

def save_dataset_to_csv(data_list, filename):
    """å°†æ•°æ®é›†ä¿å­˜ä¸ºCSVæ–‡ä»¶ - æ”¯æŒé¢„æå–å¸§è·¯å¾„"""
    df_data = []
    for item in data_list:
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¢„æå–çš„å¸§æ•°æ®
        if 'frame_path' in item:
            df_data.append({
                'frame_path': item['frame_path'],  # é¢„æå–çš„å¸§è·¯å¾„
                'label': item['label'],
                'method': item['method'],
                'num_frames': item['num_frames']
            })
        else:
            # å‘åå…¼å®¹ï¼šåŸå§‹è§†é¢‘è·¯å¾„æ ¼å¼
            df_data.append({
                'video_path': item['video_path'],
                'label': item['label'],
                'method': item['method'],
                'num_frames': len(item['frames'])
            })
    
    df = pd.DataFrame(df_data)
    df.to_csv(filename, index=False)
    print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {filename}")
    return df

print("âœ… æ•°æ®å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ")