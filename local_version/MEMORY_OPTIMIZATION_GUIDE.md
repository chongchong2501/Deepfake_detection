# å†…å­˜ä¼˜åŒ–æŒ‡å— - RTX4070æ·±åº¦ä¼ªé€ æ£€æµ‹

## æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»äº†ä¸ºRTX4070æ˜¾å¡ï¼ˆ12GBæ˜¾å­˜ï¼‰ä¸“é—¨è®¾è®¡çš„å†…å­˜ç®¡ç†å’Œä¼˜åŒ–ç³»ç»Ÿã€‚é€šè¿‡æ™ºèƒ½å†…å­˜ç›‘æ§ã€è‡ªåŠ¨æ¸…ç†å’Œå‚æ•°è°ƒä¼˜ï¼Œæ˜¾è‘—æå‡è®­ç»ƒç¨³å®šæ€§å’Œæ•ˆç‡ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨å†…å­˜ä¼˜åŒ–é…ç½®

```bash
# ä½¿ç”¨ä¸“é—¨çš„å†…å­˜ä¼˜åŒ–é…ç½®
cd local_version
python train.py --config configs/memory_optimized.yaml
```

### æ‰‹åŠ¨å†…å­˜ç®¡ç†

```python
from src.memory_manager import MemoryManager, print_memory_info

# åˆ›å»ºå†…å­˜ç®¡ç†å™¨
with MemoryManager() as manager:
    # ä½ çš„è®­ç»ƒä»£ç 
    trainer.train(epochs=20)
```

## ğŸ“Š å†…å­˜ç®¡ç†åŠŸèƒ½

### 1. æ™ºèƒ½å†…å­˜ç›‘æ§

- **å®æ—¶ç›‘æ§**: æŒç»­ç›‘æ§CPUå’ŒGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
- **è‡ªåŠ¨é˜ˆå€¼æ£€æµ‹**: å½“å†…å­˜ä½¿ç”¨è¶…è¿‡è®¾å®šé˜ˆå€¼æ—¶è‡ªåŠ¨è§¦å‘æ¸…ç†
- **å†å²ç»Ÿè®¡**: è®°å½•å†…å­˜ä½¿ç”¨å†å²ï¼Œæä¾›æ€§èƒ½åˆ†æ

### 2. è‡ªåŠ¨å†…å­˜æ¸…ç†

- **GPUç¼“å­˜æ¸…ç†**: è‡ªåŠ¨æ¸…ç†PyTorch GPUç¼“å­˜
- **CPUåƒåœ¾å›æ”¶**: å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
- **æ•°æ®é›†ç¼“å­˜æ¸…ç†**: æ¸…ç†è§†é¢‘å¸§ç¼“å­˜
- **æ¨¡å‹ç¼“å­˜æ¸…ç†**: æ¸…ç†æ¨¡å‹ä¸­é—´ç»“æœç¼“å­˜

### 3. æ™ºèƒ½å‚æ•°è°ƒä¼˜

- **åŠ¨æ€æ‰¹æ¬¡å¤§å°**: æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè‡ªåŠ¨è°ƒæ•´batch_size
- **å·¥ä½œè¿›ç¨‹ä¼˜åŒ–**: è‡ªåŠ¨è°ƒæ•´num_workersæ•°é‡
- **é¢„å¤„ç†æ¨¡å¼åˆ‡æ¢**: åœ¨GPU/CPUé¢„å¤„ç†é—´æ™ºèƒ½åˆ‡æ¢

## âš™ï¸ é…ç½®é€‰é¡¹

### å†…å­˜ç®¡ç†å™¨é…ç½®

```python
MemoryManager(
    gpu_memory_threshold=0.85,    # GPUå†…å­˜ä½¿ç”¨é˜ˆå€¼ï¼ˆ85%ï¼‰
    cpu_memory_threshold=0.80,    # CPUå†…å­˜ä½¿ç”¨é˜ˆå€¼ï¼ˆ80%ï¼‰
    auto_cleanup_interval=30.0,   # è‡ªåŠ¨æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
    enable_monitoring=True        # å¯ç”¨å®æ—¶ç›‘æ§
)
```

### YAMLé…ç½®æ–‡ä»¶

```yaml
gpu:
  memory_management:
    gpu_threshold: 0.75        # GPUå†…å­˜é˜ˆå€¼
    cpu_threshold: 0.75        # CPUå†…å­˜é˜ˆå€¼
    auto_cleanup_interval: 25  # è‡ªåŠ¨æ¸…ç†é—´éš”
    enable_monitoring: true    # å¯ç”¨å†…å­˜ç›‘æ§
    force_cleanup_every: 100   # å¼ºåˆ¶æ¸…ç†é—´éš”
```

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥

### 1. æ˜¾å­˜ä¼˜åŒ–ï¼ˆ12GB RTX4070ï¼‰

| é…ç½®é¡¹ | æ¨èå€¼ | è¯´æ˜ |
|--------|--------|------|
| batch_size | 6-8 | æ ¹æ®æ¨¡å‹å¤æ‚åº¦è°ƒæ•´ |
| max_frames | 16-24 | å¹³è¡¡æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨ |
| backbone | resnet18/resnet50 | é¿å…è¿‡å¤§çš„æ¨¡å‹ |
| mixed_precision | true | å¯ç”¨FP16æ··åˆç²¾åº¦ |
| pin_memory | false | é¿å…CUDA tensoré”™è¯¯ |

### 2. ç³»ç»Ÿå†…å­˜ä¼˜åŒ–

| é…ç½®é¡¹ | æ¨èå€¼ | è¯´æ˜ |
|--------|--------|------|
| num_workers | 2-4 | é¿å…è¿‡å¤šè¿›ç¨‹ |
| enable_cache | false | ç¦ç”¨ç¼“å­˜èŠ‚çœå†…å­˜ |
| prefetch_factor | 1-2 | å‡å°‘é¢„å–æ•°æ®é‡ |
| persistent_workers | true | é‡ç”¨å·¥ä½œè¿›ç¨‹ |

### 3. æ•°æ®å¤„ç†ä¼˜åŒ–

- **GPUé¢„å¤„ç†**: ä»…åœ¨æ˜¾å­˜å……è¶³æ—¶å¯ç”¨
- **å¸§ç¼“å­˜**: å°æ•°æ®é›†å¯ç”¨ï¼Œå¤§æ•°æ®é›†ç¦ç”¨
- **è´¨é‡è¿‡æ»¤**: é€‚å½“é™ä½é˜ˆå€¼ä¿ç•™æ›´å¤šå¸§
- **æ•°æ®å¢å¼º**: ä½¿ç”¨è½»é‡çº§å¢å¼ºç­–ç•¥

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§å†…å­˜é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### 1. CUDA Out of Memory

**é”™è¯¯ä¿¡æ¯**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```python
# è‡ªåŠ¨å¤„ç†ï¼ˆå·²é›†æˆï¼‰
@auto_memory_management(cleanup_interval=50)
def train_step(self, batch):
    # è®­ç»ƒä»£ç 
    pass

# æ‰‹åŠ¨å¤„ç†
try:
    output = model(input)
except RuntimeError as e:
    if "out of memory" in str(e):
        cleanup_memory(force=True)
        # é‡è¯•æˆ–é™ä½batch_size
```

#### 2. Pin Memoryé”™è¯¯

**é”™è¯¯ä¿¡æ¯**: `cannot pin 'torch.cuda.FloatTensor'`

**è§£å†³æ–¹æ¡ˆ**:
```yaml
dataloader:
  pin_memory: false  # åœ¨é…ç½®æ–‡ä»¶ä¸­ç¦ç”¨
```

#### 3. å·¥ä½œè¿›ç¨‹å†…å­˜æ³„æ¼

**è§£å†³æ–¹æ¡ˆ**:
```yaml
dataloader:
  num_workers: 2           # å‡å°‘å·¥ä½œè¿›ç¨‹
  persistent_workers: true # é‡ç”¨è¿›ç¨‹
```

### å†…å­˜ä½¿ç”¨ç›‘æ§

```python
# æŸ¥çœ‹å½“å‰å†…å­˜çŠ¶æ€
from src.memory_manager import print_memory_info
print_memory_info()

# è·å–ä¼˜åŒ–å»ºè®®
from src.memory_manager import get_memory_suggestions
suggestions = get_memory_suggestions()
for suggestion in suggestions:
    print(suggestion)
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### RTX4070 (12GB) æ€§èƒ½è¡¨ç°

| é…ç½® | Batch Size | æ˜¾å­˜ä½¿ç”¨ | è®­ç»ƒé€Ÿåº¦ | ç¨³å®šæ€§ |
|------|------------|----------|----------|--------|
| default.yaml | 12 | ~11GB | å¿« | ä¸ç¨³å®š |
| medium.yaml | 10 | ~9GB | ä¸­ç­‰ | è¾ƒç¨³å®š |
| memory_optimized.yaml | 6 | ~7GB | ç¨æ…¢ | éå¸¸ç¨³å®š |

### å†…å­˜ä½¿ç”¨æ¨¡å¼

- **è®­ç»ƒé˜¶æ®µ**: æ˜¾å­˜ä½¿ç”¨75-85%
- **éªŒè¯é˜¶æ®µ**: æ˜¾å­˜ä½¿ç”¨60-70%
- **å³°å€¼å†…å­˜**: é€šå¸¸åœ¨epochå¼€å§‹æ—¶
- **æ¸…ç†æ•ˆæœ**: æ¯æ¬¡æ¸…ç†å¯é‡Šæ”¾1-2GBæ˜¾å­˜

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### 1. è‡ªå®šä¹‰å†…å­˜å›è°ƒ

```python
def custom_cleanup():
    # è‡ªå®šä¹‰æ¸…ç†é€»è¾‘
    torch.cuda.empty_cache()
    gc.collect()

# æ³¨å†Œå›è°ƒ
memory_manager.register_cleanup_callback(custom_cleanup)
```

### 2. å†…å­˜åˆ†æå’Œè°ƒè¯•

```python
# å¯ç”¨å†…å­˜åˆ†æ
with MemoryManager(enable_monitoring=True) as manager:
    # è®­ç»ƒä»£ç 
    history = trainer.train(epochs=10)
    
    # è·å–å†…å­˜ç»Ÿè®¡
    stats = manager.get_memory_stats()
    print(f"å³°å€¼GPUå†…å­˜: {stats.gpu_memory_reserved_gb:.2f}GB")
```

### 3. æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–

```python
# åœ¨å†…å­˜ä¸è¶³æ—¶ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 2
for i, batch in enumerate(dataloader):
    output = model(batch)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. è®­ç»ƒå‰æ£€æŸ¥

```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
nvidia-smi
free -h

# è¿è¡Œå†…å­˜æµ‹è¯•
python -c "from src.memory_manager import print_memory_info; print_memory_info()"
```

### 2. é…ç½®é€‰æ‹©æŒ‡å—

- **é¦–æ¬¡è®­ç»ƒ**: ä½¿ç”¨ `memory_optimized.yaml`
- **è°ƒè¯•é˜¶æ®µ**: ä½¿ç”¨ `quick_demo.yaml`
- **ç”Ÿäº§è®­ç»ƒ**: æ ¹æ®ç¡¬ä»¶é€‰æ‹© `medium.yaml` æˆ– `default.yaml`

### 3. ç›‘æ§å’Œè°ƒä¼˜

- å®šæœŸæŸ¥çœ‹å†…å­˜æŠ¥å‘Š
- æ ¹æ®å»ºè®®è°ƒæ•´å‚æ•°
- è®°å½•æœ€ä½³é…ç½®ç»„åˆ

## ğŸ” å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆè¦ç¦ç”¨pin_memoryï¼Ÿ**
A: åœ¨æŸäº›PyTorchç‰ˆæœ¬ä¸­ï¼Œpin_memoryå¯èƒ½å¯¼è‡´CUDA tensoré”™è¯¯ã€‚ç¦ç”¨åå¯æé«˜ç¨³å®šæ€§ï¼Œå¯¹æ€§èƒ½å½±å“å¾ˆå°ã€‚

**Q: å†…å­˜æ¸…ç†ä¼šå½±å“è®­ç»ƒç²¾åº¦å—ï¼Ÿ**
A: ä¸ä¼šã€‚å†…å­˜æ¸…ç†åªæ¸…é™¤ç¼“å­˜å’Œä¸´æ—¶æ•°æ®ï¼Œä¸å½±å“æ¨¡å‹å‚æ•°å’Œæ¢¯åº¦ã€‚

**Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„batch_sizeï¼Ÿ**
A: ä»å°å¼€å§‹ï¼ˆå¦‚4ï¼‰ï¼Œé€æ­¥å¢åŠ ç›´åˆ°æ˜¾å­˜ä½¿ç”¨ç‡è¾¾åˆ°80-85%ã€‚

**Q: æ··åˆç²¾åº¦è®­ç»ƒå®‰å…¨å—ï¼Ÿ**
A: æ˜¯çš„ã€‚ç°ä»£GPUéƒ½æ”¯æŒFP16ï¼Œå¯ä»¥æ˜¾è‘—èŠ‚çœæ˜¾å­˜è€Œä¸å½±å“ç²¾åº¦ã€‚

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°å†…å­˜ç›¸å…³é—®é¢˜ï¼š

1. æŸ¥çœ‹å†…å­˜æŠ¥å‘Šå’Œä¼˜åŒ–å»ºè®®
2. å°è¯•ä½¿ç”¨ `memory_optimized.yaml` é…ç½®
3. æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
4. å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†

---

*æœ¬æŒ‡å—é’ˆå¯¹RTX4070æ˜¾å¡ä¼˜åŒ–ï¼Œå…¶ä»–æ˜¾å¡å¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°ã€‚*