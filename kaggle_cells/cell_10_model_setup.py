# Cell 10: æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒé…ç½® 
print("ğŸ¤– åˆ›å»ºå’Œé…ç½®æ¨¡å‹...")

# è®­ç»ƒé…ç½®å‚æ•° - ResNet18ä¼˜åŒ–ç‰ˆæœ¬
batch_size = 16  # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ŒResNet18å†…å­˜å ç”¨æ›´å°ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§æ‰¹æ¬¡

# åˆ›å»ºè½»é‡åŒ–æ¨¡å‹ - ResNet18ç‰ˆæœ¬
model = OptimizedDeepfakeDetector(
    num_classes=1,
    dropout_rate=0.2,  # é€‚ä¸­çš„dropoutç‡ï¼Œå¹³è¡¡è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ
    use_attention=False,  # ç¦ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œç®€åŒ–æ¨¡å‹
    use_multimodal=False,  # ç¦ç”¨å¤šæ¨¡æ€ç‰¹å¾èåˆ
    ensemble_mode=False   # å•æ¨¡å‹æ¨¡å¼
).to(device)

# å¤šGPUå¹¶è¡Œæ”¯æŒ
if torch.cuda.is_available() and torch.cuda.device_count() > 1:
    print(f"ğŸš€ å¯ç”¨å¤šGPUå¹¶è¡Œè®­ç»ƒï¼Œä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPU")
    model = nn.DataParallel(model)
    # è°ƒæ•´æ‰¹æ¬¡å¤§å°ä»¥å……åˆ†åˆ©ç”¨å¤šGPU
    effective_batch_size = batch_size * torch.cuda.device_count()
    print(f"ğŸ“¦ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size} (å•GPU: {batch_size})")
else:
    print("ğŸ“ å•GPUè®­ç»ƒæ¨¡å¼")

print(f"âœ… æ¨¡å‹å·²åˆ›å»ºå¹¶ç§»åŠ¨åˆ° {device}")
print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

# ä¼˜åŒ–GPUå†…å­˜é…ç½® - æ›´ä¿å®ˆçš„å†…å­˜ä½¿ç”¨é¿å…OOM
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.6)  # é™ä½åˆ°60%é¿å…å†…å­˜æº¢å‡º
    torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print(f"ğŸ”§ å†…å­˜ä½¿ç”¨é™åˆ¶: 60%")

# æŸå¤±å‡½æ•° - ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡
# è®¡ç®—ç±»åˆ«æƒé‡ - ä¿®å¤ç‰ˆæœ¬
if 'train_loader' in globals() and train_loader is not None:
    # ä»train_loaderè·å–æ•°æ®é›†
    train_dataset = train_loader.dataset
    
    if hasattr(train_dataset, 'real_count') and hasattr(train_dataset, 'fake_count'):
        # ä½¿ç”¨é¢„è®¡ç®—çš„ç»Ÿè®¡ä¿¡æ¯
        real_count = train_dataset.real_count
        fake_count = train_dataset.fake_count
    else:
        # å›é€€æ–¹æ¡ˆï¼šæ‰‹åŠ¨è®¡ç®—
        if hasattr(train_dataset, 'data_list') and train_dataset.data_list is not None:
            real_count = sum(1 for item in train_dataset.data_list if item['label'] == 0)
            fake_count = sum(1 for item in train_dataset.data_list if item['label'] == 1)
        elif hasattr(train_dataset, 'df') and train_dataset.df is not None:
            real_count = len(train_dataset.df[train_dataset.df['label'] == 0])
            fake_count = len(train_dataset.df[train_dataset.df['label'] == 1])
        else:
            # é»˜è®¤å€¼
            real_count = 1
            fake_count = 1
            print("âš ï¸ æ— æ³•è·å–ç±»åˆ«åˆ†å¸ƒï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
else:
    # å¦‚æœæ²¡æœ‰train_loaderï¼Œä½¿ç”¨é»˜è®¤å€¼
    real_count = 1
    fake_count = 1
    print("âš ï¸ train_loaderæœªå®šä¹‰ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«æƒé‡")

# ç¡®ä¿è®¡æ•°ä¸ä¸ºé›¶
real_count = max(real_count, 1)
fake_count = max(fake_count, 1)

pos_weight = torch.tensor([real_count / fake_count], device=device)

print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ - çœŸå®: {real_count}, ä¼ªé€ : {fake_count}")
print(f"âš–ï¸ æ­£æ ·æœ¬æƒé‡: {pos_weight.item():.2f}")

# ä½¿ç”¨FocalLosså¤„ç†ç±»åˆ«ä¸å¹³è¡¡ - ResNet18ä¼˜åŒ–ç‰ˆæœ¬
criterion = FocalLoss(
    alpha=0.9,   # å¢åŠ alphaå€¼ï¼Œæ›´å¤šå…³æ³¨çœŸå®è§†é¢‘(å°‘æ•°ç±»)
    gamma=2.0,   # é€‚ä¸­çš„gammaå€¼ï¼Œå…³æ³¨å›°éš¾æ ·æœ¬ä½†ä¸è¿‡åº¦
    pos_weight=pos_weight,
    reduction='mean'
)

# ä¼˜åŒ–å™¨é…ç½® - ResNet18ä¼˜åŒ–ç‰ˆæœ¬
optimizer = optim.AdamW(
    model.parameters(),
    lr=2e-4,  # é€‚ä¸­çš„å­¦ä¹ ç‡ï¼Œé€‚åˆResNet18çš„æ”¶æ•›ç‰¹æ€§
    weight_decay=0.01,  # é€‚ä¸­çš„æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    betas=(0.9, 0.999),
    eps=1e-8
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ›´æ¿€è¿›çš„å­¦ä¹ ç‡ç­–ç•¥
scheduler = CosineAnnealingWarmRestarts(
    optimizer,
    T_0=5,  # å¢åŠ é‡å¯å‘¨æœŸï¼Œè®©æ¨¡å‹æœ‰æ›´å¤šæ—¶é—´å­¦ä¹ 
    T_mult=1,  # ä¿æŒå‘¨æœŸä¸å˜
    eta_min=1e-5  # æé«˜æœ€å°å­¦ä¹ ç‡
)

# æ—©åœæœºåˆ¶ - æ›´å®½æ¾çš„ç›‘æ§
early_stopping = EarlyStopping(
    patience=10,  # å¢åŠ è€å¿ƒå€¼ï¼Œç»™æ¨¡å‹æ›´å¤šå­¦ä¹ æ—¶é—´
    min_delta=0.005,  # é™ä½æœ€å°æ”¹è¿›é˜ˆå€¼
    restore_best_weights=True
)

# æ··åˆç²¾åº¦è®­ç»ƒ - æš‚æ—¶ç¦ç”¨ä»¥è§£å†³NaNé—®é¢˜
use_amp = False  # å¼ºåˆ¶ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œé¿å…æ•°å€¼ä¸ç¨³å®š
scaler = None
print("ğŸ“ ä½¿ç”¨FP32è®­ç»ƒ (è§£å†³NaNé—®é¢˜)")

# è®­ç»ƒé…ç½® - ResNet18ä¼˜åŒ–ç‰ˆæœ¬
num_epochs = 50  # ä¿æŒè®­ç»ƒè½®æ•°ï¼Œç»™æ¨¡å‹å……åˆ†å­¦ä¹ æ—¶é—´
print(f"ğŸ¯ ResNet18ä¼˜åŒ–è®­ç»ƒé…ç½®:")
print(f"  - ä¸»å¹²ç½‘ç»œ: ResNet18 (è½»é‡åŒ–)")
print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size} (ç›¸æ¯”ResNet50å¢åŠ 100%)")
print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
print(f"  - åˆå§‹å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
print(f"  - æƒé‡è¡°å‡: {optimizer.param_groups[0]['weight_decay']:.3f}")
print(f"  - FocalLosså‚æ•°: alpha={criterion.alpha:.1f}, gamma={criterion.gamma:.1f}")
print(f"  - æ—©åœè€å¿ƒå€¼: {early_stopping.patience}")
print(f"  - æ··åˆç²¾åº¦: {'å¯ç”¨' if use_amp else 'ç¦ç”¨'}")
print(f"  - æ¨¡å‹å¤æ‚åº¦: è½»é‡åŒ–ç‰ˆæœ¬ (~11Må‚æ•°)")

print("âœ… ResNet18è½»é‡åŒ–æ¨¡å‹å’Œè®­ç»ƒé…ç½®å®Œæˆ")