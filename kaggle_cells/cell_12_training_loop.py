# Cell 12: è®­ç»ƒå¾ªç¯

import os
import time

# ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs('./models', exist_ok=True)

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
print(f"ğŸ“Š è®­ç»ƒé…ç½®: {len(train_loader)} ä¸ªè®­ç»ƒæ‰¹æ¬¡, {len(val_loader)} ä¸ªéªŒè¯æ‰¹æ¬¡")
print(f"ğŸ¯ æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
print(f"ğŸ’¾ è®¾å¤‡: {device}")
print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")

if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    torch.cuda.reset_peak_memory_stats()

# è®­ç»ƒå†å²è®°å½•
train_history = {
    'train_loss': [],
    'val_loss': [],
    'train_acc': [],
    'val_acc': [],
    'val_auc': [],
    'val_precision': [],
    'val_recall': [],
    'val_f1': []
}

best_val_loss = float('inf')
best_val_acc = 0.0
best_val_auc = 0.0

# è®­ç»ƒå¾ªç¯
print("\nğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯...")
for epoch in range(num_epochs):
    epoch_start_time = time.time()
    print(f"\nEpoch {epoch+1}/{num_epochs}")
    
    # è®­ç»ƒé˜¶æ®µ
    train_results = train_epoch(
        model, train_loader, criterion, optimizer, device, 
        scheduler=scheduler, use_amp=True, gradient_clip=1.0
    )
    
    # éªŒè¯é˜¶æ®µ
    val_results = validate_epoch(
        model, val_loader, criterion, device
    )
    
    # æå–ç»“æœ
    train_loss = train_results['loss']
    train_acc = train_results['accuracy'] * 100
    
    val_loss = val_results['loss']
    val_acc = val_results['accuracy'] * 100
    val_auc = val_results['auc']
    val_precision = val_results['precision']
    val_recall = val_results['recall']
    val_f1 = val_results['f1']
    
    # è®°å½•å†å²
    train_history['train_loss'].append(train_loss)
    train_history['train_acc'].append(train_acc)
    train_history['val_loss'].append(val_loss)
    train_history['val_acc'].append(val_acc)
    train_history['val_auc'].append(val_auc)
    train_history['val_precision'].append(val_precision)
    train_history['val_recall'].append(val_recall)
    train_history['val_f1'].append(val_f1)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']
    
    # è®¡ç®—epochæ—¶é—´
    epoch_time = time.time() - epoch_start_time
    
    # æ‰“å°ç»“æœ
    print(f"è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
    print(f"éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.2f}%, AUC={val_auc:.4f}, F1={val_f1:.4f}")
    print(f"å­¦ä¹ ç‡: {current_lr:.2e}, ç”¨æ—¶: {epoch_time:.1f}s")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_loss = val_loss
        best_val_acc = val_acc
        best_val_auc = val_auc
        
        print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! Acc: {best_val_acc:.2f}%, AUC: {best_val_auc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_loss': best_val_loss,
            'best_val_acc': best_val_acc,
            'best_val_auc': best_val_auc,
            'train_history': train_history
        }, './models/best_model.pth')
        print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
    
    # æ—©åœæ£€æŸ¥
    if early_stopping(val_loss, model):
        print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
        break
    
    # æ¸…ç†GPUç¼“å­˜ - åŒT4 GPUå†…å­˜ç®¡ç†
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ - åŒT4æœ‰æ›´å¤§å†…å­˜å®¹é‡
        memory_used = torch.cuda.memory_allocated() / 1024**3
        if memory_used > 20:  # åŒT4å¯ä»¥ä½¿ç”¨æ›´å¤šå†…å­˜ï¼Œæé«˜é˜ˆå€¼åˆ°20GB
            torch.cuda.empty_cache()
            print(f"âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ ({memory_used:.1f}GB)ï¼Œå·²æ¸…ç†ç¼“å­˜")
    
    # æ£€æŸ¥è®­ç»ƒæ—¶é—´ï¼Œé˜²æ­¢è¶…æ—¶ - åŒT4å¯ä»¥è¿è¡Œæ›´é•¿æ—¶é—´
    total_time = time.time() - epoch_start_time
    if total_time > 7200:  # åŒT4å¯ä»¥è¿è¡Œæ›´é•¿æ—¶é—´ï¼Œæé«˜åˆ°2å°æ—¶
        print(f"â° è®­ç»ƒæ—¶é—´è¿‡é•¿ ({total_time/60:.1f}åˆ†é’Ÿ)ï¼Œæå‰åœæ­¢")
        break

print("\nâœ… è®­ç»ƒå®Œæˆ!")
print(f"ğŸ† æœ€ç»ˆæœ€ä½³æ€§èƒ½: Loss={best_val_loss:.4f}, Acc={best_val_acc:.2f}%, AUC={best_val_auc:.4f}")

if torch.cuda.is_available():
    print(f"ğŸ’¾ å³°å€¼GPUå†…å­˜ä½¿ç”¨: {torch.cuda.max_memory_allocated() / 1024**3:.1f}GB")

# ç»˜åˆ¶è®­ç»ƒå†å²
def plot_training_history():
    """ç»˜åˆ¶è®­ç»ƒå†å²å›¾è¡¨"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('è®­ç»ƒå†å²', fontsize=16, fontweight='bold')
    
    # Loss
    axes[0, 0].plot(train_history['train_loss'], label='è®­ç»ƒLoss', color='blue')
    axes[0, 0].plot(train_history['val_loss'], label='éªŒè¯Loss', color='red')
    axes[0, 0].set_title('Losså˜åŒ–')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # Accuracy
    axes[0, 1].plot(train_history['train_acc'], label='è®­ç»ƒAcc', color='blue')
    axes[0, 1].plot(train_history['val_acc'], label='éªŒè¯Acc', color='red')
    axes[0, 1].set_title('å‡†ç¡®ç‡å˜åŒ–')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy (%)')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # AUC
    axes[1, 0].plot(train_history['val_auc'], label='éªŒè¯AUC', color='red')
    axes[1, 0].set_title('AUCå˜åŒ–')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('AUC')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # F1 Score
    axes[1, 1].plot(train_history['val_f1'], label='éªŒè¯F1', color='red')
    axes[1, 1].set_title('F1åˆ†æ•°å˜åŒ–')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('F1 Score')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig('./models/training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

# ç»˜åˆ¶è®­ç»ƒå†å²
plot_training_history()

print("ğŸ“Š è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜åˆ° ./models/training_history.png")