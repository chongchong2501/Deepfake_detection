{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10125851,"sourceType":"datasetVersion","datasetId":6248577}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##  1.环境设置和数据下载","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第1段：环境设置和导入\n# =============================================================================\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# PyTorch相关\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\n# 机器学习指标\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n    roc_curve, auc\n)\n\n# 设置随机种子\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\n# 检查GPU可用性\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"使用设备: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU型号: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\n# 创建必要的目录\nos.makedirs('./data', exist_ok=True)\nos.makedirs('./models', exist_ok=True)\nos.makedirs('./logs', exist_ok=True)\nos.makedirs('./results', exist_ok=True)\n\nprint(\"✅ 环境设置完成\")","metadata":{"_uuid":"f6ebcf0d-d34a-4e53-b9c4-c85254661e41","_cell_guid":"4883bd2d-9463-4607-9e61-df4d76492a78","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-04T08:53:05.603687Z","iopub.execute_input":"2025-07-04T08:53:05.603950Z","iopub.status.idle":"2025-07-04T08:53:18.901874Z","shell.execute_reply.started":"2025-07-04T08:53:05.603923Z","shell.execute_reply":"2025-07-04T08:53:18.901086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  2.数据下载与预处理\n","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第2段：轻量级优化的数据下载和预处理（Kaggle友好版本）\n# =============================================================================\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport json\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\n# 检查是否在Kaggle环境中\nIS_KAGGLE = os.path.exists('/kaggle')\n\nif IS_KAGGLE:\n    BASE_DATA_DIR = '/kaggle/input/ff-c23/FaceForensics++_C23'\n    print(\"检测到Kaggle环境\")\n    print(f\"数据基础路径: {BASE_DATA_DIR}\")\nelse:\n    BASE_DATA_DIR = './FaceForensics++_C23'\n    print(\"本地环境\")\n\n# 内存友好的帧提取函数\ndef extract_frames_memory_efficient(video_path, max_frames=24, target_size=(160, 160), \n                                   quality_threshold=30, skip_frames=2):\n    \"\"\"\n    内存友好的帧提取函数\n    - 降低分辨率减少内存使用\n    - 减少帧数\n    - 添加跳帧机制\n    - 简化质量检测\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    \n    if not cap.isOpened():\n        print(f\"无法打开视频: {video_path}\")\n        return frames\n    \n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    if total_frames == 0:\n        cap.release()\n        return frames\n    \n    # 简化采样策略：均匀采样，避免复杂计算\n    if total_frames <= max_frames:\n        frame_indices = list(range(0, total_frames, skip_frames))\n    else:\n        step = max(1, total_frames // max_frames)\n        frame_indices = list(range(0, total_frames, step))[:max_frames]\n    \n    frame_count = 0\n    for frame_idx in frame_indices:\n        if frame_count >= max_frames:\n            break\n            \n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        ret, frame = cap.read()\n        \n        if ret:\n            # 转换颜色空间\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            \n            # 简化质量检测（使用更快的方法）\n            gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n            quality = cv2.Laplacian(gray, cv2.CV_64F).var()\n            \n            if quality > quality_threshold:\n                # 调整大小\n                frame = cv2.resize(frame, target_size)\n                frames.append(frame)\n                frame_count += 1\n    \n    cap.release()\n    \n    # 如果帧数不足，简单重复最后一帧\n    while len(frames) < max_frames and len(frames) > 0:\n        frames.append(frames[-1].copy())\n    \n    return frames[:max_frames]\n\n# 简化的视频处理函数\ndef process_videos_simple(base_data_dir, max_videos_per_class=80, max_frames=24):\n    \"\"\"\n    简化的视频处理函数，避免并发和复杂操作\n    \"\"\"\n    data_list = []\n    \n    # 定义类别映射\n    fake_methods = ['Deepfakes', 'Face2Face', 'FaceShifter', 'FaceSwap', 'NeuralTextures']\n    \n    print(\"开始处理真实视频...\")\n    # 处理真实视频\n    original_dir = os.path.join(base_data_dir, 'original')\n    if os.path.exists(original_dir):\n        video_files = [f for f in os.listdir(original_dir) \n                      if f.endswith(('.mp4', '.avi', '.mov'))]\n        \n        # 限制视频数量\n        if len(video_files) > max_videos_per_class:\n            video_files = random.sample(video_files, max_videos_per_class)\n        \n        print(f\"找到 {len(video_files)} 个真实视频\")\n        \n        for i, video_file in enumerate(tqdm(video_files, desc=\"处理真实视频\")):\n            try:\n                video_path = os.path.join(original_dir, video_file)\n                \n                # 提取帧\n                frames = extract_frames_memory_efficient(video_path, max_frames)\n                \n                if len(frames) > 0:\n                    # 保存帧数据路径\n                    frame_save_dir = os.path.join('./data', 'frames', 'real')\n                    os.makedirs(frame_save_dir, exist_ok=True)\n                    \n                    video_name = os.path.splitext(video_file)[0]\n                    frame_save_path = os.path.join(frame_save_dir, f\"{video_name}.npy\")\n                    \n                    # 保存帧数据\n                    np.save(frame_save_path, np.array(frames, dtype=np.uint8))\n                    \n                    data_list.append({\n                        'video_path': video_path,\n                        'frame_path': frame_save_path,\n                        'label': 0,\n                        'category': 'real',\n                        'method': 'original',\n                        'num_frames': len(frames),\n                        'video_name': video_name\n                    })\n                \n                # 每处理10个视频清理一次内存\n                if (i + 1) % 10 == 0:\n                    gc.collect()\n                    \n            except Exception as e:\n                print(f\"处理视频 {video_file} 时出错: {e}\")\n                continue\n    \n    print(\"开始处理伪造视频...\")\n    # 处理伪造视频\n    for method in fake_methods:\n        method_dir = os.path.join(base_data_dir, method)\n        if os.path.exists(method_dir):\n            video_files = [f for f in os.listdir(method_dir) \n                          if f.endswith(('.mp4', '.avi', '.mov'))]\n            \n            # 限制每种方法的视频数量\n            method_limit = max_videos_per_class // len(fake_methods)\n            if len(video_files) > method_limit:\n                video_files = random.sample(video_files, method_limit)\n            \n            print(f\"处理 {method}: {len(video_files)} 个视频\")\n            \n            for i, video_file in enumerate(tqdm(video_files, desc=f\"处理{method}\")):\n                try:\n                    video_path = os.path.join(method_dir, video_file)\n                    \n                    # 提取帧\n                    frames = extract_frames_memory_efficient(video_path, max_frames)\n                    \n                    if len(frames) > 0:\n                        # 保存帧数据路径\n                        frame_save_dir = os.path.join('./data', 'frames', 'fake')\n                        os.makedirs(frame_save_dir, exist_ok=True)\n                        \n                        video_name = os.path.splitext(video_file)[0]\n                        frame_save_path = os.path.join(frame_save_dir, f\"{method}_{video_name}.npy\")\n                        \n                        # 保存帧数据\n                        np.save(frame_save_path, np.array(frames, dtype=np.uint8))\n                        \n                        data_list.append({\n                            'video_path': video_path,\n                            'frame_path': frame_save_path,\n                            'label': 1,\n                            'category': 'fake',\n                            'method': method,\n                            'num_frames': len(frames),\n                            'video_name': video_name\n                        })\n                    \n                    # 每处理5个视频清理一次内存\n                    if (i + 1) % 5 == 0:\n                        gc.collect()\n                        \n                except Exception as e:\n                    print(f\"处理视频 {video_file} 时出错: {e}\")\n                    continue\n    \n    print(f\"总共成功处理了 {len(data_list)} 个视频\")\n    print(f\"真实视频: {len([d for d in data_list if d['label'] == 0])} 个\")\n    print(f\"伪造视频: {len([d for d in data_list if d['label'] == 1])} 个\")\n    \n    return data_list\n\n# 简化的数据集划分\ndef create_simple_dataset_split(data_list, test_size=0.2, val_size=0.1):\n    \"\"\"\n    简化的数据集划分，避免复杂的分层采样\n    \"\"\"\n    df = pd.DataFrame(data_list)\n    \n    # 简单的分层划分\n    train_df, test_df = train_test_split(\n        df, test_size=test_size, random_state=42, \n        stratify=df['label'] if len(df) > 10 else None\n    )\n    \n    if val_size > 0 and len(train_df) > 10:\n        train_df, val_df = train_test_split(\n            train_df, test_size=val_size/(1-test_size), random_state=42,\n            stratify=train_df['label'] if len(train_df) > 10 else None\n        )\n        return train_df, val_df, test_df\n    \n    return train_df, test_df\n\n# 数据质量检查\ndef check_data_quality(data_list):\n    \"\"\"\n    简单的数据质量检查\n    \"\"\"\n    if not data_list:\n        print(\"❌ 没有有效的数据\")\n        return False\n    \n    df = pd.DataFrame(data_list)\n    \n    print(\"\\n=== 数据统计 ===\")\n    print(f\"总样本数: {len(df)}\")\n    print(f\"真实视频: {len(df[df['label']==0])} 个\")\n    print(f\"伪造视频: {len(df[df['label']==1])} 个\")\n    \n    print(\"\\n各方法分布:\")\n    method_counts = df['method'].value_counts()\n    for method, count in method_counts.items():\n        print(f\"  {method}: {count} 个\")\n    \n    # 检查数据平衡性\n    real_count = len(df[df['label']==0])\n    fake_count = len(df[df['label']==1])\n    \n    if real_count == 0 or fake_count == 0:\n        print(\"⚠️ 数据严重不平衡，缺少某一类别\")\n        return False\n    \n    ratio = min(real_count, fake_count) / max(real_count, fake_count)\n    if ratio < 0.3:\n        print(f\"⚠️ 数据不平衡，比例: {ratio:.2f}\")\n    else:\n        print(f\"✅ 数据平衡性良好，比例: {ratio:.2f}\")\n    \n    return True\n\n# 主处理流程\nprint(\"=== 开始数据预处理 ===\")\n\n# 检查数据目录\nif IS_KAGGLE and os.path.exists(BASE_DATA_DIR):\n    print(\"检查数据目录结构...\")\n    subdirs = [d for d in os.listdir(BASE_DATA_DIR) \n              if os.path.isdir(os.path.join(BASE_DATA_DIR, d))]\n    print(f\"找到子目录: {subdirs}\")\n    \n    for subdir in subdirs[:6]:  # 只显示前6个，避免输出过多\n        subdir_path = os.path.join(BASE_DATA_DIR, subdir)\n        try:\n            video_files = [f for f in os.listdir(subdir_path) \n                          if f.endswith(('.mp4', '.avi', '.mov'))]\n            print(f\"  {subdir}: {len(video_files)} 个视频文件\")\n        except:\n            print(f\"  {subdir}: 无法访问\")\n\n# 检查是否已有处理好的数据\nif (os.path.exists('./data/train.csv') and \n    os.path.exists('./data/val.csv') and \n    os.path.exists('./data/test.csv')):\n    \n    print(\"✅ 发现已有预处理数据\")\n    train_df = pd.read_csv('./data/train.csv')\n    val_df = pd.read_csv('./data/val.csv')\n    test_df = pd.read_csv('./data/test.csv')\n    \n    print(f\"训练集: {len(train_df)} 个样本\")\n    print(f\"验证集: {len(val_df)} 个样本\")\n    print(f\"测试集: {len(test_df)} 个样本\")\nelse:\n    print(\"开始处理视频数据...\")\n    \n    try:\n        # 处理视频\n        data_list = process_videos_simple(\n            BASE_DATA_DIR, \n            max_videos_per_class=200,  # 视频数量\n            max_frames=30  # 帧数\n        )\n        \n        # 检查数据质量\n        if not check_data_quality(data_list):\n            print(\"❌ 数据质量检查失败\")\n        else:\n            # 创建数据集划分\n            print(\"\\n创建数据集划分...\")\n            train_df, val_df, test_df = create_simple_dataset_split(\n                data_list, test_size=0.15, val_size=0.15\n            )\n            \n            # 保存数据集\n            print(\"保存数据集文件...\")\n            train_df.to_csv('./data/train.csv', index=False)\n            val_df.to_csv('./data/val.csv', index=False)\n            test_df.to_csv('./data/test.csv', index=False)\n            \n            # 保存处理信息\n            process_info = {\n                'total_samples': len(data_list),\n                'train_samples': len(train_df),\n                'val_samples': len(val_df),\n                'test_samples': len(test_df),\n                'processed_time': pd.Timestamp.now().isoformat(),\n                'max_frames': 20,\n                'target_size': [160, 160]\n            }\n            \n            with open('./data/process_info.json', 'w') as f:\n                json.dump(process_info, f, indent=2)\n            \n            print(f\"\\n✅ 数据处理完成\")\n            print(f\"训练集: {len(train_df)} 个样本\")\n            print(f\"验证集: {len(val_df)} 个样本\")\n            print(f\"测试集: {len(test_df)} 个样本\")\n            \n            # 最终内存清理\n            del data_list\n            gc.collect()\n            \n    except Exception as e:\n        print(f\"❌ 处理过程中出现错误: {e}\")\n        print(\"建议检查数据路径和可用内存\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T08:53:18.903778Z","iopub.execute_input":"2025-07-04T08:53:18.904147Z","iopub.status.idle":"2025-07-04T09:38:13.335969Z","shell.execute_reply.started":"2025-07-04T08:53:18.904127Z","shell.execute_reply":"2025-07-04T09:38:13.335048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.数据集类定义 ","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第3段：优化的数据集类定义\n# =============================================================================\n\nimport torch.nn.functional as F\nfrom torch.utils.data import WeightedRandomSampler\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom torchvision.transforms import InterpolationMode\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport random\nfrom collections import Counter\n\nclass AdvancedDeepfakeDataset(Dataset):\n    \"\"\"\n    高级深度伪造检测数据集（无imgaug依赖版本）\n    \"\"\"\n    def __init__(self, csv_file, transform=None, max_frames=32, \n                 augment_prob=0.5, temporal_augment=True, \n                 mixup_alpha=0.2, cutmix_alpha=1.0, mode='train'):\n        \n        self.data = pd.read_csv(csv_file)\n        self.transform = transform\n        self.max_frames = max_frames\n        self.augment_prob = augment_prob\n        self.temporal_augment = temporal_augment\n        self.mixup_alpha = mixup_alpha\n        self.cutmix_alpha = cutmix_alpha\n        self.mode = mode\n        \n        # 创建类别权重（用于处理不平衡数据）\n        self.class_weights = self._calculate_class_weights()\n        \n        # 初始化数据增强\n        self._init_augmentations()\n        \n        print(f\"数据集初始化完成: {len(self.data)} 个样本 ({mode} 模式)\")\n        print(f\"真实视频: {len(self.data[self.data['label']==0])} 个\")\n        print(f\"伪造视频: {len(self.data[self.data['label']==1])} 个\")\n    \n    def _calculate_class_weights(self):\n        \"\"\"计算类别权重\"\"\"\n        class_counts = self.data['label'].value_counts().sort_index()\n        total_samples = len(self.data)\n        weights = total_samples / (len(class_counts) * class_counts.values)\n        return torch.FloatTensor(weights)\n    \n    def _init_augmentations(self):\n        \"\"\"初始化数据增强（使用torchvision替代imgaug）\"\"\"\n        # 空间增强（使用torchvision）\n        self.spatial_transforms = [\n            T.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),\n            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            T.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n            T.RandomAutocontrast(p=0.2),\n            T.RandomEqualize(p=0.2),\n        ]\n        \n        # 时序增强\n        self.temporal_augs = {\n            'frame_drop': 0.1,      # 随机丢弃帧\n            'frame_repeat': 0.1,    # 随机重复帧\n            'temporal_shift': 0.2,  # 时序偏移\n            'reverse': 0.05,        # 时序反转\n        }\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def _load_and_preprocess_frames(self, frame_path):\n        \"\"\"加载和预处理帧数据\"\"\"\n        try:\n            frames = np.load(frame_path)\n            \n            # 确保帧数量\n            if len(frames) < self.max_frames:\n                # 智能填充：使用插值而不是简单重复\n                indices = np.linspace(0, len(frames)-1, self.max_frames)\n                new_frames = []\n                for i in indices:\n                    if i == int(i):\n                        new_frames.append(frames[int(i)])\n                    else:\n                        # 线性插值\n                        i1, i2 = int(i), min(int(i)+1, len(frames)-1)\n                        alpha = i - i1\n                        frame = (1-alpha) * frames[i1] + alpha * frames[i2]\n                        new_frames.append(frame.astype(np.uint8))\n                frames = np.array(new_frames)\n            elif len(frames) > self.max_frames:\n                # 智能采样：保留关键帧\n                indices = self._select_key_frames(frames, self.max_frames)\n                frames = frames[indices]\n            \n            return frames\n        except Exception as e:\n            print(f\"加载帧数据失败 {frame_path}: {e}\")\n            # 返回随机帧作为fallback\n            return np.random.randint(0, 255, (self.max_frames, 224, 224, 3), dtype=np.uint8)\n    \n    def _select_key_frames(self, frames, target_count):\n        \"\"\"选择关键帧\"\"\"\n        if len(frames) <= target_count:\n            return np.arange(len(frames))\n        \n        # 计算帧间差异\n        frame_diffs = []\n        for i in range(1, len(frames)):\n            diff = np.mean(np.abs(frames[i].astype(float) - frames[i-1].astype(float)))\n            frame_diffs.append(diff)\n        \n        # 选择变化最大的帧\n        key_indices = [0]  # 总是包含第一帧\n        \n        # 基于差异选择帧\n        remaining_count = target_count - 2  # 减去首尾帧\n        if remaining_count > 0:\n            diff_indices = np.argsort(frame_diffs)[-remaining_count:]\n            key_indices.extend(sorted(diff_indices + 1))  # +1因为diff_indices是相对于frames[1:]的\n        \n        key_indices.append(len(frames) - 1)  # 总是包含最后一帧\n        \n        return sorted(list(set(key_indices)))[:target_count]\n    \n    def _apply_temporal_augmentation(self, frames):\n        \"\"\"应用时序增强\"\"\"\n        if not self.temporal_augment or self.mode != 'train':\n            return frames\n        \n        frames = frames.copy()\n        \n        # 随机丢弃帧\n        if random.random() < self.temporal_augs['frame_drop']:\n            drop_count = random.randint(1, min(3, len(frames)//4))\n            drop_indices = random.sample(range(len(frames)), drop_count)\n            for idx in sorted(drop_indices, reverse=True):\n                if len(frames) > self.max_frames // 2:  # 确保不会丢弃太多帧\n                    frames = np.delete(frames, idx, axis=0)\n        \n        # 随机重复帧\n        if random.random() < self.temporal_augs['frame_repeat']:\n            repeat_idx = random.randint(0, len(frames)-1)\n            frames = np.insert(frames, repeat_idx, frames[repeat_idx], axis=0)\n        \n        # 时序偏移\n        if random.random() < self.temporal_augs['temporal_shift']:\n            shift = random.randint(-2, 2)\n            if shift != 0:\n                frames = np.roll(frames, shift, axis=0)\n        \n        # 时序反转\n        if random.random() < self.temporal_augs['reverse']:\n            frames = frames[::-1]\n        \n        # 确保帧数量\n        if len(frames) != self.max_frames:\n            if len(frames) < self.max_frames:\n                # 重复最后几帧\n                repeat_count = self.max_frames - len(frames)\n                last_frames = frames[-repeat_count:]\n                frames = np.concatenate([frames, last_frames], axis=0)\n            else:\n                # 截断\n                frames = frames[:self.max_frames]\n        \n        return frames\n    \n    def _apply_spatial_augmentation(self, frames):\n        \"\"\"应用空间增强（使用torchvision替代imgaug）\"\"\"\n        if self.mode != 'train' or random.random() > self.augment_prob:\n            return frames\n        \n        # 对每一帧应用增强\n        augmented_frames = []\n        for frame in frames:\n            if random.random() < 0.7:  # 70%的概率对单帧进行增强\n                # 转换为PIL图像\n                frame_pil = T.ToPILImage()(torch.tensor(frame).permute(2, 0, 1))\n                \n                # 随机选择一个增强\n                if self.spatial_transforms:\n                    aug = random.choice(self.spatial_transforms)\n                    frame_pil = aug(frame_pil)\n                \n                # 转换回numpy\n                frame = np.array(frame_pil)\n            \n            augmented_frames.append(frame)\n        \n        return np.array(augmented_frames)\n    \n    def _apply_mixup(self, frames1, label1, frames2, label2):\n        \"\"\"应用MixUp增强\"\"\"\n        if self.mixup_alpha <= 0:\n            return frames1, label1\n        \n        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n        mixed_frames = lam * frames1 + (1 - lam) * frames2\n        mixed_label = lam * label1 + (1 - lam) * label2\n        \n        return mixed_frames, mixed_label\n    \n    def _apply_cutmix(self, frames, label):\n        \"\"\"应用CutMix增强\"\"\"\n        if self.cutmix_alpha <= 0 or self.mode != 'train':\n            return frames, label\n        \n        # 随机选择另一个样本\n        mix_idx = random.randint(0, len(self.data) - 1)\n        mix_row = self.data.iloc[mix_idx]\n        mix_frames = self._load_and_preprocess_frames(mix_row['frame_path'])\n        mix_label = mix_row['label']\n        \n        # 应用CutMix\n        lam = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n        \n        H, W = frames.shape[1], frames.shape[2]\n        cut_rat = np.sqrt(1. - lam)\n        cut_w = int(W * cut_rat)\n        cut_h = int(H * cut_rat)\n        \n        cx = np.random.randint(W)\n        cy = np.random.randint(H)\n        \n        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n        bby1 = np.clip(cy - cut_h // 2, 0, H)\n        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n        bby2 = np.clip(cy + cut_h // 2, 0, H)\n        \n        frames[:, bby1:bby2, bbx1:bbx2, :] = mix_frames[:, bby1:bby2, bbx1:bbx2, :]\n        \n        # 调整标签\n        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n        mixed_label = lam * label + (1 - lam) * mix_label\n        \n        return frames, mixed_label\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # 加载帧数据\n        frames = self._load_and_preprocess_frames(row['frame_path'])\n        label = float(row['label'])\n        \n        # 应用时序增强\n        frames = self._apply_temporal_augmentation(frames)\n        \n        # 应用空间增强\n        frames = self._apply_spatial_augmentation(frames)\n        \n        # 应用CutMix（训练时）\n        if self.mode == 'train' and random.random() < 0.1:\n            frames, label = self._apply_cutmix(frames, label)\n        \n        # 转换为tensor\n        if self.transform:\n            transformed_frames = []\n            for frame in frames:\n                # 确保frame是uint8类型\n                if frame.dtype != np.uint8:\n                    frame = np.clip(frame, 0, 255).astype(np.uint8)\n                transformed_frame = self.transform(frame)\n                transformed_frames.append(transformed_frame)\n            frames = torch.stack(transformed_frames)\n        else:\n            frames = torch.tensor(frames, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n        \n        # 添加额外的元数据\n        metadata = {\n            'video_name': row.get('video_name', ''),\n            'method': row.get('method', ''),\n            'avg_quality': row.get('avg_quality', 0.0)\n        }\n        \n        return frames, torch.tensor(label, dtype=torch.float32), metadata\n\n# 创建加权采样器\ndef create_weighted_sampler(dataset):\n    \"\"\"创建加权随机采样器以处理类别不平衡\"\"\"\n    labels = [dataset.data.iloc[i]['label'] for i in range(len(dataset))]\n    class_counts = Counter(labels)\n    \n    # 计算每个样本的权重\n    weights = []\n    for label in labels:\n        weight = 1.0 / class_counts[label]\n        weights.append(weight)\n    \n    return WeightedRandomSampler(weights, len(weights), replacement=True)\n\n# 优化的数据变换\ndef get_optimized_transforms(mode='train', image_size=224):\n    \"\"\"获取优化的数据变换\"\"\"\n    if mode == 'train':\n        transform = A.Compose([\n            A.Resize(image_size, image_size),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n            A.OneOf([\n                A.MotionBlur(blur_limit=3, p=0.2),\n                A.GaussianBlur(blur_limit=3, p=0.2),\n                A.MedianBlur(blur_limit=3, p=0.2)\n            ], p=0.2),\n            A.OneOf([\n                A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n                A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=0.2),\n            ], p=0.2),\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.3),\n            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.2),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ToTensorV2()\n        ])\n    else:\n        transform = A.Compose([\n            A.Resize(image_size, image_size),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ToTensorV2()\n        ])\n    \n    return lambda x: transform(image=x)['image']\n\n# 创建数据加载器\ndef create_optimized_dataloaders(train_csv, val_csv, test_csv=None, \n                               batch_size=16, num_workers=4, \n                               max_frames=32, image_size=224):\n    \"\"\"创建优化的数据加载器\"\"\"\n    \n    # 获取变换\n    train_transform = get_optimized_transforms('train', image_size)\n    val_transform = get_optimized_transforms('val', image_size)\n    \n    # 创建数据集\n    train_dataset = AdvancedDeepfakeDataset(\n        train_csv, transform=train_transform, max_frames=max_frames,\n        augment_prob=0.6, temporal_augment=True, mode='train'\n    )\n    \n    val_dataset = AdvancedDeepfakeDataset(\n        val_csv, transform=val_transform, max_frames=max_frames,\n        augment_prob=0.0, temporal_augment=False, mode='val'\n    )\n    \n    # 创建采样器\n    train_sampler = create_weighted_sampler(train_dataset)\n    \n    # 创建数据加载器\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, sampler=train_sampler,\n        num_workers=num_workers, pin_memory=True, drop_last=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=True\n    )\n    \n    loaders = {'train': train_loader, 'val': val_loader}\n    \n    # 测试集（如果提供）\n    if test_csv and os.path.exists(test_csv):\n        test_dataset = AdvancedDeepfakeDataset(\n            test_csv, transform=val_transform, max_frames=max_frames,\n            augment_prob=0.0, temporal_augment=False, mode='test'\n        )\n        \n        test_loader = DataLoader(\n            test_dataset, batch_size=batch_size, shuffle=False,\n            num_workers=num_workers, pin_memory=True\n        )\n        \n        loaders['test'] = test_loader\n    \n    return loaders\n\nprint(\"✅ 数据集定义完成\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T09:46:42.140655Z","iopub.execute_input":"2025-07-04T09:46:42.140956Z","iopub.status.idle":"2025-07-04T09:46:43.475794Z","shell.execute_reply.started":"2025-07-04T09:46:42.140934Z","shell.execute_reply":"2025-07-04T09:46:43.475025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.模型定义","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第4段：模型定义\n# =============================================================================\n\n# 改进的CNN特征提取器\nclass ImprovedCNNFeatureExtractor(nn.Module):  \n    def __init__(self, pretrained=True, backbone='resnet50'):\n        super(ImprovedCNNFeatureExtractor, self).__init__()\n        \n        if backbone == 'resnet50':\n            self.backbone = models.resnet50(pretrained=pretrained)\n            self.feature_dim = 2048\n        elif backbone == 'efficientnet':\n            self.backbone = models.efficientnet_b0(pretrained=pretrained)\n            self.feature_dim = 1280\n        else:\n            self.backbone = models.resnet18(pretrained=pretrained)\n            self.feature_dim = 512\n            \n        # 移除最后的分类层\n        if hasattr(self.backbone, 'classifier'):\n            self.backbone.classifier = nn.Identity()\n        else:\n            self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n        \n        # 添加特征降维层\n        self.feature_reducer = nn.Sequential(\n            nn.Linear(self.feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        self.output_dim = 512\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        features = features.view(features.size(0), -1)\n        features = self.feature_reducer(features)\n        return features\n\n# 改进的注意力层\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_dim, num_heads=8):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.head_dim = hidden_dim // num_heads\n        \n        self.query = nn.Linear(hidden_dim, hidden_dim)\n        self.key = nn.Linear(hidden_dim, hidden_dim)\n        self.value = nn.Linear(hidden_dim, hidden_dim)\n        self.output = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, x):\n        batch_size, seq_len, hidden_dim = x.size()\n        \n        # 计算Q, K, V\n        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # 计算注意力分数\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        attention_weights = torch.softmax(scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        \n        # 应用注意力\n        attended = torch.matmul(attention_weights, V)\n        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n        \n        # 输出投影\n        output = self.output(attended)\n        \n        # 全局平均池化\n        global_attended = torch.mean(output, dim=1)\n        \n        return global_attended, attention_weights.mean(dim=1)\n\n# 优化的深度伪造检测模型\nclass OptimizedDeepfakeDetector(nn.Module):\n    def __init__(self, num_classes=1, hidden_dim=512, num_layers=3, dropout=0.3, backbone='resnet50'):\n        super(OptimizedDeepfakeDetector, self).__init__()\n        \n        # 改进的CNN特征提取器\n        self.cnn = ImprovedCNNFeatureExtractor(pretrained=True, backbone=backbone)\n        \n        # 双向LSTM层\n        self.lstm = nn.LSTM(\n            input_size=self.cnn.output_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True\n        )\n        \n        # 多头注意力机制\n        self.attention = MultiHeadAttention(hidden_dim * 2, num_heads=8)\n        \n        # 改进的分类器\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.BatchNorm1d(hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout // 2),\n            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n            nn.ReLU(),\n            nn.Dropout(dropout // 4),\n            nn.Linear(hidden_dim // 4, num_classes)\n        )\n        \n        # 初始化权重\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.LSTM):\n                for name, param in m.named_parameters():\n                    if 'weight' in name:\n                        nn.init.xavier_uniform_(param)\n                    elif 'bias' in name:\n                        nn.init.constant_(param, 0)\n    \n    def forward(self, x):\n        batch_size, seq_len, channels, height, width = x.size()\n        \n        # CNN特征提取\n        x = x.view(batch_size * seq_len, channels, height, width)\n        features = self.cnn(x)\n        features = features.view(batch_size, seq_len, -1)\n        \n        # LSTM处理\n        lstm_out, _ = self.lstm(features)\n        \n        # 多头注意力机制\n        attended, attention_weights = self.attention(lstm_out)\n        \n        # 分类\n        output = self.classifier(attended)\n        \n        return torch.sigmoid(output.squeeze()), attention_weights\n\n# 集成模型\nclass EnsembleDeepfakeDetector(nn.Module):\n    def __init__(self, num_classes=1, hidden_dim=512):\n        super(EnsembleDeepfakeDetector, self).__init__()\n        \n        # 多个不同的模型\n        self.model1 = OptimizedDeepfakeDetector(num_classes, hidden_dim, backbone='resnet50')\n        self.model2 = OptimizedDeepfakeDetector(num_classes, hidden_dim//2, backbone='efficientnet')\n        \n        # 融合层\n        self.fusion = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        out1, _ = self.model1(x)\n        out2, _ = self.model2(x)\n        \n        # 融合预测结果\n        combined = torch.stack([out1, out2], dim=1)\n        final_output = self.fusion(combined)\n        \n        return final_output.squeeze(), None\n\n# 模型创建函数\ndef create_optimized_model(model_type='optimized', hidden_dim=512, backbone='resnet50'):\n    \"\"\"\n    创建优化后的模型\n    \n    Args:\n        model_type: 'optimized' 或 'ensemble'\n        hidden_dim: 隐藏层维度\n        backbone: CNN骨干网络\n    \"\"\"\n    if model_type == 'ensemble':\n        model = EnsembleDeepfakeDetector(num_classes=1, hidden_dim=hidden_dim)\n    else:\n        model = OptimizedDeepfakeDetector(\n            num_classes=1, \n            hidden_dim=hidden_dim, \n            num_layers=3, \n            dropout=0.3,\n            backbone=backbone\n        )\n    \n    return model\n\nprint(\"✅ 模型定义完成\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T09:46:50.517574Z","iopub.execute_input":"2025-07-04T09:46:50.517840Z","iopub.status.idle":"2025-07-04T09:46:50.540119Z","shell.execute_reply.started":"2025-07-04T09:46:50.517819Z","shell.execute_reply":"2025-07-04T09:46:50.539457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.训练和验证函数","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第5段：训练和验证函数\n# =============================================================================\n\n# 改进的数据增强\ndef get_enhanced_transforms(is_training=True):\n    if is_training:\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((144, 144)),  # 稍大一些然后裁剪\n            transforms.RandomCrop((128, 128)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.RandomRotation(degrees=5),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))\n        ])\n    else:\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((128, 128)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n# 焦点损失函数\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n    \n    def forward(self, inputs, targets):\n        bce_loss = nn.BCELoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n# 改进的训练函数\ndef train_epoch(model, train_loader, criterion, optimizer, device, epoch, total_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    # 使用tqdm显示进度\n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{total_epochs} [Train]')\n    \n    for batch_idx, (data, target) in enumerate(pbar):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        \n        # 前向传播\n        output, _ = model(data)\n        loss = criterion(output, target)\n        \n        # 反向传播\n        loss.backward()\n        \n        # 梯度裁剪\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        # 统计\n        running_loss += loss.item()\n        predicted = (output > 0.5).float()\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n        \n        # 更新进度条\n        pbar.set_postfix({\n            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n            'Acc': f'{100.*correct/total:.2f}%'\n        })\n    \n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = 100. * correct / total\n    \n    return epoch_loss, epoch_acc\n\n# 改进的验证函数\ndef validate_epoch(model, val_loader, criterion, device, epoch, total_epochs):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_predictions = []\n    all_targets = []\n    \n    pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{total_epochs} [Val]')\n    \n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(pbar):\n            data, target = data.to(device), target.to(device)\n            \n            output, _ = model(data)\n            loss = criterion(output, target)\n            \n            running_loss += loss.item()\n            predicted = (output > 0.5).float()\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            \n            # 收集预测和目标用于计算指标\n            all_predictions.extend(output.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            \n            pbar.set_postfix({\n                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n    \n    epoch_loss = running_loss / len(val_loader)\n    epoch_acc = 100. * correct / total\n    \n    # 计算AUC\n    try:\n        auc_score = roc_auc_score(all_targets, all_predictions)\n    except:\n        auc_score = 0.0\n    \n    return epoch_loss, epoch_acc, auc_score\n\n# 学习率调度器\nclass CosineAnnealingWarmRestarts(optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1):\n        self.T_0 = T_0\n        self.T_i = T_0\n        self.T_mult = T_mult\n        self.eta_min = eta_min\n        self.T_cur = last_epoch\n        super(CosineAnnealingWarmRestarts, self).__init__(optimizer, last_epoch)\n    \n    def get_lr(self):\n        return [self.eta_min + (base_lr - self.eta_min) * \n                (1 + np.cos(np.pi * self.T_cur / self.T_i)) / 2\n                for base_lr in self.base_lrs]\n    \n    def step(self, epoch=None):\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        self.T_cur = epoch\n        if epoch >= self.T_i:\n            self.T_cur = 0\n            self.T_i *= self.T_mult\n        super(CosineAnnealingWarmRestarts, self).step(epoch)\n\n# 早停机制\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n    \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n        \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()\n\n# 优化的训练主函数\ndef train_optimized_model(model, train_loader, val_loader, num_epochs=50, \n                         learning_rate=0.001, device='cuda', save_path='./models'):\n    \"\"\"\n    优化的模型训练函数\n    \"\"\"\n    # 使用焦点损失\n    criterion = FocalLoss(alpha=1, gamma=2)\n    \n    # 使用AdamW优化器\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    \n    # 学习率调度器\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n    \n    # 早停机制\n    early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n    \n    # 记录训练历史\n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': [], 'val_auc': []\n    }\n    \n    best_val_auc = 0.0\n    \n    print(f\"开始训练，共 {num_epochs} 个epoch\")\n    print(f\"设备: {device}\")\n    print(f\"模型参数数量: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n    \n    for epoch in range(num_epochs):\n        # 训练\n        train_loss, train_acc = train_epoch(\n            model, train_loader, criterion, optimizer, device, epoch, num_epochs\n        )\n        \n        # 验证\n        val_loss, val_acc, val_auc = validate_epoch(\n            model, val_loader, criterion, device, epoch, num_epochs\n        )\n        \n        # 更新学习率\n        scheduler.step()\n        \n        # 记录历史\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        history['val_auc'].append(val_auc)\n        \n        # 打印epoch结果\n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val AUC: {val_auc:.4f}')\n        print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n        \n        # 保存最佳模型\n        if val_auc > best_val_auc:\n            best_val_auc = val_auc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_auc': val_auc,\n                'history': history\n            }, f'{save_path}/best_model.pth')\n            print(f'  ✅ 保存最佳模型 (AUC: {val_auc:.4f})')\n        \n        # 早停检查\n        if early_stopping(val_loss, model):\n            print(f'早停触发，在第 {epoch+1} 个epoch停止训练')\n            break\n        \n        print('-' * 60)\n    \n    return history\n\nprint(\"✅ 优化后的训练和验证函数定义完成\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T09:46:55.894265Z","iopub.execute_input":"2025-07-04T09:46:55.894546Z","iopub.status.idle":"2025-07-04T09:46:55.920898Z","shell.execute_reply.started":"2025-07-04T09:46:55.894529Z","shell.execute_reply":"2025-07-04T09:46:55.920054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  6.模型训练","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第6段：模型训练\n# =============================================================================\n\nimport time\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler, autocast\n\n# 焦点损失函数 - 解决类别不平衡问题\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n    \n    def forward(self, inputs, targets):\n        ce_loss = nn.BCELoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\n# 早停机制\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n    \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n        \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()\n\n# 改进的数据增强\ntrain_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((144, 144)),  # 稍大一些，为裁剪留空间\n    transforms.RandomCrop((128, 128)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomRotation(degrees=10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))  # 随机擦除\n])\n\nval_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# 优化的训练函数\ndef train_epoch(model, train_loader, criterion, optimizer, device, scaler=None):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n    \n    # 创建进度条\n    pbar = tqdm(train_loader, desc='Training', leave=False)\n    \n    for batch_idx, (data, target) in enumerate(pbar):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        \n        # 使用混合精度训练\n        if scaler is not None:\n            with autocast():\n                output, _ = model(data)\n                loss = criterion(output, target)\n            \n            scaler.scale(loss).backward()\n            # 梯度裁剪\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            output, _ = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # 计算准确率\n        predicted = (output > 0.5).float()\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n        \n        # 收集预测和目标用于AUC计算\n        all_preds.extend(output.detach().cpu().numpy())\n        all_targets.extend(target.detach().cpu().numpy())\n        \n        # 更新进度条\n        pbar.set_postfix({\n            'Loss': f'{loss.item():.4f}',\n            'Acc': f'{100.*correct/total:.2f}%'\n        })\n    \n    avg_loss = total_loss / len(train_loader)\n    accuracy = 100. * correct / total\n    \n    # 计算AUC\n    try:\n        auc_score = roc_auc_score(all_targets, all_preds)\n    except:\n        auc_score = 0.0\n    \n    return avg_loss, accuracy, auc_score\n\n# 优化的验证函数\ndef validate_epoch(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        pbar = tqdm(val_loader, desc='Validation', leave=False)\n        \n        for data, target in pbar:\n            data, target = data.to(device), target.to(device)\n            \n            output, _ = model(data)\n            loss = criterion(output, target)\n            \n            total_loss += loss.item()\n            \n            # 计算准确率\n            predicted = (output > 0.5).float()\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            \n            # 收集预测和目标\n            all_preds.extend(output.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            \n            # 更新进度条\n            pbar.set_postfix({\n                'Loss': f'{loss.item():.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n    \n    avg_loss = total_loss / len(val_loader)\n    accuracy = 100. * correct / total\n    \n    # 计算AUC\n    try:\n        auc_score = roc_auc_score(all_targets, all_preds)\n    except:\n        auc_score = 0.0\n    \n    return avg_loss, accuracy, auc_score\n\n# 训练配置\nTRAIN_CONFIG = {\n    'batch_size': 8,  # 增加批次大小\n    'learning_rate': 1e-4,  # 降低学习率\n    'num_epochs': 50,  # 增加训练轮数\n    'weight_decay': 1e-4,\n    'patience': 10,  # 早停耐心值\n    'use_focal_loss': True,\n    'use_mixed_precision': True,\n    'gradient_clip': 1.0\n}\n\nprint(\"✅ 优化的模型训练函数定义完成\")\nprint(f\"训练配置: {TRAIN_CONFIG}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T09:47:00.757387Z","iopub.execute_input":"2025-07-04T09:47:00.758287Z","iopub.status.idle":"2025-07-04T09:47:00.778602Z","shell.execute_reply.started":"2025-07-04T09:47:00.758260Z","shell.execute_reply":"2025-07-04T09:47:00.777930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.执行训练循环","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第7段：执行训练循环\n# =============================================================================\n\n# 添加必要的导入\nimport os\nimport sys\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import GradScaler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom torchvision import transforms\nimport random\nfrom collections import Counter\n\n# 直接定义DeepfakeVideoDataset类（适用于Kaggle环境）\nclass DeepfakeVideoDataset(Dataset):\n    \"\"\"深度伪造检测数据集（Kaggle兼容版本）\"\"\"\n    def __init__(self, csv_file, transform=None, max_frames=30, mode='train'):\n        self.data = pd.read_csv(csv_file)\n        self.transform = transform\n        self.max_frames = max_frames\n        self.mode = mode\n        \n        print(f\"数据集初始化完成: {len(self.data)} 个样本 ({mode} 模式)\")\n        if 'label' in self.data.columns:\n            print(f\"真实视频: {len(self.data[self.data['label']==0])} 个\")\n            print(f\"伪造视频: {len(self.data[self.data['label']==1])} 个\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # 处理帧路径\n        if 'frames' in row:\n            try:\n                frame_paths = eval(row['frames'])  # 将字符串转换为列表\n                if isinstance(frame_paths, str):\n                    frame_paths = [frame_paths]\n            except:\n                frame_paths = [row['frames']]\n        elif 'frame_path' in row:\n            frame_paths = [row['frame_path']]\n        else:\n            # 如果没有找到帧路径，创建随机帧\n            frame_paths = []\n        \n        label = float(row['label']) if 'label' in row else 0.0\n        \n        # 加载帧\n        frames = []\n        for i, frame_path in enumerate(frame_paths[:self.max_frames]):\n            try:\n                if os.path.exists(frame_path):\n                    frame = cv2.imread(frame_path)\n                    if frame is not None:\n                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                        frame = cv2.resize(frame, (224, 224))\n                    else:\n                        frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n                else:\n                    frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n            except:\n                frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n            \n            if self.transform:\n                frame = self.transform(frame)\n            else:\n                frame = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1) / 255.0\n            \n            frames.append(frame)\n        \n        # 如果帧数不足，用零填充或重复最后一帧\n        while len(frames) < self.max_frames:\n            if frames:\n                frames.append(frames[-1].clone())\n            else:\n                # 如果没有帧，创建零张量\n                if self.transform:\n                    dummy_frame = np.zeros((224, 224, 3), dtype=np.uint8)\n                    frames.append(self.transform(dummy_frame))\n                else:\n                    frames.append(torch.zeros(3, 224, 224))\n        \n        # 将帧堆叠成张量\n        frames_tensor = torch.stack(frames[:self.max_frames])\n        \n        return frames_tensor, torch.tensor(label, dtype=torch.float32)\n\n# 定义简化的模型类（如果需要）\nclass OptimizedDeepfakeDetector(nn.Module):\n    \"\"\"优化的深度伪造检测模型\"\"\"\n    def __init__(self, backbone='resnet50', hidden_dim=512, num_layers=2, dropout=0.3, num_heads=8):\n        super().__init__()\n        \n        # 使用预训练的ResNet作为特征提取器\n        if backbone == 'resnet50':\n            from torchvision.models import resnet50\n            self.backbone = resnet50(pretrained=True)\n            self.backbone.fc = nn.Identity()  # 移除最后的分类层\n            feature_dim = 2048\n        else:\n            # 简化的CNN特征提取器\n            self.backbone = nn.Sequential(\n                nn.Conv2d(3, 64, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(64, 128, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(128, 256, 3, padding=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d((1, 1)),\n                nn.Flatten()\n            )\n            feature_dim = 256\n        \n        # 时序处理\n        self.lstm = nn.LSTM(feature_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        \n        # 注意力机制\n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n        \n        # 分类器\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        batch_size, seq_len, c, h, w = x.shape\n        \n        # 提取每帧特征\n        x = x.view(-1, c, h, w)\n        features = self.backbone(x)\n        features = features.view(batch_size, seq_len, -1)\n        \n        # LSTM处理时序信息\n        lstm_out, _ = self.lstm(features)\n        \n        # 注意力机制\n        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n        \n        # 全局平均池化\n        pooled = torch.mean(attn_out, dim=1)\n        \n        # 分类\n        output = self.classifier(pooled)\n        \n        return output.squeeze(-1)\n\n# 定义损失函数\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n        return focal_loss.mean()\n\n# 早停机制\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n    \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n        \n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n    \n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()\n\n# 训练函数\ndef train_epoch(model, dataloader, criterion, optimizer, device, scaler=None):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n    \n    for batch_idx, (data, target) in enumerate(dataloader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        \n        if scaler:\n            with torch.cuda.amp.autocast():\n                output = model(data)\n                loss = criterion(output, target)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n        \n        total_loss += loss.item()\n        all_preds.extend(output.detach().cpu().numpy())\n        all_targets.extend(target.detach().cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(np.array(all_targets) > 0.5, np.array(all_preds) > 0.5) * 100\n    auc = roc_auc_score(all_targets, all_preds)\n    \n    return avg_loss, accuracy, auc\n\n# 验证函数\ndef validate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for data, target in dataloader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            \n            total_loss += loss.item()\n            all_preds.extend(output.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = accuracy_score(np.array(all_targets) > 0.5, np.array(all_preds) > 0.5) * 100\n    auc = roc_auc_score(all_targets, all_preds)\n    \n    return avg_loss, accuracy, auc\n\n# 定义训练配置（保持您的原始参数）\nTRAIN_CONFIG = {\n    'batch_size': 16,\n    'learning_rate': 1e-4,\n    'weight_decay': 1e-4,\n    'num_epochs': 50,\n    'patience': 10,\n    'use_focal_loss': True,\n    'use_mixed_precision': True\n}\n\n# 设备配置\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"使用设备: {device}\")\n\n# 数据变换（保持您的原始配置）\ntrain_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# 创建必要的目录\nos.makedirs('./models', exist_ok=True)\nos.makedirs('./logs', exist_ok=True)\nos.makedirs('./results', exist_ok=True)\n\n# 创建优化的数据集和数据加载器\nprint(\"创建数据集和数据加载器...\")\n\n# 使用改进的数据增强\ntrain_dataset = DeepfakeVideoDataset('./data/train.csv', transform=train_transform, max_frames=30)\nval_dataset = DeepfakeVideoDataset('./data/val.csv', transform=val_transform, max_frames=30)\n\n# ... existing code ...","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T09:59:19.074794Z","iopub.execute_input":"2025-07-04T09:59:19.075519Z","iopub.status.idle":"2025-07-04T09:59:19.118144Z","shell.execute_reply.started":"2025-07-04T09:59:19.075494Z","shell.execute_reply":"2025-07-04T09:59:19.117334Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8.模型评估","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# 第8段：模型评估\n# =============================================================================\n\nimport time\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n    roc_curve, auc, precision_recall_curve, average_precision_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle\n\n# 优化的评估函数\ndef evaluate_model_optimized(model, test_loader, criterion, device, save_attention=True):\n    \"\"\"\n    优化的模型评估函数，包含更全面的指标和分析\n    \"\"\"\n    model.eval()\n    running_loss = 0.0\n    all_predictions = []\n    all_targets = []\n    all_scores = []\n    all_attention_weights = []\n    inference_times = []\n    \n    with torch.no_grad():\n        progress_bar = tqdm(test_loader, desc='模型评估中')\n        \n        for inputs, labels in progress_bar:\n            inputs = inputs.to(device)\n            labels = labels.float().to(device)\n            \n            # 记录推理时间\n            start_time = time.time()\n            \n            # 前向传播\n            if hasattr(model, 'forward') and len(inspect.signature(model.forward).parameters) > 1:\n                outputs, attention_weights = model(inputs)\n                if save_attention:\n                    all_attention_weights.append(attention_weights.cpu().numpy())\n            else:\n                outputs = model(inputs)\n                attention_weights = None\n            \n            inference_time = time.time() - start_time\n            inference_times.append(inference_time)\n            \n            outputs = outputs.squeeze()\n            \n            # 计算损失\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            \n            # 收集预测和目标\n            preds = (outputs > 0.5).float().cpu().numpy()\n            all_predictions.extend(preds)\n            all_targets.extend(labels.cpu().numpy())\n            all_scores.extend(outputs.cpu().numpy())\n            \n            # 更新进度条\n            progress_bar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'avg_time': f'{np.mean(inference_times):.3f}s'\n            })\n    \n    # 计算平均损失和推理时间\n    test_loss = running_loss / len(test_loader.dataset)\n    avg_inference_time = np.mean(inference_times)\n    \n    return {\n        'loss': test_loss,\n        'predictions': all_predictions,\n        'targets': all_targets,\n        'scores': all_scores,\n        'attention_weights': all_attention_weights,\n        'avg_inference_time': avg_inference_time,\n        'total_inference_time': sum(inference_times)\n    }\n\n# 计算全面的评估指标\ndef calculate_comprehensive_metrics(predictions, targets, scores):\n    \"\"\"\n    计算更全面的评估指标\n    \"\"\"\n    # 基础指标\n    accuracy = accuracy_score(targets, predictions)\n    precision = precision_score(targets, predictions, zero_division=0)\n    recall = recall_score(targets, predictions, zero_division=0)\n    f1 = f1_score(targets, predictions, zero_division=0)\n    \n    # ROC和PR曲线指标\n    if len(set(targets)) > 1:\n        auc_roc = roc_auc_score(targets, scores)\n        precision_vals, recall_vals, _ = precision_recall_curve(targets, scores)\n        auc_pr = average_precision_score(targets, scores)\n    else:\n        auc_roc = 0.0\n        auc_pr = 0.0\n    \n    # 混淆矩阵\n    cm = confusion_matrix(targets, predictions)\n    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n    \n    # 额外指标\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # 负预测值\n    balanced_accuracy = (recall + specificity) / 2\n    \n    # 打印详细结果\n    print(\"=\" * 60)\n    print(\"📊 模型评估结果\")\n    print(\"=\" * 60)\n    print(f\"🎯 准确率 (Accuracy): {accuracy:.4f}\")\n    print(f\"🎯 平衡准确率 (Balanced Accuracy): {balanced_accuracy:.4f}\")\n    print(f\"🔍 精确率 (Precision): {precision:.4f}\")\n    print(f\"🔍 召回率 (Recall/Sensitivity): {recall:.4f}\")\n    print(f\"🔍 特异性 (Specificity): {specificity:.4f}\")\n    print(f\"🔍 负预测值 (NPV): {npv:.4f}\")\n    print(f\"⚖️ F1分数: {f1:.4f}\")\n    print(f\"📈 AUC-ROC: {auc_roc:.4f}\")\n    print(f\"📈 AUC-PR: {auc_pr:.4f}\")\n    print(\"=\" * 60)\n    \n    # 混淆矩阵详情\n    print(\"\\n📋 混淆矩阵详情:\")\n    print(f\"真阴性 (TN): {tn}\")\n    print(f\"假阳性 (FP): {fp}\")\n    print(f\"假阴性 (FN): {fn}\")\n    print(f\"真阳性 (TP): {tp}\")\n    \n    # 分类报告\n    print(\"\\n📊 详细分类报告:\")\n    print(classification_report(targets, predictions, \n                             target_names=['真实视频', '伪造视频'],\n                             digits=4))\n    \n    return {\n        'accuracy': accuracy,\n        'balanced_accuracy': balanced_accuracy,\n        'precision': precision,\n        'recall': recall,\n        'specificity': specificity,\n        'npv': npv,\n        'f1': f1,\n        'auc_roc': auc_roc,\n        'auc_pr': auc_pr,\n        'confusion_matrix': cm,\n        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n    }\n\n# 增强的可视化函数\ndef plot_enhanced_confusion_matrix(cm, save_path=None):\n    \"\"\"\n    绘制增强的混淆矩阵\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n    \n    # 计算百分比\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    \n    # 创建标注\n    annotations = []\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            annotations.append(f'{cm[i,j]}\\n({cm_percent[i,j]:.1f}%)')\n    \n    annotations = np.array(annotations).reshape(cm.shape)\n    \n    # 绘制热力图\n    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', \n                cbar_kws={'label': '样本数量'},\n                xticklabels=['真实视频', '伪造视频'], \n                yticklabels=['真实视频', '伪造视频'])\n    \n    plt.xlabel('预测标签', fontsize=12, fontweight='bold')\n    plt.ylabel('真实标签', fontsize=12, fontweight='bold')\n    plt.title('混淆矩阵 (数量和百分比)', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"✅ 混淆矩阵已保存到: {save_path}\")\n    \n    plt.show()\n\n# 绘制ROC和PR曲线\ndef plot_roc_pr_curves(targets, scores, save_path=None):\n    \"\"\"\n    同时绘制ROC曲线和PR曲线\n    \"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # ROC曲线\n    fpr, tpr, _ = roc_curve(targets, scores)\n    roc_auc = auc(fpr, tpr)\n    \n    ax1.plot(fpr, tpr, color='darkorange', lw=2, \n             label=f'ROC曲线 (AUC = {roc_auc:.4f})')\n    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.8)\n    ax1.set_xlim([0.0, 1.0])\n    ax1.set_ylim([0.0, 1.05])\n    ax1.set_xlabel('假阳性率 (FPR)', fontweight='bold')\n    ax1.set_ylabel('真阳性率 (TPR)', fontweight='bold')\n    ax1.set_title('ROC曲线', fontsize=14, fontweight='bold')\n    ax1.legend(loc='lower right')\n    ax1.grid(True, linestyle='--', alpha=0.7)\n    \n    # PR曲线\n    precision_vals, recall_vals, _ = precision_recall_curve(targets, scores)\n    pr_auc = average_precision_score(targets, scores)\n    \n    ax2.plot(recall_vals, precision_vals, color='darkgreen', lw=2,\n             label=f'PR曲线 (AUC = {pr_auc:.4f})')\n    ax2.axhline(y=np.mean(targets), color='navy', linestyle='--', alpha=0.8,\n                label=f'基线 ({np.mean(targets):.3f})')\n    ax2.set_xlim([0.0, 1.0])\n    ax2.set_ylim([0.0, 1.05])\n    ax2.set_xlabel('召回率 (Recall)', fontweight='bold')\n    ax2.set_ylabel('精确率 (Precision)', fontweight='bold')\n    ax2.set_title('精确率-召回率曲线', fontsize=14, fontweight='bold')\n    ax2.legend(loc='lower left')\n    ax2.grid(True, linestyle='--', alpha=0.7)\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        print(f\"✅ ROC和PR曲线已保存到: {save_path}\")\n    \n    plt.show()\n\n# 注意力权重可视化\ndef visualize_attention_weights(attention_weights, save_path=None, num_samples=5):\n    \"\"\"\n    可视化注意力权重\n    \"\"\"\n    if not attention_weights:\n        print(\"⚠️ 没有注意力权重数据\")\n        return\n    \n    # 选择前几个样本进行可视化\n    num_samples = min(num_samples, len(attention_weights))\n    \n    fig, axes = plt.subplots(num_samples, 1, figsize=(12, 2*num_samples))\n    if num_samples == 1:\n        axes = [axes]\n    \n    for i in range(num_samples):\n        weights = attention_weights[i].squeeze()\n        if len(weights.shape) > 1:\n            weights = weights.mean(axis=0)  # 如果是多头注意力，取平均\n        \n        axes[i].bar(range(len(weights)), weights, alpha=0.7)\n        axes[i].set_title(f'样本 {i+1} 的注意力权重分布')\n        axes[i].set_xlabel('帧序号')\n        axes[i].set_ylabel('注意力权重')\n        axes[i].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')  \n        print(f\"✅ 注意力权重可视化已保存到: {save_path}\")\n    \n    plt.show()\n\nprint(\"✅ 优化的模型评估模块已定义\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T09:59:28.560238Z","iopub.execute_input":"2025-07-04T09:59:28.560588Z","iopub.status.idle":"2025-07-04T09:59:28.587722Z","shell.execute_reply.started":"2025-07-04T09:59:28.560562Z","shell.execute_reply":"2025-07-04T09:59:28.586950Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9.执行模型评估","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# Kaggle 兼容的深度伪造检测模型评估脚本\n# =============================================================================\n\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score, \n    roc_auc_score, confusion_matrix, classification_report, \n    roc_curve, auc, average_precision_score, precision_recall_curve\n)\nimport cv2\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# 设置设备\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"使用设备: {device}\")\n\n# 定义数据转换\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# 数据集类定义\nclass DeepfakeVideoDataset(Dataset):\n    \"\"\"深度伪造检测数据集（Kaggle兼容版本）\"\"\"\n    def __init__(self, csv_file, transform=None, max_frames=30, mode='train'):\n        self.data = pd.read_csv(csv_file)\n        self.transform = transform\n        self.max_frames = max_frames\n        self.mode = mode\n        \n        print(f\"数据集初始化完成: {len(self.data)} 个样本 ({mode} 模式)\")\n        if 'label' in self.data.columns:\n            print(f\"真实视频: {len(self.data[self.data['label']==0])} 个\")\n            print(f\"伪造视频: {len(self.data[self.data['label']==1])} 个\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        # 处理帧路径\n        if 'frames' in row:\n            try:\n                frame_paths = eval(row['frames'])  # 将字符串转换为列表\n                if isinstance(frame_paths, str):\n                    frame_paths = [frame_paths]\n            except:\n                frame_paths = [row['frames']]\n        elif 'frame_path' in row:\n            frame_paths = [row['frame_path']]\n        else:\n            frame_paths = []\n        \n        label = float(row['label']) if 'label' in row else 0.0\n        \n        # 加载帧\n        frames = []\n        for i, frame_path in enumerate(frame_paths[:self.max_frames]):\n            try:\n                if os.path.exists(frame_path):\n                    frame = cv2.imread(frame_path)\n                    if frame is not None:\n                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                        frame = cv2.resize(frame, (224, 224))\n                    else:\n                        frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n                else:\n                    frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n            except:\n                frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n            \n            if self.transform:\n                frame = self.transform(frame)\n            else:\n                frame = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1) / 255.0\n            \n            frames.append(frame)\n        \n        # 如果帧数不足，用零填充或重复最后一帧\n        while len(frames) < self.max_frames:\n            if frames:\n                frames.append(frames[-1].clone())\n            else:\n                if self.transform:\n                    dummy_frame = np.zeros((224, 224, 3), dtype=np.uint8)\n                    frames.append(self.transform(dummy_frame))\n                else:\n                    frames.append(torch.zeros(3, 224, 224))\n        \n        # 将帧堆叠成张量\n        frames_tensor = torch.stack(frames[:self.max_frames])\n        \n        return frames_tensor, torch.tensor(label, dtype=torch.float32)\n\n# 模型定义\nclass OptimizedDeepfakeDetector(nn.Module):\n    \"\"\"优化的深度伪造检测模型\"\"\"\n    def __init__(self, backbone='resnet50', hidden_dim=512, num_layers=2, dropout=0.3, use_attention=True):\n        super().__init__()\n        \n        # 使用预训练的ResNet作为特征提取器\n        if backbone == 'resnet50':\n            self.backbone = models.resnet50(pretrained=True)\n            self.backbone.fc = nn.Identity()  # 移除最后的分类层\n            feature_dim = 2048\n        else:\n            # 简化的CNN特征提取器\n            self.backbone = nn.Sequential(\n                nn.Conv2d(3, 64, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(64, 128, 3, padding=1),\n                nn.ReLU(),\n                nn.MaxPool2d(2),\n                nn.Conv2d(128, 256, 3, padding=1),\n                nn.ReLU(),\n                nn.AdaptiveAvgPool2d((1, 1)),\n                nn.Flatten()\n            )\n            feature_dim = 256\n        \n        # 时序处理\n        self.lstm = nn.LSTM(feature_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n        \n        # 注意力机制\n        self.use_attention = use_attention\n        if use_attention:\n            self.attention = nn.MultiheadAttention(hidden_dim, 8, dropout=dropout, batch_first=True)\n        \n        # 分类器\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        batch_size, seq_len, c, h, w = x.shape\n        \n        # 提取每帧特征\n        x = x.view(-1, c, h, w)\n        features = self.backbone(x)\n        features = features.view(batch_size, seq_len, -1)\n        \n        # LSTM处理时序信息\n        lstm_out, _ = self.lstm(features)\n        \n        # 注意力机制\n        if self.use_attention:\n            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n            pooled = torch.mean(attn_out, dim=1)\n        else:\n            pooled = torch.mean(lstm_out, dim=1)\n        \n        # 分类\n        output = self.classifier(pooled)\n        \n        return output.squeeze(-1)\n\n# 优化的模型评估函数\ndef evaluate_model_optimized(model, test_loader, criterion, device):\n    \"\"\"优化的模型评估函数\"\"\"\n    model.eval()\n    running_loss = 0.0\n    predictions = []\n    targets = []\n    scores = []\n    \n    total_inference_time = 0\n    \n    with torch.no_grad():\n        for batch_idx, (inputs, labels) in enumerate(test_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            # 记录推理时间\n            import time\n            start_time = time.time()\n            \n            # 前向传播\n            outputs = model(inputs)\n            \n            batch_time = time.time() - start_time\n            total_inference_time += batch_time\n            \n            if len(outputs.shape) > 1:\n                outputs = outputs.squeeze()\n            \n            # 计算损失\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            \n            # 收集预测结果\n            batch_preds = (outputs > 0.5).float().cpu().numpy()\n            batch_targets = labels.cpu().numpy()\n            batch_scores = outputs.cpu().numpy()\n            \n            predictions.extend(batch_preds)\n            targets.extend(batch_targets)\n            scores.extend(batch_scores)\n    \n    avg_loss = running_loss / len(test_loader.dataset)\n    avg_inference_time = total_inference_time / len(test_loader.dataset)\n    \n    return {\n        'loss': avg_loss,\n        'predictions': np.array(predictions),\n        'targets': np.array(targets),\n        'scores': np.array(scores),\n        'total_inference_time': total_inference_time,\n        'avg_inference_time': avg_inference_time\n    }\n\n# 计算全面的评估指标\ndef calculate_comprehensive_metrics(predictions, targets, scores):\n    \"\"\"计算全面的评估指标\"\"\"\n    # 基础指标\n    accuracy = accuracy_score(targets, predictions)\n    precision = precision_score(targets, predictions, zero_division=0)\n    recall = recall_score(targets, predictions, zero_division=0)\n    f1 = f1_score(targets, predictions, zero_division=0)\n    \n    # 混淆矩阵\n    cm = confusion_matrix(targets, predictions)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # 额外指标\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # 负预测值\n    balanced_accuracy = (recall + specificity) / 2\n    \n    # AUC指标\n    try:\n        auc_roc = roc_auc_score(targets, scores)\n        auc_pr = average_precision_score(targets, scores)\n    except:\n        auc_roc = 0.0\n        auc_pr = 0.0\n    \n    return {\n        'accuracy': accuracy,\n        'balanced_accuracy': balanced_accuracy,\n        'precision': precision,\n        'recall': recall,\n        'specificity': specificity,\n        'f1': f1,\n        'npv': npv,\n        'auc_roc': auc_roc,\n        'auc_pr': auc_pr,\n        'confusion_matrix': cm,\n        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n    }\n\n# 增强的混淆矩阵可视化\ndef plot_enhanced_confusion_matrix(cm, save_path):\n    \"\"\"绘制增强的混淆矩阵\"\"\"\n    plt.figure(figsize=(10, 8))\n    \n    # 计算百分比\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    \n    # 创建标签\n    labels = np.array([[\n        f'{cm[i,j]}\\n({cm_percent[i,j]:.1f}%)' \n        for j in range(cm.shape[1])\n    ] for i in range(cm.shape[0])])\n    \n    # 绘制热力图\n    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', \n                xticklabels=['真实', '伪造'], \n                yticklabels=['真实', '伪造'],\n                cbar_kws={'label': '样本数量'})\n    \n    plt.xlabel('预测标签', fontsize=12)\n    plt.ylabel('真实标签', fontsize=12)\n    plt.title('增强混淆矩阵', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n# ROC和PR曲线\ndef plot_roc_pr_curves(targets, scores, save_path):\n    \"\"\"绘制ROC和PR曲线\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # ROC曲线\n    fpr, tpr, _ = roc_curve(targets, scores)\n    roc_auc = auc(fpr, tpr)\n    \n    ax1.plot(fpr, tpr, color='darkorange', lw=2, \n             label=f'ROC曲线 (AUC = {roc_auc:.4f})')\n    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax1.set_xlim([0.0, 1.0])\n    ax1.set_ylim([0.0, 1.05])\n    ax1.set_xlabel('假阳性率')\n    ax1.set_ylabel('真阳性率')\n    ax1.set_title('ROC曲线')\n    ax1.legend(loc='lower right')\n    ax1.grid(True, alpha=0.3)\n    \n    # PR曲线\n    precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)\n    pr_auc = auc(recall_curve, precision_curve)\n    \n    ax2.plot(recall_curve, precision_curve, color='darkgreen', lw=2,\n             label=f'PR曲线 (AUC = {pr_auc:.4f})')\n    ax2.set_xlim([0.0, 1.0])\n    ax2.set_ylim([0.0, 1.05])\n    ax2.set_xlabel('召回率')\n    ax2.set_ylabel('精确率')\n    ax2.set_title('精确率-召回率曲线')\n    ax2.legend(loc='lower left')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n# 主评估流程\nprint(\"🔄 创建优化的测试数据加载器...\")\n\n# 使用更大的批次大小以提高效率\noptimized_batch_size = 16 if torch.cuda.is_available() else 8\n\ntest_dataset = DeepfakeVideoDataset(\n    csv_file='./data/val.csv',\n    transform=transform,\n    max_frames=30\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=optimized_batch_size,\n    shuffle=False,\n    num_workers=2,  # 减少worker数量以适应Kaggle环境\n    pin_memory=True\n)\n\nprint(f\"📊 测试集大小: {len(test_dataset)} 个样本\")\nprint(f\"🔧 批次大小: {optimized_batch_size}\")\nprint(f\"🔧 批次数量: {len(test_loader)}\")\n\n# 加载最佳模型\nprint(\"\\n🤖 加载训练好的模型...\")\n\ntry:\n    checkpoint = torch.load('./models/best_model.pth', \n                          map_location=device, \n                          weights_only=False)\nexcept TypeError:\n    checkpoint = torch.load('./models/best_model.pth', map_location=device)\n\n# 创建模型并加载权重\nmodel = OptimizedDeepfakeDetector(\n    backbone='resnet50',\n    hidden_dim=512,\n    num_layers=2,\n    dropout=0.3,\n    use_attention=True\n).to(device)\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"✅ 模型加载成功\")\nprint(f\"📈 最佳验证准确率: {checkpoint.get('best_val_acc', 'N/A')}\")\nprint(f\"🔄 训练轮数: {checkpoint.get('epoch', 'N/A')}\")\n\n# 记录评估开始时间\neval_start_time = datetime.now()\nprint(f\"\\n⏰ 评估开始时间: {eval_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# 执行优化的模型评估\nprint(\"\\n🚀 开始执行模型评估...\")\nprint(\"=\" * 60)\n\ncriterion = nn.BCELoss()\n\n# 执行评估\neval_results = evaluate_model_optimized(\n    model=model,\n    test_loader=test_loader,\n    criterion=criterion,\n    device=device\n)\n\n# 计算全面的评估指标\nmetrics = calculate_comprehensive_metrics(\n    predictions=eval_results['predictions'],\n    targets=eval_results['targets'],\n    scores=eval_results['scores']\n)\n\n# 性能分析\neval_end_time = datetime.now()\ntotal_eval_time = (eval_end_time - eval_start_time).total_seconds()\n\nprint(f\"\\n⏱️ 评估性能分析:\")\nprint(f\"总评估时间: {total_eval_time:.2f} 秒\")\nprint(f\"平均每样本推理时间: {eval_results['avg_inference_time']*1000:.2f} ms\")\nprint(f\"推理吞吐量: {len(test_dataset)/eval_results['total_inference_time']:.1f} 样本/秒\")\n\n# 打印详细评估结果\nprint(f\"\\n📊 详细评估结果:\")\nprint(f\"测试损失: {eval_results['loss']:.4f}\")\nprint(f\"准确率: {metrics['accuracy']:.4f}\")\nprint(f\"平衡准确率: {metrics['balanced_accuracy']:.4f}\")\nprint(f\"精确率: {metrics['precision']:.4f}\")\nprint(f\"召回率: {metrics['recall']:.4f}\")\nprint(f\"特异性: {metrics['specificity']:.4f}\")\nprint(f\"F1分数: {metrics['f1']:.4f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.4f}\")\nprint(f\"AUC-PR: {metrics['auc_pr']:.4f}\")\nprint(f\"负预测值: {metrics['npv']:.4f}\")\n\n# 创建结果目录\nos.makedirs('./results/evaluation', exist_ok=True)\n\n# 绘制增强的可视化图表\nprint(\"\\n📊 生成评估图表...\")\n\n# 1. 增强的混淆矩阵\nplot_enhanced_confusion_matrix(\n    cm=metrics['confusion_matrix'],\n    save_path='./results/evaluation/enhanced_confusion_matrix.png'\n)\n\n# 2. ROC和PR曲线\nplot_roc_pr_curves(\n    targets=eval_results['targets'],\n    scores=eval_results['scores'],\n    save_path='./results/evaluation/roc_pr_curves.png'\n)\n\n# 保存详细的评估报告\ndetailed_report = {\n    'evaluation_info': {\n        'timestamp': eval_start_time.isoformat(),\n        'model_path': './models/best_model.pth',\n        'test_dataset_size': len(test_dataset),\n        'batch_size': optimized_batch_size\n    },\n    'performance_metrics': {\n        'test_loss': float(eval_results['loss']),\n        'accuracy': float(metrics['accuracy']),\n        'balanced_accuracy': float(metrics['balanced_accuracy']),\n        'precision': float(metrics['precision']),\n        'recall': float(metrics['recall']),\n        'specificity': float(metrics['specificity']),\n        'f1_score': float(metrics['f1']),\n        'auc_roc': float(metrics['auc_roc']),\n        'auc_pr': float(metrics['auc_pr']),\n        'npv': float(metrics['npv'])\n    },\n    'confusion_matrix': {\n        'true_negative': int(metrics['tn']),\n        'false_positive': int(metrics['fp']),\n        'false_negative': int(metrics['fn']),\n        'true_positive': int(metrics['tp'])\n    },\n    'performance_analysis': {\n        'total_evaluation_time_seconds': total_eval_time,\n        'average_inference_time_ms': eval_results['avg_inference_time'] * 1000,\n        'throughput_samples_per_second': len(test_dataset) / eval_results['total_inference_time']\n    }\n}\n\n# 保存JSON格式的详细报告\nwith open('./results/evaluation/detailed_evaluation_report.json', 'w', encoding='utf-8') as f:\n    json.dump(detailed_report, f, indent=2, ensure_ascii=False)\n\n# 保存CSV格式的简要报告\nsummary_df = pd.DataFrame([{\n    '评估时间': eval_start_time.strftime('%Y-%m-%d %H:%M:%S'),\n    '测试损失': f\"{eval_results['loss']:.4f}\",\n    '准确率': f\"{metrics['accuracy']:.4f}\",\n    '平衡准确率': f\"{metrics['balanced_accuracy']:.4f}\",\n    '精确率': f\"{metrics['precision']:.4f}\",\n    '召回率': f\"{metrics['recall']:.4f}\",\n    'F1分数': f\"{metrics['f1']:.4f}\",\n    'AUC-ROC': f\"{metrics['auc_roc']:.4f}\",\n    'AUC-PR': f\"{metrics['auc_pr']:.4f}\",\n    '推理时间(ms)': f\"{eval_results['avg_inference_time']*1000:.2f}\",\n    '吞吐量(样本/秒)': f\"{len(test_dataset)/eval_results['total_inference_time']:.1f}\"\n}])\n\nsummary_df.to_csv('./results/evaluation/evaluation_summary.csv', index=False, encoding='utf-8')\n\nprint(\"\\n📁 评估结果已保存到:\")\nprint(\"  📊 ./results/evaluation/enhanced_confusion_matrix.png\")\nprint(\"  📈 ./results/evaluation/roc_pr_curves.png\")\nprint(\"  📋 ./results/evaluation/detailed_evaluation_report.json\")\nprint(\"  📊 ./results/evaluation/evaluation_summary.csv\")\n\nprint(\"\\n🎉 模型评估完成！\")\nprint(\"=\" * 60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T10:11:39.218894Z","iopub.execute_input":"2025-07-04T10:11:39.219627Z","iopub.status.idle":"2025-07-04T10:11:39.285324Z","shell.execute_reply.started":"2025-07-04T10:11:39.219600Z","shell.execute_reply":"2025-07-04T10:11:39.284452Z"}},"outputs":[],"execution_count":null}]}