# Cell 12: è®­ç»ƒå¾ªç¯ - é›†æˆä¼˜åŒ–ç‰ˆæœ¬

# ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
import os
import torch
from datetime import datetime
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

os.makedirs('./models', exist_ok=True)

# æ£€æŸ¥æ˜¯å¦å¯ç”¨é›†æˆå­¦ä¹ 
enable_ensemble = getattr(model, 'ensemble_mode', False)
if enable_ensemble:
    print("ğŸ¯ å¯ç”¨é›†æˆå­¦ä¹ æ¨¡å¼")
    # åˆ›å»ºå¤šä¸ªæ¨¡å‹ç”¨äºé›†æˆ
    from .cell_05_model_definition import create_ensemble_models
    ensemble_models = create_ensemble_models(num_models=3, device=device)
    print(f"ğŸ“Š åˆ›å»ºäº† {len(ensemble_models)} ä¸ªé›†æˆæ¨¡å‹")
else:
    ensemble_models = [model]

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
print(f"ğŸ“Š è®­ç»ƒé…ç½®: {len(train_loader)} ä¸ªè®­ç»ƒæ‰¹æ¬¡, {len(val_loader)} ä¸ªéªŒè¯æ‰¹æ¬¡")
print(f"ğŸ¯ æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
print(f"ğŸ’¾ è®¾å¤‡: {device}")
print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")

if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    torch.cuda.reset_peak_memory_stats()

# è®­ç»ƒå†å²è®°å½• - æ‰©å±•ç‰ˆæœ¬
train_history = {
    'train_loss': [],
    'val_loss': [],
    'train_acc': [],
    'val_acc': [],
    'train_auc': [],
    'val_auc': [],
    'train_precision': [],
    'val_precision': [],
    'train_recall': [],
    'val_recall': [],
    'train_f1': [],
    'val_f1': []
}

# é›†æˆå­¦ä¹ å†å²è®°å½•
if enable_ensemble:
    ensemble_history = {
        'ensemble_val_acc': [],
        'ensemble_val_auc': [],
        'ensemble_val_f1': [],
        'individual_performances': []
    }

best_val_loss = float('inf')
best_val_acc = 0.0
best_val_auc = 0.0
best_ensemble_acc = 0.0
best_model_states = []

# è®­ç»ƒå¾ªç¯
print("\nğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯...")
for epoch in range(num_epochs):
    epoch_start_time = time.time()
    print(f"\nEpoch {epoch+1}/{num_epochs}")
    
    if enable_ensemble:
        # é›†æˆè®­ç»ƒ
        from .cell_07_training_functions import train_ensemble_models, ensemble_predict
        
        # è®­ç»ƒæ‰€æœ‰é›†æˆæ¨¡å‹
        ensemble_train_results = train_ensemble_models(
            ensemble_models, train_loader, criterion, 
            [optimizer] * len(ensemble_models), device, scaler
        )
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŒ‡æ ‡
        train_loss = np.mean([r['loss'] for r in ensemble_train_results])
        train_acc = np.mean([r['accuracy'] for r in ensemble_train_results])
        train_auc = np.mean([r['auc'] for r in ensemble_train_results])
        train_precision = np.mean([r['precision'] for r in ensemble_train_results])
        train_recall = np.mean([r['recall'] for r in ensemble_train_results])
        train_f1 = np.mean([r['f1'] for r in ensemble_train_results])
        
        # é›†æˆéªŒè¯
        val_results = []
        all_val_preds = []
        all_val_labels = []
        
        for model_idx, ens_model in enumerate(ensemble_models):
            val_loss_single, val_acc_single, val_auc_single, val_metrics = validate_epoch(
                ens_model, val_loader, criterion, device, scaler, return_detailed_metrics=True
            )
            val_results.append({
                'loss': val_loss_single,
                'accuracy': val_acc_single,
                'auc': val_auc_single,
                **val_metrics
            })
        
        # è®¡ç®—é›†æˆé¢„æµ‹ç»“æœ
        ensemble_val_acc, ensemble_val_auc, ensemble_val_f1 = ensemble_predict(
            ensemble_models, val_loader, device
        )
        
        # è®¡ç®—å¹³å‡éªŒè¯æŒ‡æ ‡
        val_loss = np.mean([r['loss'] for r in val_results])
        val_acc = np.mean([r['accuracy'] for r in val_results])
        val_auc = np.mean([r['auc'] for r in val_results])
        val_precision = np.mean([r['precision'] for r in val_results])
        val_recall = np.mean([r['recall'] for r in val_results])
        val_f1 = np.mean([r['f1'] for r in val_results])
        
        # è®°å½•é›†æˆå†å²
        ensemble_history['ensemble_val_acc'].append(ensemble_val_acc)
        ensemble_history['ensemble_val_auc'].append(ensemble_val_auc)
        ensemble_history['ensemble_val_f1'].append(ensemble_val_f1)
        ensemble_history['individual_performances'].append(val_results)
        
    else:
        # å•æ¨¡å‹è®­ç»ƒ
        train_loss, train_acc, train_auc, train_metrics = train_epoch(
            model, train_loader, criterion, optimizer, device, scaler, return_detailed_metrics=True
        )
        val_loss, val_acc, val_auc, val_metrics = validate_epoch(
            model, val_loader, criterion, device, scaler, return_detailed_metrics=True
        )
        
        train_precision = train_metrics['precision']
        train_recall = train_metrics['recall']
        train_f1 = train_metrics['f1']
        val_precision = val_metrics['precision']
        val_recall = val_metrics['recall']
        val_f1 = val_metrics['f1']
    
    # è®°å½•å†å²
    train_history['train_loss'].append(train_loss)
    train_history['train_acc'].append(train_acc)
    train_history['train_auc'].append(train_auc)
    train_history['train_precision'].append(train_precision)
    train_history['train_recall'].append(train_recall)
    train_history['train_f1'].append(train_f1)
    train_history['val_loss'].append(val_loss)
    train_history['val_acc'].append(val_acc)
    train_history['val_auc'].append(val_auc)
    train_history['val_precision'].append(val_precision)
    train_history['val_recall'].append(val_recall)
    train_history['val_f1'].append(val_f1)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']
    
    # è®¡ç®—epochæ—¶é—´
    epoch_time = time.time() - epoch_start_time
    
    # æ‰“å°ç»“æœ
    print(f"è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.2f}%, AUC={train_auc:.4f}, F1={train_f1:.4f}")
    print(f"éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.2f}%, AUC={val_auc:.4f}, F1={val_f1:.4f}")
    
    if enable_ensemble:
        print(f"é›†æˆ: Acc={ensemble_val_acc:.2f}%, AUC={ensemble_val_auc:.4f}, F1={ensemble_val_f1:.4f}")
    
    print(f"å­¦ä¹ ç‡: {current_lr:.2e}, ç”¨æ—¶: {epoch_time:.1f}s")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    current_metric = ensemble_val_acc if enable_ensemble else val_acc
    if current_metric > best_val_acc:
        best_val_loss = val_loss
        best_val_acc = val_acc
        best_val_auc = val_auc
        
        if enable_ensemble:
            best_ensemble_acc = ensemble_val_acc
            best_model_states = [model.state_dict().copy() for model in ensemble_models]
            print(f"ğŸ¯ æ–°çš„æœ€ä½³é›†æˆæ¨¡å‹! é›†æˆAcc: {best_ensemble_acc:.2f}%, å¹³å‡AUC: {best_val_auc:.4f}")
        else:
            best_model_states = [model.state_dict().copy()]
            print(f"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! Acc: {best_val_acc:.2f}%, AUC: {best_val_auc:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°æ–‡ä»¶
        save_dict = {
            'epoch': epoch,
            'best_val_loss': best_val_loss,
            'best_val_acc': best_val_acc,
            'best_val_auc': best_val_auc,
            'train_history': train_history,
            'enable_ensemble': enable_ensemble,
            'num_models': len(best_model_states)
        }
        
        if enable_ensemble:
            save_dict['best_ensemble_acc'] = best_ensemble_acc
            save_dict['ensemble_history'] = ensemble_history
            for i, state in enumerate(best_model_states):
                save_dict[f'model_{i}_state_dict'] = state
        else:
            save_dict['model_state_dict'] = best_model_states[0]
            save_dict['optimizer_state_dict'] = optimizer.state_dict()
        
        torch.save(save_dict, './models/best_model.pth')
        print("ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜")
    
    # æ—©åœæ£€æŸ¥
    if early_stopping(val_loss, model):
        print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
        break
    
    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("\nâœ… è®­ç»ƒå®Œæˆ!")
if enable_ensemble:
    print(f"ğŸ† æœ€ç»ˆæœ€ä½³æ€§èƒ½: é›†æˆAcc={best_ensemble_acc:.2f}%, å¹³å‡Loss={best_val_loss:.4f}, å¹³å‡AUC={best_val_auc:.4f}")
else:
    print(f"ğŸ† æœ€ç»ˆæœ€ä½³æ€§èƒ½: Loss={best_val_loss:.4f}, Acc={best_val_acc:.2f}%, AUC={best_val_auc:.4f}")

if torch.cuda.is_available():
    print(f"ğŸ’¾ å³°å€¼GPUå†…å­˜ä½¿ç”¨: {torch.cuda.max_memory_allocated() / 1024**3:.1f}GB")

# ç»˜åˆ¶è®­ç»ƒå†å²
def plot_training_history():
    """ç»˜åˆ¶è®­ç»ƒå†å²å›¾è¡¨"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('è®­ç»ƒå†å² - é›†æˆä¼˜åŒ–ç‰ˆæœ¬', fontsize=16, fontweight='bold')
    
    # Loss
    axes[0, 0].plot(train_history['train_loss'], label='è®­ç»ƒLoss', color='blue')
    axes[0, 0].plot(train_history['val_loss'], label='éªŒè¯Loss', color='red')
    axes[0, 0].set_title('Losså˜åŒ–')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    # Accuracy
    axes[0, 1].plot(train_history['train_acc'], label='è®­ç»ƒAcc', color='blue')
    axes[0, 1].plot(train_history['val_acc'], label='éªŒè¯Acc', color='red')
    if enable_ensemble:
        axes[0, 1].plot(ensemble_history['ensemble_val_acc'], label='é›†æˆAcc', color='green', linewidth=2)
    axes[0, 1].set_title('å‡†ç¡®ç‡å˜åŒ–')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy (%)')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # AUC
    axes[0, 2].plot(train_history['train_auc'], label='è®­ç»ƒAUC', color='blue')
    axes[0, 2].plot(train_history['val_auc'], label='éªŒè¯AUC', color='red')
    if enable_ensemble:
        axes[0, 2].plot(ensemble_history['ensemble_val_auc'], label='é›†æˆAUC', color='green', linewidth=2)
    axes[0, 2].set_title('AUCå˜åŒ–')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('AUC')
    axes[0, 2].legend()
    axes[0, 2].grid(True)
    
    # Precision
    axes[1, 0].plot(train_history['train_precision'], label='è®­ç»ƒPrecision', color='blue')
    axes[1, 0].plot(train_history['val_precision'], label='éªŒè¯Precision', color='red')
    axes[1, 0].set_title('ç²¾ç¡®ç‡å˜åŒ–')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # Recall
    axes[1, 1].plot(train_history['train_recall'], label='è®­ç»ƒRecall', color='blue')
    axes[1, 1].plot(train_history['val_recall'], label='éªŒè¯Recall', color='red')
    axes[1, 1].set_title('å¬å›ç‡å˜åŒ–')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Recall')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    # F1 Score
    axes[1, 2].plot(train_history['train_f1'], label='è®­ç»ƒF1', color='blue')
    axes[1, 2].plot(train_history['val_f1'], label='éªŒè¯F1', color='red')
    if enable_ensemble:
        axes[1, 2].plot(ensemble_history['ensemble_val_f1'], label='é›†æˆF1', color='green', linewidth=2)
    axes[1, 2].set_title('F1åˆ†æ•°å˜åŒ–')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('F1 Score')
    axes[1, 2].legend()
    axes[1, 2].grid(True)
    
    plt.tight_layout()
    plt.savefig('./models/training_history.png', dpi=300, bbox_inches='tight')
    plt.show()

# ç»˜åˆ¶è®­ç»ƒå†å²
plot_training_history()

print("ğŸ“Š è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜åˆ° ./models/training_history.png")