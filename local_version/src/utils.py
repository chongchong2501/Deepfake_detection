# å·¥å…·å‡½æ•°å’ŒæŸå¤±å‡½æ•° - æœ¬åœ°RTX4070ä¼˜åŒ–ç‰ˆæœ¬

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
import time
import psutil
from config import config

class FocalLoss(nn.Module):
    """ç„¦ç‚¹æŸå¤±å‡½æ•° - è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜"""
    
    def __init__(self, alpha=None, gamma=None, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha or config.FOCAL_ALPHA
        self.gamma = gamma or config.FOCAL_GAMMA
        self.reduction = reduction

    def forward(self, inputs, targets):
        # ä½¿ç”¨ BCEWithLogitsLoss ä»¥å…¼å®¹æ··åˆç²¾åº¦è®­ç»ƒ
        ce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)
        # è®¡ç®—æ¦‚ç‡ç”¨äºfocal weight
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience=None, min_delta=None, restore_best_weights=True):
        self.patience = patience or config.EARLY_STOPPING_PATIENCE
        self.min_delta = min_delta or config.EARLY_STOPPING_MIN_DELTA
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        self.best_epoch = 0

    def __call__(self, val_loss, model, epoch=None):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.best_epoch = epoch or 0
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.best_epoch = epoch or 0
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1

        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
                print(f"æ—©åœè§¦å‘ï¼Œæ¢å¤ç¬¬{self.best_epoch}è½®çš„æœ€ä½³æƒé‡")
            return True
        return False

    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()
    
    def get_best_loss(self):
        return self.best_loss

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    def __init__(self):
        # å°è¯•é›†æˆæ–°çš„å†…å­˜ç®¡ç†å™¨
        try:
            from memory_manager import get_memory_manager
            self.memory_manager = get_memory_manager()
            self.enhanced_monitoring = True
        except ImportError:
            self.memory_manager = None
            self.enhanced_monitoring = False
        self.reset()
    
    def reset(self):
        self.start_time = time.time()
        self.gpu_memory_peak = 0
        self.cpu_usage_history = []
        self.gpu_usage_history = []
    
    def update(self):
        if self.enhanced_monitoring and self.memory_manager:
            # ä½¿ç”¨å¢å¼ºçš„å†…å­˜ç®¡ç†å™¨
            stats = self.memory_manager.get_memory_stats()
            self.cpu_usage_history.append(stats.cpu_percent)
            gpu_memory = stats.gpu_memory_allocated_gb
            self.gpu_memory_peak = max(self.gpu_memory_peak, gpu_memory)
        else:
            # å›é€€åˆ°åŸå§‹å®ç°
            cpu_percent = psutil.cpu_percent()
            self.cpu_usage_history.append(cpu_percent)
            
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                self.gpu_memory_peak = max(self.gpu_memory_peak, gpu_memory)
    
    def get_stats(self):
        elapsed_time = time.time() - self.start_time
        avg_cpu = np.mean(self.cpu_usage_history) if self.cpu_usage_history else 0
        
        stats = {
            'elapsed_time': elapsed_time,
            'avg_cpu_usage': avg_cpu,
            'peak_gpu_memory_gb': self.gpu_memory_peak
        }
        
        # å¦‚æœæœ‰å¢å¼ºç›‘æ§ï¼Œæ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
        if self.enhanced_monitoring and self.memory_manager:
            current_stats = self.memory_manager.get_memory_stats()
            stats.update({
                'current_gpu_memory_gb': current_stats.gpu_memory_allocated_gb,
                'current_cpu_memory_gb': current_stats.cpu_memory_gb,
                'gpu_memory_percent': current_stats.gpu_memory_percent,
                'cpu_memory_percent': current_stats.cpu_memory_percent
            })
        elif torch.cuda.is_available():
            stats['current_gpu_memory_gb'] = torch.cuda.memory_allocated() / 1024**3
            stats['max_gpu_memory_gb'] = torch.cuda.max_memory_allocated() / 1024**3
        
        return stats
    
    def get_memory_suggestions(self):
        """è·å–å†…å­˜ä¼˜åŒ–å»ºè®®"""
        if self.enhanced_monitoring and self.memory_manager:
            return self.memory_manager.get_optimization_suggestions()
        else:
            # åŸºç¡€å»ºè®®
            suggestions = []
            if self.gpu_memory_peak > 10:  # å‡è®¾12GBæ˜¾å¡
                suggestions.append("ğŸŸ¡ GPUå†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®å‡å°batch_size")
            if len(self.cpu_usage_history) > 0 and max(self.cpu_usage_history) > 90:
                suggestions.append("ğŸŸ¡ CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å‡å°‘num_workers")
            if not suggestions:
                suggestions.append("âœ… ç³»ç»Ÿèµ„æºä½¿ç”¨æ­£å¸¸")
            return suggestions

def get_transforms(mode='train', image_size=None):
    """è·å–æ•°æ®å˜æ¢"""
    if image_size is None:
        image_size = config.FRAME_SIZE[0]  # å‡è®¾æ˜¯æ­£æ–¹å½¢
    
    if mode == 'train':
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((int(image_size * 1.1), int(image_size * 1.1))),
            transforms.RandomCrop((image_size, image_size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.RandomRotation(degrees=10),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))
        ])
    else:
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

def calculate_accuracy(outputs, targets, threshold=0.5):
    """è®¡ç®—å‡†ç¡®ç‡"""
    predictions = torch.sigmoid(outputs) > threshold
    targets_bool = targets > threshold
    correct = (predictions == targets_bool).float()
    return correct.mean().item() * 100

def calculate_auc(outputs, targets):
    """è®¡ç®—AUCåˆ†æ•°"""
    try:
        from sklearn.metrics import roc_auc_score
        probs = torch.sigmoid(outputs).detach().cpu().numpy()
        targets_np = targets.detach().cpu().numpy()
        return roc_auc_score(targets_np, probs)
    except Exception:
        return 0.0

def save_model_checkpoint(model, optimizer, scheduler, epoch, loss, accuracy, auc, filepath):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'loss': loss,
        'accuracy': accuracy,
        'auc': auc,
        'config': {
            'backbone': config.BACKBONE,
            'hidden_dim': config.HIDDEN_DIM,
            'num_layers': config.NUM_LSTM_LAYERS,
            'dropout': config.DROPOUT,
            'use_attention': config.USE_ATTENTION,
            'max_frames': config.MAX_FRAMES,
            'frame_size': config.FRAME_SIZE
        }
    }
    
    torch.save(checkpoint, filepath)
    print(f"æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {filepath}")

def load_model_checkpoint(filepath, model, optimizer=None, scheduler=None):
    """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(filepath, map_location=config.get_device())
    
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    
    epoch = checkpoint.get('epoch', 0)
    loss = checkpoint.get('loss', float('inf'))
    accuracy = checkpoint.get('accuracy', 0.0)
    auc = checkpoint.get('auc', 0.0)
    
    print(f"æ¨¡å‹æ£€æŸ¥ç‚¹å·²åŠ è½½: epoch={epoch}, loss={loss:.4f}, acc={accuracy:.2f}%, auc={auc:.4f}")
    
    return epoch, loss, accuracy, auc

def print_gpu_memory_info():
    """æ‰“å°GPUå†…å­˜ä¿¡æ¯ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
    try:
        from memory_manager import print_memory_info
        print_memory_info()
    except ImportError:
        # å›é€€åˆ°åŸå§‹å®ç°
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            max_allocated = torch.cuda.max_memory_allocated() / 1024**3
            
            print(f"GPUå†…å­˜ - å·²åˆ†é…: {allocated:.2f}GB, å·²ç¼“å­˜: {cached:.2f}GB, å³°å€¼: {max_allocated:.2f}GB")
        else:
            print("CUDAä¸å¯ç”¨")

def cleanup_gpu_memory():
    """æ¸…ç†GPUå†…å­˜ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
    try:
        from memory_manager import cleanup_memory
        return cleanup_memory()
    except ImportError:
        # å›é€€åˆ°åŸå§‹å®ç°
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        return 0.0

def set_random_seed(seed=None):
    """è®¾ç½®éšæœºç§å­"""
    if seed is None:
        seed = config.RANDOM_SEED
    
    import random
    import numpy as np
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    print(f"éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")

def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = seconds // 60
        seconds = seconds % 60
        return f"{int(minutes)}åˆ†{seconds:.1f}ç§’"
    else:
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        seconds = seconds % 60
        return f"{int(hours)}å°æ—¶{int(minutes)}åˆ†{seconds:.1f}ç§’"

def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°æ•°é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return {
        'total': total_params,
        'trainable': trainable_params,
        'non_trainable': total_params - trainable_params
    }