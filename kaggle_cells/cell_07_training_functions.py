# Cell 7: è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# æ‰€æœ‰importè¯­å¥å·²ç§»è‡³cell_01_imports_and_setup.py

def train_epoch(model, train_loader, criterion, optimizer, device, scaler=None):
    """GPUä¼˜åŒ–çš„è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []
    
    # é«˜çº§æ€§èƒ½ç›‘æ§
    data_load_times = []
    compute_times = []
    gpu_memory_peak = 0
    
    # å¯ç”¨GPUæµæ°´çº¿ä¼˜åŒ–
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True

    pbar = tqdm(train_loader, desc='Training', leave=False)
    
    print(f"â³ å¼€å§‹åŠ è½½ç¬¬ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡...")
    for batch_idx, (data, target) in enumerate(pbar):
        data_start_time = time.time()
        
        if batch_idx == 0:
            print(f"âœ… ç¬¬ä¸€ä¸ªæ‰¹æ¬¡åŠ è½½å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {data.shape}")
            print(f"ğŸ“Š æ•°æ®ç±»å‹: {data.dtype}, è®¾å¤‡: {data.device}")
        
        # é«˜æ•ˆæ•°æ®ä¼ è¾“ï¼ˆæ£€æŸ¥æ•°æ®æ˜¯å¦å·²åœ¨GPUä¸Šï¼‰
        if not data.is_cuda:
            data = data.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
        
        # ç¡®ä¿æ•°æ®ä¼ è¾“å®Œæˆ
        if torch.cuda.is_available():
            torch.cuda.synchronize()
        
        data_load_time = time.time() - data_start_time
        data_load_times.append(data_load_time)
        
        compute_start_time = time.time()
        
        # æ›´é«˜æ•ˆçš„æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad(set_to_none=True)

        if scaler is not None:
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with autocast():
                output, _ = model(data)
                loss = criterion(output, target)
            
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            output, _ = model(data)
            loss = criterion(output, target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

        total_loss += loss.item()
        
        # åœ¨GPUä¸Šè®¡ç®—å‡†ç¡®ç‡
        with torch.no_grad():
            probs = torch.sigmoid(output)
            predicted = (probs > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()

            # æ‰¹é‡æ”¶é›†é¢„æµ‹ç»“æœ
            all_preds.extend(probs.detach().cpu().numpy())
            all_targets.extend(target.detach().cpu().numpy())

        compute_time = time.time() - compute_start_time
        compute_times.append(compute_time)
        
        # GPUå†…å­˜ç›‘æ§
        if torch.cuda.is_available():
            current_memory = torch.cuda.memory_allocated() / 1024**3
            gpu_memory_peak = max(gpu_memory_peak, current_memory)
        
        # æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½ä¿¡æ¯
        if batch_idx % 20 == 0 and batch_idx > 0:
            avg_data_time = np.mean(data_load_times[-20:])
            avg_compute_time = np.mean(compute_times[-20:])
            gpu_util = avg_compute_time / (avg_data_time + avg_compute_time) * 100
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%',
                'GPU%': f'{gpu_util:.1f}',
                'Mem': f'{current_memory:.1f}GB',
                'DataT': f'{avg_data_time*1000:.1f}ms'
            })
        else:
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        # æ™ºèƒ½GPUç¼“å­˜ç®¡ç†
        if batch_idx % 30 == 0 and torch.cuda.is_available():
            if torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() > 0.9:
                torch.cuda.empty_cache()

    avg_loss = total_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    # æ€§èƒ½ç»Ÿè®¡
    avg_data_load_time = np.mean(data_load_times)
    avg_compute_time = np.mean(compute_times)
    gpu_utilization = avg_compute_time / (avg_data_load_time + avg_compute_time) * 100
    
    print(f"\nğŸš€ è®­ç»ƒæ€§èƒ½æ·±åº¦åˆ†æ:")
    print(f"å¹³å‡æ•°æ®åŠ è½½æ—¶é—´: {avg_data_load_time*1000:.2f}ms")
    print(f"å¹³å‡è®¡ç®—æ—¶é—´: {avg_compute_time*1000:.2f}ms")
    print(f"æ•°æ®åŠ è½½/è®¡ç®—æ¯”ç‡: {avg_data_load_time/avg_compute_time:.3f} (ç›®æ ‡: <0.1)")
    print(f"GPUå†…å­˜å³°å€¼ä½¿ç”¨: {gpu_memory_peak:.1f}GB")
    print(f"GPUåˆ©ç”¨ç‡: {gpu_utilization:.1f}%")
    print(f"GPUåˆ©ç”¨ç‡ä¼˜åŒ–: {'âœ… ä¼˜ç§€' if avg_data_load_time/avg_compute_time < 0.1 else 'âš ï¸ éœ€ä¼˜åŒ–'}")

    try:
        auc_score = roc_auc_score(all_targets, all_preds)
    except:
        auc_score = 0.0

    return avg_loss, accuracy, auc_score

def validate_epoch(model, val_loader, criterion, device, scaler=None):
    """GPUä¼˜åŒ–çš„éªŒè¯ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        pbar = tqdm(val_loader, desc='Validation', leave=False)

        for batch_idx, (data, target) in enumerate(pbar):
            # éé˜»å¡æ•°æ®ä¼ è¾“åˆ°GPU
            data = data.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            
            # æ··åˆç²¾åº¦æ¨ç†
            with autocast():
                output, _ = model(data)
                loss = criterion(output, target)

            total_loss += loss.item()
            
            # åœ¨GPUä¸Šè®¡ç®—å‡†ç¡®ç‡
            probs = torch.sigmoid(output)
            predicted = (probs > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()

            # æ‰¹é‡æ”¶é›†é¢„æµ‹ç»“æœ
            all_preds.extend(probs.cpu().numpy())
            all_targets.extend(target.cpu().numpy())

            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
            
            # å®šæœŸæ¸…ç†GPUç¼“å­˜
            if batch_idx % 20 == 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()

    avg_loss = total_loss / len(val_loader)
    accuracy = 100. * correct / total

    try:
        auc_score = roc_auc_score(all_targets, all_preds)
    except:
        auc_score = 0.0

    return avg_loss, accuracy, auc_score

print("âœ… è®­ç»ƒå’ŒéªŒè¯å‡½æ•°å®šä¹‰å®Œæˆ")