# æ•°æ®é›†ç±» - æœ¬åœ°RTX4070ä¼˜åŒ–ç‰ˆæœ¬

import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset
from config import config
from video_processor import get_video_processor, process_video_safe, read_video_safe
from memory_manager import auto_memory_management, cleanup_memory

class DeepfakeVideoDataset(Dataset):
    """æ·±åº¦ä¼ªé€ è§†é¢‘æ•°æ®é›†ç±» - RTX4070ä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(self, csv_file=None, data_list=None, transform=None, max_frames=None, 
                 gpu_preprocessing=True, cache_frames=True):
        if csv_file is not None:
            self.df = pd.read_csv(csv_file)
            self.data_list = None
        elif data_list is not None:
            self.data_list = data_list
            self.df = None
        else:
            raise ValueError("å¿…é¡»æä¾›csv_fileæˆ–data_list")
            
        self.transform = transform
        self.max_frames = max_frames or config.MAX_FRAMES
        self.gpu_preprocessing = gpu_preprocessing and torch.cuda.is_available()
        self.cache_frames = cache_frames
        self.device = config.get_device()
        
        # RTX4070ä¼˜åŒ–ï¼šæ›´å¤§çš„ç¼“å­˜ç³»ç»Ÿ
        self.frame_cache = {} if cache_frames else None
        self.cache_hits = 0
        self.cache_misses = 0
        
        # è§†é¢‘å¤„ç†å™¨
        self.video_processor = get_video_processor()
        
        # GPUé¢„å¤„ç†çš„æ ‡å‡†åŒ–å‚æ•°
        if self.gpu_preprocessing:
            self.mean = torch.tensor([0.485, 0.456, 0.406], device=self.device, dtype=torch.float32)
            self.std = torch.tensor([0.229, 0.224, 0.225], device=self.device, dtype=torch.float32)
    
    def __len__(self):
        if self.df is not None:
            return len(self.df)
        return len(self.data_list)
    
    def __getitem__(self, idx):
        if self.data_list is not None:
            # ç›´æ¥ä»å†…å­˜ä¸­çš„æ•°æ®åˆ—è¡¨è·å–
            item = self.data_list[idx]
            frames = item['frames']
            label = item['label']
            video_path = item.get('video_path', None)
        else:
            # ä»CSVæ–‡ä»¶è·å–è·¯å¾„
            row = self.df.iloc[idx]
            video_path = row['video_path']
            label = row['label']
            frames = None
        
        # è·å–è§†é¢‘å¸§
        if frames is None:
            # æ£€æŸ¥ç¼“å­˜
            if self.cache_frames and video_path in self.frame_cache:
                frames = self.frame_cache[video_path]
                self.cache_hits += 1
            else:
                # ä½¿ç”¨æ–°çš„å®‰å…¨è§†é¢‘è¯»å–
                raw_frames = read_video_safe(video_path, max_frames=self.max_frames)
                if not raw_frames:
                    # å¦‚æœè¯»å–å¤±è´¥ï¼Œè¿”å›é»˜è®¤å¸§
                    frames = []
                else:
                    frames = process_video_safe(raw_frames)
                
                self.cache_misses += 1
                # ç¼“å­˜å¸§æ•°æ®
                if self.cache_frames and len(frames) > 0 and len(self.frame_cache) < 1000:  # é™åˆ¶ç¼“å­˜å¤§å°
                    self.frame_cache[video_path] = frames
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§
        frames = self._ensure_frame_count(frames)
        
        # æ•°æ®é¢„å¤„ç†
        if self.transform:
            frames = [self.transform(frame) for frame in frames]
            video_tensor = torch.stack(frames)
        elif self.gpu_preprocessing:
            video_tensor = self._gpu_preprocess(frames)
        else:
            video_tensor = self._cpu_preprocess(frames)
        
        # æ ‡ç­¾å¤„ç†
        label_tensor = torch.tensor(label, dtype=torch.float32)
        if self.gpu_preprocessing:
            label_tensor = label_tensor.to(self.device, non_blocking=True)
        
        return video_tensor, label_tensor
    
    def _ensure_frame_count(self, frames):
        """ç¡®ä¿å¸§æ•°ç¬¦åˆè¦æ±‚"""
        if len(frames) == 0:
            frames = [np.zeros((*config.FRAME_SIZE, 3), dtype=np.uint8) for _ in range(self.max_frames)]
        
        # å¡«å……æˆ–æˆªæ–­åˆ°æŒ‡å®šå¸§æ•°
        while len(frames) < self.max_frames:
            frames.append(frames[-1].copy() if frames else np.zeros((*config.FRAME_SIZE, 3), dtype=np.uint8))
        
        return frames[:self.max_frames]
    
    @auto_memory_management(cleanup_interval=100)
    def _gpu_preprocess(self, frames):
        """GPUé¢„å¤„ç†ï¼ˆå¸¦å†…å­˜ç®¡ç†ï¼‰"""
        try:
            # ç¡®ä¿framesæ˜¯numpyæ•°ç»„
            if isinstance(frames[0], torch.Tensor):
                # å¦‚æœæ˜¯tensorï¼Œå…ˆç§»åŠ¨åˆ°CPUå†è½¬æ¢ä¸ºnumpy
                frames = [frame.cpu().numpy() if frame.is_cuda else frame.numpy() for frame in frames]
            
            # è½¬æ¢ä¸ºtensor
            frames_array = np.stack(frames)  # (T, H, W, C)
            video_tensor = torch.from_numpy(frames_array).permute(0, 3, 1, 2).float()  # (T, C, H, W)
            
            # ç§»åŠ¨åˆ°GPUå¹¶è¿›è¡Œé¢„å¤„ç†
            video_tensor = video_tensor.to(self.device, non_blocking=True, dtype=torch.float32) / 255.0
            
            # æ ‡å‡†åŒ–
            video_tensor = (video_tensor - self.mean.view(1, 3, 1, 1)) / self.std.view(1, 3, 1, 1)
            
            return video_tensor
            
        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"ğŸš¨ GPUå†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜åé‡è¯•...")
                cleanup_memory(force=True)
                self.clear_cache()
                # é‡è¯•ä¸€æ¬¡
                try:
                    frames_array = np.stack(frames)
                    video_tensor = torch.from_numpy(frames_array).permute(0, 3, 1, 2).float()
                    video_tensor = video_tensor.to(self.device, non_blocking=True, dtype=torch.float32) / 255.0
                    video_tensor = (video_tensor - self.mean.view(1, 3, 1, 1)) / self.std.view(1, 3, 1, 1)
                    return video_tensor
                except:
                    print(f"GPUé¢„å¤„ç†é‡è¯•å¤±è´¥ï¼Œå›é€€åˆ°CPU")
                    return self._cpu_preprocess(frames)
            else:
                print(f"GPUé¢„å¤„ç†å¤±è´¥: {e}ï¼Œå›é€€åˆ°CPU")
                return self._cpu_preprocess(frames)
    
    def _cpu_preprocess(self, frames):
        """CPUé¢„å¤„ç†"""
        import cv2
        
        # ç¡®ä¿framesæ˜¯numpyæ•°ç»„
        if isinstance(frames[0], torch.Tensor):
            frames = [frame.cpu().numpy() if frame.is_cuda else frame.numpy() for frame in frames]
        
        # è°ƒæ•´æ‰€æœ‰å¸§åˆ°ç›¸åŒå°ºå¯¸ (224, 224)
        processed_frames = []
        for frame in frames:
            # ç¡®ä¿frameæ˜¯æ­£ç¡®çš„å½¢çŠ¶ (H, W, C)
            if frame.shape[-1] != 3:
                frame = frame.transpose(1, 2, 0)  # ä» (C, H, W) è½¬æ¢ä¸º (H, W, C)
            
            # è°ƒæ•´å°ºå¯¸åˆ°224x224
            frame_resized = cv2.resize(frame, (224, 224))
            processed_frames.append(frame_resized)
        
        # è½¬æ¢ä¸ºtensor
        frames = [torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0 for frame in processed_frames]
        video_tensor = torch.stack(frames)
        
        # æ ‡å‡†åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)
        video_tensor = (video_tensor - mean) / std
        
        return video_tensor
    
    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.frame_cache) if self.frame_cache else 0
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.frame_cache:
            cache_size_before = len(self.frame_cache)
            self.frame_cache.clear()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # æ¸…ç†GPUå†…å­˜
            cleanup_memory()
            
            print(f"ğŸ§¹ ç¼“å­˜å·²æ¸…ç©º: é‡Šæ”¾äº†{cache_size_before}ä¸ªç¼“å­˜é¡¹")

def create_data_loaders(train_data, val_data, test_data, batch_size=None, quick_mode=False):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå¸¦å†…å­˜ç®¡ç†ï¼‰"""
    if batch_size is None:
        batch_size = config.BATCH_SIZE
    
    # å†…å­˜ä¼˜åŒ–ï¼šæ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´å‚æ•°
    try:
        from memory_manager import get_memory_manager
        memory_manager = get_memory_manager()
        stats = memory_manager.get_memory_stats()
        
        # æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè‡ªåŠ¨è°ƒæ•´å‚æ•°
        if stats.gpu_memory_percent > 0.7:
            print("âš ï¸ GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œè‡ªåŠ¨ä¼˜åŒ–å‚æ•°")
            if batch_size > 4:
                batch_size = max(4, batch_size // 2)
                print(f"   batch_sizeè°ƒæ•´ä¸º: {batch_size}")
            quick_mode = True
        
        if stats.cpu_memory_percent > 0.8:
            print("âš ï¸ CPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œè‡ªåŠ¨ä¼˜åŒ–å‚æ•°")
            quick_mode = True
    except ImportError:
        print("å†…å­˜ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
    
    # åœ¨å¿«é€Ÿæ¨¡å¼ä¸‹ç¦ç”¨GPUé¢„å¤„ç†å’Œç¼“å­˜ä»¥èŠ‚çœå†…å­˜
    gpu_preprocessing = not quick_mode
    cache_frames = not quick_mode
    
    # åˆ›å»ºæ•°æ®é›† - æ”¯æŒDataFrameæˆ–æ•°æ®åˆ—è¡¨
    if isinstance(train_data, list):
        train_dataset = DeepfakeVideoDataset(
            csv_file=None,
            data_list=train_data,
            gpu_preprocessing=gpu_preprocessing,
            cache_frames=cache_frames
        )
    else:
        # å¯¹äºDataFrameï¼Œåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ç©ºåˆ—è¡¨æ¥ç»•è¿‡åˆå§‹åŒ–æ£€æŸ¥
        train_dataset = DeepfakeVideoDataset(
            csv_file=None,
            data_list=[],  # ä¸´æ—¶ç©ºåˆ—è¡¨
            gpu_preprocessing=gpu_preprocessing,
            cache_frames=cache_frames
        )
        train_dataset.df = train_data
        train_dataset.data_list = None  # é‡ç½®ä¸ºNone
    
    if isinstance(val_data, list):
        val_dataset = DeepfakeVideoDataset(
            csv_file=None,
            data_list=val_data,
            gpu_preprocessing=gpu_preprocessing,
            cache_frames=cache_frames
        )
    else:
        val_dataset = DeepfakeVideoDataset(
            csv_file=None,
            data_list=[],  # ä¸´æ—¶ç©ºåˆ—è¡¨
            gpu_preprocessing=gpu_preprocessing,
            cache_frames=cache_frames
        )
        val_dataset.df = val_data
        val_dataset.data_list = None  # é‡ç½®ä¸ºNone
    
    if isinstance(test_data, list):
        test_dataset = DeepfakeVideoDataset(
            csv_file=None,
            data_list=test_data,
            gpu_preprocessing=gpu_preprocessing,
            cache_frames=cache_frames
        )
    else:
        test_dataset = DeepfakeVideoDataset(
            csv_file=None,
            data_list=[],  # ä¸´æ—¶ç©ºåˆ—è¡¨
            gpu_preprocessing=gpu_preprocessing,
            cache_frames=cache_frames
        )
        test_dataset.df = test_data
        test_dataset.data_list = None  # é‡ç½®ä¸ºNone
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    from torch.utils.data import DataLoader
    
    # åœ¨å¿«é€Ÿæ¨¡å¼ä¸‹ä½¿ç”¨æ›´ä¿å®ˆçš„è®¾ç½®ä»¥èŠ‚çœå†…å­˜
    num_workers = 0 if quick_mode else config.NUM_WORKERS
    pin_memory = False if quick_mode else config.PIN_MEMORY
    prefetch_factor = None if quick_mode else config.PREFETCH_FACTOR  # num_workers=0æ—¶å¿…é¡»ä¸ºNone
    persistent_workers = False if (quick_mode or num_workers == 0) else config.PERSISTENT_WORKERS
    
    # æ„å»ºDataLoaderå‚æ•°
    loader_kwargs = {
        'batch_size': batch_size,
        'num_workers': num_workers,
        'pin_memory': pin_memory,
        'persistent_workers': persistent_workers
    }
    
    # åªæœ‰åœ¨å¤šè¿›ç¨‹æ¨¡å¼ä¸‹æ‰è®¾ç½®prefetch_factor
    if num_workers > 0:
        loader_kwargs['prefetch_factor'] = prefetch_factor
    
    train_loader = DataLoader(
        train_dataset,
        shuffle=True,
        drop_last=True,
        **loader_kwargs
    )
    
    val_loader = DataLoader(
        val_dataset,
        shuffle=False,
        **loader_kwargs
    )
    
    test_loader = DataLoader(
        test_dataset,
        shuffle=False,
        **loader_kwargs
    )
    
    print(f"\nğŸ“Š æ•°æ®åŠ è½½å™¨ç»Ÿè®¡:")
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)} (æ‰¹æ¬¡å¤§å°: {batch_size})")
    print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    print(f"æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
    print(f"æ•°æ®åŠ è½½workeræ•°: {num_workers}")
    if quick_mode:
        print(f"âš¡ å¿«é€Ÿæ¨¡å¼ä¼˜åŒ–: GPUé¢„å¤„ç†={gpu_preprocessing}, ç¼“å­˜={cache_frames}, Pinå†…å­˜={pin_memory}")
    
    return train_loader, val_loader, test_loader