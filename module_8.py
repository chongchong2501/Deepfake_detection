#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ç¬¬8æ®µï¼šæ¨¡å‹è¯„ä¼°
# 
# Kaggle Deepfake Detection Module
# This module can be run as a single cell in Kaggle environment
# 
# Usage:
# 1. Create a new code cell in Kaggle
# 2. Copy the entire content of this file to the cell
# 3. Run the cell

# =============================================================================
# ç¬¬8æ®µï¼šæ¨¡å‹è¯„ä¼°
# =============================================================================

import time
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, auc, precision_recall_curve, average_precision_score
)
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle

# ä¼˜åŒ–çš„è¯„ä¼°å‡½æ•°
def evaluate_model_optimized(model, test_loader, criterion, device, save_attention=True):
    """
    ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼ŒåŒ…å«æ›´å…¨é¢çš„æŒ‡æ ‡å’Œåˆ†æ
    """
    model.eval()
    running_loss = 0.0
    all_predictions = []
    all_targets = []
    all_scores = []
    all_attention_weights = []
    inference_times = []
    
    with torch.no_grad():
        progress_bar = tqdm(test_loader, desc='æ¨¡å‹è¯„ä¼°ä¸­')
        
        for inputs, labels in progress_bar:
            inputs = inputs.to(device)
            labels = labels.float().to(device)
            
            # è®°å½•æ¨ç†æ—¶é—´
            start_time = time.time()
            
            # å‰å‘ä¼ æ’­
            if hasattr(model, 'forward') and len(inspect.signature(model.forward).parameters) > 1:
                outputs, attention_weights = model(inputs)
                if save_attention:
                    all_attention_weights.append(attention_weights.cpu().numpy())
            else:
                outputs = model(inputs)
                attention_weights = None
            
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            outputs = outputs.squeeze()
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(0)
            
            # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡
            preds = (outputs > 0.5).float().cpu().numpy()
            all_predictions.extend(preds)
            all_targets.extend(labels.cpu().numpy())
            all_scores.extend(outputs.cpu().numpy())
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'avg_time': f'{np.mean(inference_times):.3f}s'
            })
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œæ¨ç†æ—¶é—´
    test_loss = running_loss / len(test_loader.dataset)
    avg_inference_time = np.mean(inference_times)
    
    return {
        'loss': test_loss,
        'predictions': all_predictions,
        'targets': all_targets,
        'scores': all_scores,
        'attention_weights': all_attention_weights,
        'avg_inference_time': avg_inference_time,
        'total_inference_time': sum(inference_times)
    }

# è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡
def calculate_comprehensive_metrics(predictions, targets, scores):
    """
    è®¡ç®—æ›´å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡
    """
    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, zero_division=0)
    recall = recall_score(targets, predictions, zero_division=0)
    f1 = f1_score(targets, predictions, zero_division=0)
    
    # ROCå’ŒPRæ›²çº¿æŒ‡æ ‡
    if len(set(targets)) > 1:
        auc_roc = roc_auc_score(targets, scores)
        precision_vals, recall_vals, _ = precision_recall_curve(targets, scores)
        auc_pr = average_precision_score(targets, scores)
    else:
        auc_roc = 0.0
        auc_pr = 0.0
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(targets, predictions)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
    
    # é¢å¤–æŒ‡æ ‡
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # è´Ÿé¢„æµ‹å€¼
    balanced_accuracy = (recall + specificity) / 2
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("=" * 60)
    print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("=" * 60)
    print(f"ğŸ¯ å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
    print(f"ğŸ¯ å¹³è¡¡å‡†ç¡®ç‡ (Balanced Accuracy): {balanced_accuracy:.4f}")
    print(f"ğŸ” ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"ğŸ” å¬å›ç‡ (Recall/Sensitivity): {recall:.4f}")
    print(f"ğŸ” ç‰¹å¼‚æ€§ (Specificity): {specificity:.4f}")
    print(f"ğŸ” è´Ÿé¢„æµ‹å€¼ (NPV): {npv:.4f}")
    print(f"âš–ï¸ F1åˆ†æ•°: {f1:.4f}")
    print(f"ğŸ“ˆ AUC-ROC: {auc_roc:.4f}")
    print(f"ğŸ“ˆ AUC-PR: {auc_pr:.4f}")
    print("=" * 60)
    
    # æ··æ·†çŸ©é˜µè¯¦æƒ…
    print("\nğŸ“‹ æ··æ·†çŸ©é˜µè¯¦æƒ…:")
    print(f"çœŸé˜´æ€§ (TN): {tn}")
    print(f"å‡é˜³æ€§ (FP): {fp}")
    print(f"å‡é˜´æ€§ (FN): {fn}")
    print(f"çœŸé˜³æ€§ (TP): {tp}")
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“Š è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(targets, predictions, 
                             target_names=['çœŸå®è§†é¢‘', 'ä¼ªé€ è§†é¢‘'],
                             digits=4))
    
    return {
        'accuracy': accuracy,
        'balanced_accuracy': balanced_accuracy,
        'precision': precision,
        'recall': recall,
        'specificity': specificity,
        'npv': npv,
        'f1': f1,
        'auc_roc': auc_roc,
        'auc_pr': auc_pr,
        'confusion_matrix': cm,
        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp
    }

# å¢å¼ºçš„å¯è§†åŒ–å‡½æ•°
def plot_enhanced_confusion_matrix(cm, save_path=None):
    """
    ç»˜åˆ¶å¢å¼ºçš„æ··æ·†çŸ©é˜µ
    """
    plt.figure(figsize=(10, 8))
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ ‡æ³¨
    annotations = []
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            annotations.append(f'{cm[i,j]}\n({cm_percent[i,j]:.1f}%)')
    
    annotations = np.array(annotations).reshape(cm.shape)
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues', 
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'},
                xticklabels=['çœŸå®è§†é¢‘', 'ä¼ªé€ è§†é¢‘'], 
                yticklabels=['çœŸå®è§†é¢‘', 'ä¼ªé€ è§†é¢‘'])
    
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12, fontweight='bold')
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12, fontweight='bold')
    plt.title('æ··æ·†çŸ©é˜µ (æ•°é‡å’Œç™¾åˆ†æ¯”)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

# ç»˜åˆ¶ROCå’ŒPRæ›²çº¿
def plot_roc_pr_curves(targets, scores, save_path=None):
    """
    åŒæ—¶ç»˜åˆ¶ROCæ›²çº¿å’ŒPRæ›²çº¿
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(targets, scores)
    roc_auc = auc(fpr, tpr)
    
    ax1.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.8)
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('å‡é˜³æ€§ç‡ (FPR)', fontweight='bold')
    ax1.set_ylabel('çœŸé˜³æ€§ç‡ (TPR)', fontweight='bold')
    ax1.set_title('ROCæ›²çº¿', fontsize=14, fontweight='bold')
    ax1.legend(loc='lower right')
    ax1.grid(True, linestyle='--', alpha=0.7)
    
    # PRæ›²çº¿
    precision_vals, recall_vals, _ = precision_recall_curve(targets, scores)
    pr_auc = average_precision_score(targets, scores)
    
    ax2.plot(recall_vals, precision_vals, color='darkgreen', lw=2,
             label=f'PRæ›²çº¿ (AUC = {pr_auc:.4f})')
    ax2.axhline(y=np.mean(targets), color='navy', linestyle='--', alpha=0.8,
                label=f'åŸºçº¿ ({np.mean(targets):.3f})')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('å¬å›ç‡ (Recall)', fontweight='bold')
    ax2.set_ylabel('ç²¾ç¡®ç‡ (Precision)', fontweight='bold')
    ax2.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.legend(loc='lower left')
    ax2.grid(True, linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ROCå’ŒPRæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

# æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
def visualize_attention_weights(attention_weights, save_path=None, num_samples=5):
    """
    å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡
    """
    if not attention_weights:
        print("âš ï¸ æ²¡æœ‰æ³¨æ„åŠ›æƒé‡æ•°æ®")
        return
    
    # é€‰æ‹©å‰å‡ ä¸ªæ ·æœ¬è¿›è¡Œå¯è§†åŒ–
    num_samples = min(num_samples, len(attention_weights))
    
    fig, axes = plt.subplots(num_samples, 1, figsize=(12, 2*num_samples))
    if num_samples == 1:
        axes = [axes]
    
    for i in range(num_samples):
        weights = attention_weights[i].squeeze()
        if len(weights.shape) > 1:
            weights = weights.mean(axis=0)  # å¦‚æœæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼Œå–å¹³å‡
        
        axes[i].bar(range(len(weights)), weights, alpha=0.7)
        axes[i].set_title(f'æ ·æœ¬ {i+1} çš„æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒ')
        axes[i].set_xlabel('å¸§åºå·')
        axes[i].set_ylabel('æ³¨æ„åŠ›æƒé‡')
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight') 
        print(f"âœ… æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
    
    plt.show()

print("âœ… ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°æ¨¡å—å·²å®šä¹‰")