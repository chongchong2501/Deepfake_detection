# æ¨¡å‹å®šä¹‰ - æœ¬åœ°RTX4070ä¼˜åŒ–ç‰ˆæœ¬

import torch
import torch.nn as nn
import torchvision.models as models
from config import config

class OptimizedDeepfakeDetector(nn.Module):
    """ä¼˜åŒ–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹ - RTX4070ç‰ˆæœ¬"""
    
    def __init__(self, backbone=None, hidden_dim=None, num_layers=None, 
                 dropout=None, use_attention=None):
        super(OptimizedDeepfakeDetector, self).__init__()
        
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼
        self.backbone_name = backbone or config.BACKBONE
        self.hidden_dim = hidden_dim or config.HIDDEN_DIM
        self.num_layers = num_layers or config.NUM_LSTM_LAYERS
        self.dropout = dropout or config.DROPOUT
        self.use_attention = use_attention if use_attention is not None else config.USE_ATTENTION
        
        # ç‰¹å¾æå–å™¨
        if self.backbone_name == 'resnet50':
            self.backbone = models.resnet50(pretrained=True)
            feature_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        elif self.backbone_name == 'resnet18':
            self.backbone = models.resnet18(pretrained=True)
            feature_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        elif self.backbone_name == 'resnet101':
            self.backbone = models.resnet101(pretrained=True)
            feature_dim = self.backbone.fc.in_features
            self.backbone.fc = nn.Identity()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„backbone: {self.backbone_name}")
        
        # æ—¶åºå»ºæ¨¡ - åŒå‘LSTM
        self.lstm = nn.LSTM(
            input_size=feature_dim,
            hidden_size=self.hidden_dim,
            num_layers=self.num_layers,
            batch_first=True,
            dropout=self.dropout if self.num_layers > 1 else 0,
            bidirectional=True
        )
        
        lstm_output_dim = self.hidden_dim * 2  # åŒå‘LSTM
        
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        if self.use_attention:
            self.attention = nn.MultiheadAttention(
                embed_dim=lstm_output_dim,
                num_heads=8,
                dropout=self.dropout,
                batch_first=True
            )
            
            # å±‚å½’ä¸€åŒ–
            self.layer_norm = nn.LayerNorm(lstm_output_dim)
        
        # åˆ†ç±»å™¨ - æ›´æ·±çš„ç½‘ç»œç»“æ„
        self.classifier = nn.Sequential(
            nn.Linear(lstm_output_dim, self.hidden_dim),
            nn.BatchNorm1d(self.hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout),
            
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.BatchNorm1d(self.hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout),
            
            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),
            nn.ReLU(inplace=True),
            nn.Dropout(self.dropout // 2),
            
            nn.Linear(self.hidden_dim // 4, 1)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LSTM):
                for name, param in m.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.xavier_uniform_(param.data)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        nn.init.constant_(param.data, 0)
        
    def forward(self, x):
        # x shape: (batch_size, num_frames, channels, height, width)
        batch_size, num_frames = x.shape[:2]
        
        # é‡å¡‘ä¸º (batch_size * num_frames, channels, height, width)
        x = x.view(-1, *x.shape[2:])
        
        # ç‰¹å¾æå–
        with torch.cuda.amp.autocast(enabled=config.USE_MIXED_PRECISION):
            features = self.backbone(x)  # (batch_size * num_frames, feature_dim)
        
        # é‡å¡‘å›æ—¶åºæ ¼å¼
        features = features.view(batch_size, num_frames, -1)
        
        # ç¡®ä¿LSTMè¾“å…¥ä¸ºfloat32ç±»å‹
        features = features.float()
        
        # LSTMå¤„ç†
        lstm_out, (hidden, cell) = self.lstm(features)  # (batch_size, num_frames, hidden_dim*2)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = None
        if self.use_attention:
            # å¤šå¤´è‡ªæ³¨æ„åŠ›
            attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            attended_out = self.layer_norm(attended_out + lstm_out)
            
            # å…¨å±€å¹³å‡æ± åŒ–
            pooled = attended_out.mean(dim=1)  # (batch_size, hidden_dim*2)
        else:
            # ç®€å•çš„å…¨å±€å¹³å‡æ± åŒ–
            pooled = lstm_out.mean(dim=1)
        
        # åˆ†ç±»
        output = self.classifier(pooled)
        
        return output.squeeze(-1), attention_weights
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'backbone': self.backbone_name,
            'hidden_dim': self.hidden_dim,
            'num_layers': self.num_layers,
            'use_attention': self.use_attention,
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': total_params * 4 / 1024**2
        }
    
    def freeze_backbone(self, freeze=True):
        """å†»ç»“æˆ–è§£å†»backboneå‚æ•°"""
        for param in self.backbone.parameters():
            param.requires_grad = not freeze
        print(f"Backboneå‚æ•°å·²{'å†»ç»“' if freeze else 'è§£å†»'}")
    
    def unfreeze_backbone_layers(self, num_layers=2):
        """è§£å†»backboneçš„æœ€åå‡ å±‚"""
        # è·å–backboneçš„æ‰€æœ‰å±‚
        backbone_layers = list(self.backbone.children())
        
        # å†»ç»“æ‰€æœ‰å±‚
        self.freeze_backbone(True)
        
        # è§£å†»æœ€åå‡ å±‚
        for layer in backbone_layers[-num_layers:]:
            for param in layer.parameters():
                param.requires_grad = True
        
        print(f"å·²è§£å†»backboneçš„æœ€å{num_layers}å±‚")

def create_model(device=None):
    """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    if device is None:
        device = config.get_device()
    
    model = OptimizedDeepfakeDetector()
    model = model.to(device)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    model_info = model.get_model_info()
    print(f"\nğŸ¤– æ¨¡å‹ä¿¡æ¯:")
    print(f"æ¶æ„: {model_info['backbone']} + LSTM + {'Attention' if model_info['use_attention'] else 'No Attention'}")
    print(f"æ€»å‚æ•°æ•°é‡: {model_info['total_params']:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {model_info['trainable_params']:,}")
    print(f"æ¨¡å‹å¤§å°: {model_info['model_size_mb']:.1f} MB")
    print(f"è®¾å¤‡: {device}")
    
    return model

def test_model_forward(model, device=None):
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
    if device is None:
        device = config.get_device()
    
    model.eval()
    with torch.no_grad():
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        batch_size = 2
        num_frames = config.MAX_FRAMES
        channels, height, width = 3, *config.FRAME_SIZE
        
        test_input = torch.randn(batch_size, num_frames, channels, height, width, device=device)
        
        print(f"\nğŸ” æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
        
        try:
            outputs, attention_weights = model(test_input)
            print(f"è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
            print(f"è¾“å‡ºèŒƒå›´: [{outputs.min():.3f}, {outputs.max():.3f}]")
            
            if attention_weights is not None:
                print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
            
            print("âœ… æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
            return False