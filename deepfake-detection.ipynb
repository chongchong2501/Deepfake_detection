{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10125851,"sourceType":"datasetVersion","datasetId":6248577}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install av","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:15.603749Z","iopub.execute_input":"2025-07-06T09:53:15.604009Z","iopub.status.idle":"2025-07-06T09:53:21.017086Z","shell.execute_reply.started":"2025-07-06T09:53:15.603982Z","shell.execute_reply":"2025-07-06T09:53:21.016118Z"}},"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\nDownloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av\nSuccessfully installed av-15.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Cell 1: å¯¼å…¥åº“å’Œç¯å¢ƒè®¾ç½®","metadata":{}},{"cell_type":"code","source":"# Cell 1: å¯¼å…¥åº“å’Œç¯å¢ƒè®¾ç½®\n\n# ä¿®å¤CUDAå¤šè¿›ç¨‹é—®é¢˜\nimport multiprocessing as mp\ntry:\n    mp.set_start_method('spawn', force=True)\nexcept RuntimeError:\n    pass  # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥é”™è¯¯\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport random\nimport warnings\nimport gc\nimport json\nimport time\nimport sys\nfrom pathlib import Path\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n# PyTorchç›¸å…³\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as TF\nimport torchvision.models as models\nfrom torchvision.io import read_video\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler, autocast\n\n# æœºå™¨å­¦ä¹ æŒ‡æ ‡\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n    roc_curve, auc, precision_recall_curve, balanced_accuracy_score\n)\nfrom sklearn.model_selection import train_test_split\n\n# ç³»ç»Ÿç›‘æ§å’Œæ€§èƒ½åˆ†æ\nimport psutil\nimport traceback\n\n\n# è§†é¢‘å¤„ç† (PyAV)\ntry:\n    import av\n    PYAV_AVAILABLE = True\n    print(\"âœ… PyAVå·²å®‰è£…ï¼Œæ”¯æŒGPUè§†é¢‘å¤„ç†\")\nexcept ImportError:\n    PYAV_AVAILABLE = False\n    print(\"âš ï¸ PyAVæœªå®‰è£…ï¼Œè§†é¢‘å¤„ç†å°†å›é€€åˆ°CPUæ¨¡å¼\")\n\n# æ•°æ®å¢å¼º\ntry:\n    import albumentations as A\n    from albumentations.pytorch import ToTensorV2\n    ALBUMENTATIONS_AVAILABLE = True\nexcept ImportError:\n    ALBUMENTATIONS_AVAILABLE = False\n    print(\"è­¦å‘Š: albumentationsæœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€æ•°æ®å¢å¼º\")\n\nprint(\"âœ… æ‰€æœ‰åº“å¯¼å…¥å®Œæˆ\")","metadata":{"_uuid":"f6ebcf0d-d34a-4e53-b9c4-c85254661e41","_cell_guid":"4883bd2d-9463-4607-9e61-df4d76492a78","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-06T09:53:21.018536Z","iopub.execute_input":"2025-07-06T09:53:21.018800Z","iopub.status.idle":"2025-07-06T09:53:31.485053Z","shell.execute_reply.started":"2025-07-06T09:53:21.018775Z","shell.execute_reply":"2025-07-06T09:53:31.484306Z"}},"outputs":[{"name":"stdout","text":"âœ… PyAVå·²å®‰è£…ï¼Œæ”¯æŒGPUè§†é¢‘å¤„ç†\nâœ… æ‰€æœ‰åº“å¯¼å…¥å®Œæˆ\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Cell 2: å…¨å±€é…ç½®å’Œå·¥å…·å‡½æ•°","metadata":{}},{"cell_type":"code","source":"# Cell 2: å…¨å±€é…ç½®å’Œå·¥å…·å‡½æ•° - Kaggle T4 ä¼˜åŒ–ç‰ˆæœ¬\n\ndef set_seed(seed=42):\n    \"\"\"è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    # Kaggleç¯å¢ƒä¼˜åŒ–ï¼šå¹³è¡¡æ€§èƒ½å’Œå¯é‡å¤æ€§\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nset_seed(42)\n\n# Kaggle T4 GPUé…ç½®\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"GPUå‹å·: {gpu_name}\")\n    print(f\"GPUå†…å­˜: {gpu_memory:.1f} GB\")\n    \n    # Kaggle T4 GPUä¼˜åŒ–é…ç½®\n    torch.cuda.set_per_process_memory_fraction(0.9)  # ä¿å®ˆå†…å­˜ä½¿ç”¨\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    print(\"âœ… Kaggle T4 GPUä¼˜åŒ–é…ç½®å·²å¯ç”¨\")\n\n# åˆ›å»ºå¿…è¦çš„ç›®å½•\nfor dir_name in ['./data', './models', './logs', './results']:\n    os.makedirs(dir_name, exist_ok=True)\n\n# Kaggleç¯å¢ƒæ£€æµ‹\nIS_KAGGLE = os.path.exists('/kaggle')\nBASE_DATA_DIR = '/kaggle/input/ff-c23/FaceForensics++_C23' if IS_KAGGLE else './FaceForensics++_C23'\n\n# ç»Ÿä¸€æ•°æ®ç±»å‹é…ç½® - å…¨éƒ¨ä½¿ç”¨FP32æå‡å…¼å®¹æ€§\nUSE_FP32_ONLY = True  # å¼ºåˆ¶ä½¿ç”¨FP32ï¼Œç¡®ä¿æœ€ä½³å…¼å®¹æ€§\nprint(f\"æ•°æ®ç±»å‹ç­–ç•¥: FP32 (å…¼å®¹æ€§ä¼˜å…ˆ)\")\n\nprint(f\"ç¯å¢ƒ: {'Kaggle' if IS_KAGGLE else 'æœ¬åœ°'}\")\nprint(f\"æ•°æ®åŸºç¡€è·¯å¾„: {BASE_DATA_DIR}\")\nprint(\"âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.485737Z","iopub.execute_input":"2025-07-06T09:53:31.486060Z","iopub.status.idle":"2025-07-06T09:53:31.783621Z","shell.execute_reply.started":"2025-07-06T09:53:31.486043Z","shell.execute_reply":"2025-07-06T09:53:31.783076Z"}},"outputs":[{"name":"stdout","text":"ä½¿ç”¨è®¾å¤‡: cuda\nGPUå‹å·: Tesla T4\nGPUå†…å­˜: 14.7 GB\nâœ… GPUæ€§èƒ½ä¼˜åŒ–é…ç½®å·²å¯ç”¨\nç¯å¢ƒ: Kaggle\næ•°æ®åŸºç¡€è·¯å¾„: /kaggle/input/ff-c23/FaceForensics++_C23\nâœ… ç¯å¢ƒè®¾ç½®å®Œæˆ\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Cell 3: æ•°æ®å¤„ç†æ¨¡å—\n","metadata":{}},{"cell_type":"code","source":"# Cell 3: GPUåŠ é€Ÿæ•°æ®å¤„ç†æ¨¡å—\n\ndef extract_frames_gpu_accelerated(video_path, max_frames=16, target_size=(224, 224),\n                                  quality_threshold=20, use_gpu=True):\n    \"\"\"GPUåŠ é€Ÿçš„å¸§æå–å‡½æ•°\"\"\"\n    try:\n        # æ£€æŸ¥PyAVæ˜¯å¦å¯ç”¨\n        if not globals().get('PYAV_AVAILABLE', False):\n            print(f\"PyAVä¸å¯ç”¨ï¼Œä½¿ç”¨CPUå›é€€å¤„ç†: {video_path}\")\n            return extract_frames_cpu_fallback(video_path, max_frames, target_size, quality_threshold)\n            \n        # ä½¿ç”¨torchvisionçš„GPUåŠ é€Ÿè§†é¢‘è¯»å–\n        if use_gpu and torch.cuda.is_available():\n            device = torch.device('cuda')\n        else:\n            device = torch.device('cpu')\n            \n        # è¯»å–è§†é¢‘ï¼ˆtorchvisionè‡ªåŠ¨å¤„ç†è§£ç ï¼‰\n        try:\n            video_tensor, audio, info = read_video(video_path, pts_unit='sec')\n            # video_tensor shape: (T, H, W, C)\n        except Exception as e:\n            print(f\"GPUè§†é¢‘è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}\")\n            return extract_frames_cpu_fallback(video_path, max_frames, target_size, quality_threshold)\n        \n        if video_tensor.size(0) == 0:\n            return []\n            \n        # ç§»åŠ¨åˆ°GPUè¿›è¡Œå¤„ç†\n        video_tensor = video_tensor.to(device, non_blocking=True)\n        total_frames = video_tensor.size(0)\n        \n        # æ™ºèƒ½å¸§é‡‡æ ·ç­–ç•¥\n        if total_frames <= max_frames:\n            frame_indices = torch.arange(0, total_frames, device=device)\n        else:\n            # å‡åŒ€é‡‡æ ·\n            step = total_frames / max_frames\n            frame_indices = torch.arange(0, total_frames, step, device=device).long()[:max_frames]\n        \n        # æ‰¹é‡æå–å¸§\n        selected_frames = video_tensor[frame_indices]  # (max_frames, H, W, C)\n        \n        # GPUä¸Šè¿›è¡Œè´¨é‡æ£€æµ‹ï¼ˆä½¿ç”¨Sobelç®—å­ä»£æ›¿Laplacianï¼‰\n        if quality_threshold > 0:\n            # è½¬æ¢ä¸ºç°åº¦å›¾è¿›è¡Œè´¨é‡æ£€æµ‹ï¼ˆå…ˆè½¬æ¢ä¸ºfloatç±»å‹ï¼‰\n            gray_frames = selected_frames.float().mean(dim=-1, keepdim=True)  # (T, H, W, 1)\n            gray_frames = gray_frames.permute(0, 3, 1, 2)  # (T, 1, H, W)\n            \n            # ä½¿ç”¨Sobelç®—å­è®¡ç®—å›¾åƒè´¨é‡\n            sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n                                 dtype=torch.float32, device=device).view(1, 1, 3, 3)\n            sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n                                 dtype=torch.float32, device=device).view(1, 1, 3, 3)\n            \n            grad_x = F.conv2d(gray_frames, sobel_x, padding=1)\n            grad_y = F.conv2d(gray_frames, sobel_y, padding=1)\n            quality_scores = (grad_x.pow(2) + grad_y.pow(2)).mean(dim=[1, 2, 3])\n            \n            # è¿‡æ»¤ä½è´¨é‡å¸§\n            quality_mask = quality_scores > quality_threshold\n            if quality_mask.sum() > 0:\n                selected_frames = selected_frames[quality_mask]\n            \n        # GPUä¸Šè¿›è¡Œå°ºå¯¸è°ƒæ•´\n        selected_frames = selected_frames.permute(0, 3, 1, 2).float()  # (T, C, H, W)\n        if selected_frames.size(-1) != target_size[0] or selected_frames.size(-2) != target_size[1]:\n            selected_frames = F.interpolate(selected_frames, size=target_size, \n                                          mode='bilinear', align_corners=False)\n        \n        # ç¡®ä¿å¸§æ•°è¶³å¤Ÿ\n        current_frames = selected_frames.size(0)\n        if current_frames < max_frames:\n            # é‡å¤æœ€åä¸€å¸§\n            if current_frames > 0:\n                last_frame = selected_frames[-1:].repeat(max_frames - current_frames, 1, 1, 1)\n                selected_frames = torch.cat([selected_frames, last_frame], dim=0)\n            else:\n                # åˆ›å»ºé»‘è‰²å¸§\n                selected_frames = torch.zeros(max_frames, 3, target_size[0], target_size[1], \n                                            device=device, dtype=torch.float32)\n        \n        # é™åˆ¶åˆ°æœ€å¤§å¸§æ•°\n        selected_frames = selected_frames[:max_frames]\n        \n        # è½¬æ¢å›CPU numpyæ ¼å¼ï¼ˆä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç ï¼‰\n        frames_cpu = selected_frames.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n        frames_list = [frame for frame in frames_cpu]\n        \n        return frames_list\n        \n    except Exception as e:\n        print(f\"GPUå¸§æå–å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}\")\n        return extract_frames_cpu_fallback(video_path, max_frames, target_size, quality_threshold)\n\ndef extract_frames_cpu_fallback(video_path, max_frames=16, target_size=(224, 224), quality_threshold=20):\n    \"\"\"CPUå›é€€çš„å¸§æå–å‡½æ•°\"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n\n    if not cap.isOpened():\n        print(f\"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}\")\n        return frames\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    if total_frames == 0:\n        cap.release()\n        return frames\n\n    # å‡åŒ€é‡‡æ ·ç­–ç•¥\n    if total_frames <= max_frames:\n        frame_indices = list(range(0, total_frames, max(1, total_frames // max_frames)))\n    else:\n        step = max(1, total_frames // max_frames)\n        frame_indices = list(range(0, total_frames, step))[:max_frames]\n\n    frame_count = 0\n    for frame_idx in frame_indices:\n        if frame_count >= max_frames:\n            break\n\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        ret, frame = cap.read()\n\n        if ret:\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            \n            # è´¨é‡æ£€æµ‹\n            if quality_threshold > 0:\n                gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n                quality = cv2.Laplacian(gray, cv2.CV_64F).var()\n                if quality <= quality_threshold:\n                    continue\n            \n            frame = cv2.resize(frame, target_size)\n            frames.append(frame)\n            frame_count += 1\n\n    cap.release()\n\n    # å¦‚æœå¸§æ•°ä¸è¶³ï¼Œé‡å¤æœ€åä¸€å¸§\n    while len(frames) < max_frames and len(frames) > 0:\n        frames.append(frames[-1].copy())\n\n    return frames[:max_frames]\n\n# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸå‡½æ•°å\ndef extract_frames_memory_efficient(video_path, max_frames=16, target_size=(224, 224),\n                                   quality_threshold=20, skip_frames=3):\n    \"\"\"å…¼å®¹æ€§åŒ…è£…å‡½æ•°ï¼Œä¼˜å…ˆä½¿ç”¨GPUåŠ é€Ÿ\"\"\"\n    return extract_frames_gpu_accelerated(video_path, max_frames, target_size, quality_threshold)\n\ndef process_videos_simple(base_data_dir, max_videos_per_class=60, max_frames=16):\n    \"\"\"ç®€åŒ–çš„è§†é¢‘å¤„ç†å‡½æ•°\"\"\"\n    data_list = []\n    fake_methods = ['Deepfakes', 'Face2Face', 'FaceShifter', 'FaceSwap', 'NeuralTextures']\n\n    print(\"å¼€å§‹å¤„ç†çœŸå®è§†é¢‘...\")\n    # å¤„ç†çœŸå®è§†é¢‘\n    original_dir = os.path.join(base_data_dir, 'original')\n    if os.path.exists(original_dir):\n        video_files = [f for f in os.listdir(original_dir)\n                      if f.endswith(('.mp4', '.avi', '.mov'))]\n        \n        if len(video_files) > max_videos_per_class:\n            video_files = random.sample(video_files, max_videos_per_class)\n\n        print(f\"æ‰¾åˆ° {len(video_files)} ä¸ªçœŸå®è§†é¢‘\")\n\n        for video_file in tqdm(video_files, desc=\"å¤„ç†çœŸå®è§†é¢‘\"):\n            try:\n                video_path = os.path.join(original_dir, video_file)\n                frames = extract_frames_memory_efficient(video_path, max_frames)\n                \n                if len(frames) >= max_frames // 2:  # è‡³å°‘è¦æœ‰ä¸€åŠçš„å¸§\n                    data_list.append({\n                        'video_path': video_path,\n                        'frames': frames,\n                        'label': 0,  # çœŸå®è§†é¢‘\n                        'method': 'original'\n                    })\n            except Exception as e:\n                print(f\"å¤„ç†è§†é¢‘ {video_file} æ—¶å‡ºé”™: {e}\")\n                continue\n\n    # å¤„ç†ä¼ªé€ è§†é¢‘\n    print(\"å¼€å§‹å¤„ç†ä¼ªé€ è§†é¢‘...\")\n    for method in fake_methods:\n        method_dir = os.path.join(base_data_dir, method)\n        if os.path.exists(method_dir):\n            video_files = [f for f in os.listdir(method_dir)\n                          if f.endswith(('.mp4', '.avi', '.mov'))]\n            \n            if len(video_files) > max_videos_per_class:\n                video_files = random.sample(video_files, max_videos_per_class)\n\n            print(f\"å¤„ç† {method}: {len(video_files)} ä¸ªè§†é¢‘\")\n\n            for video_file in tqdm(video_files, desc=f\"å¤„ç†{method}\"):\n                try:\n                    video_path = os.path.join(method_dir, video_file)\n                    frames = extract_frames_memory_efficient(video_path, max_frames)\n                    \n                    if len(frames) >= max_frames // 2:\n                        data_list.append({\n                            'video_path': video_path,\n                            'frames': frames,\n                            'label': 1,  # ä¼ªé€ è§†é¢‘\n                            'method': method\n                        })\n                except Exception as e:\n                    print(f\"å¤„ç†è§†é¢‘ {video_file} æ—¶å‡ºé”™: {e}\")\n                    continue\n\n    print(f\"\\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(data_list)} ä¸ªè§†é¢‘\")\n    return data_list\n\ndef create_dataset_split(data_list, test_size=0.2, val_size=0.1):\n    \"\"\"åˆ›å»ºæ•°æ®é›†åˆ’åˆ†\"\"\"\n    # åˆ†ç¦»çœŸå®å’Œä¼ªé€ æ•°æ®\n    real_data = [item for item in data_list if item['label'] == 0]\n    fake_data = [item for item in data_list if item['label'] == 1]\n    \n    print(f\"çœŸå®è§†é¢‘: {len(real_data)} ä¸ª\")\n    print(f\"ä¼ªé€ è§†é¢‘: {len(fake_data)} ä¸ª\")\n    \n    # åˆ†åˆ«åˆ’åˆ†çœŸå®å’Œä¼ªé€ æ•°æ®\n    real_train, real_temp = train_test_split(real_data, test_size=test_size+val_size, random_state=42)\n    real_val, real_test = train_test_split(real_temp, test_size=test_size/(test_size+val_size), random_state=42)\n    \n    fake_train, fake_temp = train_test_split(fake_data, test_size=test_size+val_size, random_state=42)\n    fake_val, fake_test = train_test_split(fake_temp, test_size=test_size/(test_size+val_size), random_state=42)\n    \n    # åˆå¹¶æ•°æ®\n    train_data = real_train + fake_train\n    val_data = real_val + fake_val\n    test_data = real_test + fake_test\n    \n    # æ‰“ä¹±æ•°æ®\n    random.shuffle(train_data)\n    random.shuffle(val_data)\n    random.shuffle(test_data)\n    \n    return train_data, val_data, test_data\n\ndef save_dataset_to_csv(data_list, filename):\n    \"\"\"å°†æ•°æ®é›†ä¿å­˜ä¸ºCSVæ–‡ä»¶\"\"\"\n    df_data = []\n    for item in data_list:\n        df_data.append({\n            'video_path': item['video_path'],\n            'label': item['label'],\n            'method': item['method'],\n            'num_frames': len(item['frames'])\n        })\n    \n    df = pd.DataFrame(df_data)\n    df.to_csv(filename, index=False)\n    print(f\"æ•°æ®é›†å·²ä¿å­˜åˆ°: {filename}\")\n    return df\n\nprint(\"âœ… æ•°æ®å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.785347Z","iopub.execute_input":"2025-07-06T09:53:31.785530Z","iopub.status.idle":"2025-07-06T09:53:31.812934Z","shell.execute_reply.started":"2025-07-06T09:53:31.785516Z","shell.execute_reply":"2025-07-06T09:53:31.812253Z"}},"outputs":[{"name":"stdout","text":"âœ… æ•°æ®å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Cell 4: æ•°æ®é›†ç±»å®šä¹‰\n","metadata":{}},{"cell_type":"code","source":"# Cell 4: æ•°æ®é›†ç±»å®šä¹‰\n\nclass DeepfakeVideoDataset(Dataset):\n    \"\"\"æ·±åº¦ä¼ªé€ è§†é¢‘æ•°æ®é›†ç±» - Kaggle T4 ä¼˜åŒ–ç‰ˆæœ¬\"\"\"\n    \n    def __init__(self, csv_file=None, data_list=None, transform=None, max_frames=16, \n                 gpu_preprocessing=True, cache_frames=False):\n        if csv_file is not None:\n            self.df = pd.read_csv(csv_file)\n            self.data_list = None\n        elif data_list is not None:\n            self.data_list = data_list\n            self.df = None\n        else:\n            raise ValueError(\"å¿…é¡»æä¾›csv_fileæˆ–data_list\")\n            \n        self.transform = transform\n        self.max_frames = max_frames\n        self.gpu_preprocessing = gpu_preprocessing and torch.cuda.is_available()\n        self.cache_frames = cache_frames\n        \n        # ç®€åŒ–ç¼“å­˜ç³»ç»Ÿ - ä»…CPUç¼“å­˜ï¼Œé¿å…GPUå†…å­˜å‹åŠ›\n        self.frame_cache = {} if cache_frames else None\n        self.cache_hits = 0\n        self.cache_misses = 0\n        \n        # GPUé¢„å¤„ç†çš„æ ‡å‡†åŒ–å‚æ•° - ç»Ÿä¸€ä½¿ç”¨FP32\n        if self.gpu_preprocessing:\n            self.mean = torch.tensor([0.485, 0.456, 0.406], device='cuda', dtype=torch.float32)\n            self.std = torch.tensor([0.229, 0.224, 0.225], device='cuda', dtype=torch.float32)\n            \n        print(f\"ğŸš€ æ•°æ®é›†åˆå§‹åŒ–: GPUé¢„å¤„ç†={'å¯ç”¨' if self.gpu_preprocessing else 'ç¦ç”¨'}, \"\n              f\"ç¼“å­˜={'å¯ç”¨' if self.cache_frames else 'ç¦ç”¨'}, æ•°æ®ç±»å‹=FP32\")\n    \n    def __len__(self):\n        if self.df is not None:\n            return len(self.df)\n        return len(self.data_list)\n    \n    def __getitem__(self, idx):\n        if self.data_list is not None:\n            # ç›´æ¥ä»å†…å­˜ä¸­çš„æ•°æ®åˆ—è¡¨è·å–\n            item = self.data_list[idx]\n            frames = item['frames']\n            label = item['label']\n            video_path = None\n        else:\n            # ä»CSVæ–‡ä»¶è·å–è·¯å¾„\n            row = self.df.iloc[idx]\n            video_path = row['video_path']\n            label = row['label']\n            frames = None\n        \n        # ç®€åŒ–çš„æ•°æ®å¤„ç†æµç¨‹\n        if frames is None:\n            # æ£€æŸ¥CPUç¼“å­˜\n            if self.cache_frames and video_path in self.frame_cache:\n                frames = self.frame_cache[video_path]\n                self.cache_hits += 1\n            else:\n                frames = extract_frames_gpu_accelerated(video_path, self.max_frames, target_size=(224, 224))\n                self.cache_misses += 1\n                # ç¼“å­˜å¸§æ•°æ®\n                if self.cache_frames and len(frames) > 0:\n                    self.frame_cache[video_path] = frames\n        \n        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§\n        if len(frames) == 0:\n            frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.max_frames)]\n        \n        while len(frames) < self.max_frames:\n            frames.append(frames[-1].copy() if frames else np.zeros((224, 224, 3), dtype=np.uint8))\n        \n        frames = frames[:self.max_frames]\n        \n        # ç»Ÿä¸€çš„æ•°æ®é¢„å¤„ç† - å…¨éƒ¨ä½¿ç”¨FP32\n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n            video_tensor = torch.stack(frames)\n        elif self.gpu_preprocessing:\n            # GPUé¢„å¤„ç†ï¼šå‡å°‘CPU-GPUä¼ è¾“æ¬¡æ•°\n            frames_array = np.stack(frames)  # (T, H, W, C)\n            video_tensor = torch.from_numpy(frames_array).permute(0, 3, 1, 2).float()  # (T, C, H, W)\n            \n            # ç§»åŠ¨åˆ°GPUå¹¶è¿›è¡Œé¢„å¤„ç† - ç»Ÿä¸€ä½¿ç”¨FP32\n            video_tensor = video_tensor.to('cuda', non_blocking=True, dtype=torch.float32) / 255.0\n            \n            # æ ‡å‡†åŒ–\n            video_tensor = (video_tensor - self.mean.view(1, 3, 1, 1)) / self.std.view(1, 3, 1, 1)\n        else:\n            # CPUé¢„å¤„ç†\n            frames = [torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0 for frame in frames]\n            video_tensor = torch.stack(frames)\n        \n        # æ ‡ç­¾å¤„ç† - ç»Ÿä¸€ä½¿ç”¨FP32\n        label_tensor = torch.tensor(label, dtype=torch.float32)\n        if self.gpu_preprocessing:\n            label_tensor = label_tensor.to('cuda', non_blocking=True)\n        \n        return video_tensor, label_tensor\n    \n    def get_cache_stats(self):\n        \"\"\"è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯\"\"\"\n        total_requests = self.cache_hits + self.cache_misses\n        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0\n        return {\n            'cache_hits': self.cache_hits,\n            'cache_misses': self.cache_misses,\n            'hit_rate': hit_rate,\n            'cpu_cache_size': len(self.frame_cache) if self.frame_cache else 0\n        }\n\nprint(\"âœ… æ•°æ®é›†ç±»å®šä¹‰å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.813656Z","iopub.execute_input":"2025-07-06T09:53:31.813908Z","iopub.status.idle":"2025-07-06T09:53:31.840730Z","shell.execute_reply.started":"2025-07-06T09:53:31.813891Z","shell.execute_reply":"2025-07-06T09:53:31.840017Z"}},"outputs":[{"name":"stdout","text":"âœ… æ•°æ®é›†ç±»å®šä¹‰å®Œæˆ\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Cell 5: æ¨¡å‹å®šä¹‰","metadata":{}},{"cell_type":"code","source":"# Cell 5: æ¨¡å‹å®šä¹‰\n\nclass OptimizedDeepfakeDetector(nn.Module):\n    \"\"\"ä¼˜åŒ–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹\"\"\"\n    \n    def __init__(self, backbone='resnet50', hidden_dim=512, num_layers=2, \n                 dropout=0.3, use_attention=True):\n        super(OptimizedDeepfakeDetector, self).__init__()\n        \n        self.use_attention = use_attention\n        \n        # ç‰¹å¾æå–å™¨\n        if backbone == 'resnet50':\n            self.backbone = models.resnet50(pretrained=True)\n            feature_dim = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n        elif backbone == 'resnet18':\n            self.backbone = models.resnet18(pretrained=True)\n            feature_dim = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n        else:\n            raise ValueError(f\"ä¸æ”¯æŒçš„backbone: {backbone}\")\n        \n        # æ—¶åºå»ºæ¨¡\n        self.lstm = nn.LSTM(\n            input_size=feature_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True\n        )\n        \n        lstm_output_dim = hidden_dim * 2  # åŒå‘LSTM\n        \n        # æ³¨æ„åŠ›æœºåˆ¶\n        if self.use_attention:\n            self.attention = nn.MultiheadAttention(\n                embed_dim=lstm_output_dim,\n                num_heads=8,\n                dropout=dropout,\n                batch_first=True\n            )\n        \n        # åˆ†ç±»å™¨ (ç§»é™¤ Sigmoidï¼Œå› ä¸ºä½¿ç”¨ BCEWithLogitsLoss)\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_output_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n        \n    def forward(self, x):\n        # x shape: (batch_size, num_frames, channels, height, width)\n        batch_size, num_frames = x.shape[:2]\n        \n        # é‡å¡‘ä¸º (batch_size * num_frames, channels, height, width)\n        x = x.view(-1, *x.shape[2:])\n        \n        # ç‰¹å¾æå–\n        features = self.backbone(x)  # (batch_size * num_frames, feature_dim)\n        \n        # é‡å¡‘å›æ—¶åºæ ¼å¼\n        features = features.view(batch_size, num_frames, -1)\n        \n        # LSTMå¤„ç†\n        lstm_out, _ = self.lstm(features)  # (batch_size, num_frames, hidden_dim*2)\n        \n        # æ³¨æ„åŠ›æœºåˆ¶\n        attention_weights = None\n        if self.use_attention:\n            attended_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)\n            # å…¨å±€å¹³å‡æ± åŒ–\n            pooled = attended_out.mean(dim=1)  # (batch_size, hidden_dim*2)\n        else:\n            # ç®€å•çš„å…¨å±€å¹³å‡æ± åŒ–\n            pooled = lstm_out.mean(dim=1)\n        \n        # åˆ†ç±»\n        output = self.classifier(pooled)\n        \n        return output.squeeze(-1), attention_weights\n\nprint(\"âœ… æ¨¡å‹å®šä¹‰å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.841496Z","iopub.execute_input":"2025-07-06T09:53:31.841742Z","iopub.status.idle":"2025-07-06T09:53:31.862352Z","shell.execute_reply.started":"2025-07-06T09:53:31.841720Z","shell.execute_reply":"2025-07-06T09:53:31.861641Z"}},"outputs":[{"name":"stdout","text":"âœ… æ¨¡å‹å®šä¹‰å®Œæˆ\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Cell 6: æŸå¤±å‡½æ•°å’Œå·¥å…·ç±»","metadata":{}},{"cell_type":"code","source":"# Cell 6: æŸå¤±å‡½æ•°å’Œå·¥å…·ç±»\n\nclass FocalLoss(nn.Module):\n    \"\"\"ç„¦ç‚¹æŸå¤±å‡½æ•° - è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜\"\"\"\n    \n    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        # ä½¿ç”¨ BCEWithLogitsLoss ä»¥å…¼å®¹ autocast\n        ce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n        # è®¡ç®—æ¦‚ç‡ç”¨äºfocal weight\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\nclass EarlyStopping:\n    \"\"\"æ—©åœæœºåˆ¶\"\"\"\n    \n    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.restore_best_weights = restore_best_weights\n        self.best_loss = None\n        self.counter = 0\n        self.best_weights = None\n\n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(model)\n        elif val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.save_checkpoint(model)\n        else:\n            self.counter += 1\n\n        if self.counter >= self.patience:\n            if self.restore_best_weights:\n                model.load_state_dict(self.best_weights)\n            return True\n        return False\n\n    def save_checkpoint(self, model):\n        self.best_weights = model.state_dict().copy()\n\ndef get_transforms(mode='train', image_size=160):\n    \"\"\"è·å–æ•°æ®å˜æ¢\"\"\"\n    if mode == 'train':\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((int(image_size * 1.1), int(image_size * 1.1))),\n            transforms.RandomCrop((image_size, image_size)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.RandomRotation(degrees=10),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))\n        ])\n    else:\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\nprint(\"âœ… æŸå¤±å‡½æ•°å’Œå·¥å…·ç±»å®šä¹‰å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.863051Z","iopub.execute_input":"2025-07-06T09:53:31.863268Z","iopub.status.idle":"2025-07-06T09:53:31.881479Z","shell.execute_reply.started":"2025-07-06T09:53:31.863244Z","shell.execute_reply":"2025-07-06T09:53:31.880809Z"}},"outputs":[{"name":"stdout","text":"âœ… æŸå¤±å‡½æ•°å’Œå·¥å…·ç±»å®šä¹‰å®Œæˆ\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Cell 7: è®­ç»ƒå’ŒéªŒè¯å‡½æ•°","metadata":{}},{"cell_type":"code","source":"# Cell 7: è®­ç»ƒå’ŒéªŒè¯å‡½æ•°\n\ndef train_epoch(model, train_loader, criterion, optimizer, device, scaler=None):\n    \"\"\"Kaggle T4 GPUä¼˜åŒ–çš„è®­ç»ƒå‡½æ•°\"\"\"\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n\n    pbar = tqdm(train_loader, desc='Training', leave=False)\n    \n    for batch_idx, (data, target) in enumerate(pbar):\n        # æ•°æ®ä¼ è¾“åˆ°GPU\n        data = data.to(device, non_blocking=True)\n        target = target.to(device, non_blocking=True)\n        \n        # æ¢¯åº¦æ¸…é›¶\n        optimizer.zero_grad(set_to_none=True)\n\n        # å‰å‘ä¼ æ’­ï¼ˆç»Ÿä¸€ä½¿ç”¨FP32ï¼‰\n        output, _ = model(data)\n        loss = criterion(output, target)\n        \n        # åå‘ä¼ æ’­\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n        \n        # è®¡ç®—å‡†ç¡®ç‡\n        with torch.no_grad():\n            probs = torch.sigmoid(output)\n            predicted = (probs > 0.5).float()\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n            # æ”¶é›†é¢„æµ‹ç»“æœ\n            all_preds.extend(probs.detach().cpu().numpy())\n            all_targets.extend(target.detach().cpu().numpy())\n\n        pbar.set_postfix({\n            'Loss': f'{loss.item():.4f}',\n            'Acc': f'{100.*correct/total:.2f}%'\n        })\n        \n        # å®šæœŸæ¸…ç†GPUç¼“å­˜\n        if batch_idx % 50 == 0 and torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    avg_loss = total_loss / len(train_loader)\n    accuracy = 100. * correct / total\n\n    try:\n        auc_score = roc_auc_score(all_targets, all_preds)\n    except:\n        auc_score = 0.0\n\n    return avg_loss, accuracy, auc_score\n\ndef validate_epoch(model, val_loader, criterion, device, scaler=None):\n    \"\"\"Kaggle T4 GPUä¼˜åŒ–çš„éªŒè¯å‡½æ•°\"\"\"\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n\n    with torch.no_grad():\n        pbar = tqdm(val_loader, desc='Validation', leave=False)\n\n        for batch_idx, (data, target) in enumerate(pbar):\n            # æ•°æ®ä¼ è¾“åˆ°GPU\n            data = data.to(device, non_blocking=True)\n            target = target.to(device, non_blocking=True)\n            \n            # å‰å‘ä¼ æ’­ï¼ˆç»Ÿä¸€ä½¿ç”¨FP32ï¼‰\n            output, _ = model(data)\n            loss = criterion(output, target)\n\n            total_loss += loss.item()\n            \n            # è®¡ç®—å‡†ç¡®ç‡\n            probs = torch.sigmoid(output)\n            predicted = (probs > 0.5).float()\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n            # æ”¶é›†é¢„æµ‹ç»“æœ\n            all_preds.extend(probs.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n\n            pbar.set_postfix({\n                'Loss': f'{loss.item():.4f}',\n                'Acc': f'{100.*correct/total:.2f}%'\n            })\n            \n            # å®šæœŸæ¸…ç†GPUç¼“å­˜\n            if batch_idx % 50 == 0 and torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n    avg_loss = total_loss / len(val_loader)\n    accuracy = 100. * correct / total\n\n    try:\n        auc_score = roc_auc_score(all_targets, all_preds)\n    except:\n        auc_score = 0.0\n\n    return avg_loss, accuracy, auc_score\n\nprint(\"âœ… è®­ç»ƒå’ŒéªŒè¯å‡½æ•°å®šä¹‰å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.882273Z","iopub.execute_input":"2025-07-06T09:53:31.882572Z","iopub.status.idle":"2025-07-06T09:53:31.902988Z","shell.execute_reply.started":"2025-07-06T09:53:31.882553Z","shell.execute_reply":"2025-07-06T09:53:31.902466Z"}},"outputs":[{"name":"stdout","text":"âœ… è®­ç»ƒå’ŒéªŒè¯å‡½æ•°å®šä¹‰å®Œæˆ\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Cell 8: è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–","metadata":{}},{"cell_type":"code","source":"# Cell 8: è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–\n\ndef evaluate_model_optimized(model, test_loader, criterion, device):\n    \"\"\"ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°å‡½æ•°\"\"\"\n    model.eval()\n    total_loss = 0.0\n    all_predictions = []\n    all_targets = []\n    all_scores = []\n    \n    inference_times = []\n    \n    print(\"ğŸš€ å¼€å§‹æ¨¡å‹è¯„ä¼°...\")\n    \n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(tqdm(test_loader, desc=\"è¯„ä¼°è¿›åº¦\")):\n            data, target = data.to(device), target.to(device)\n            \n            # è®°å½•æ¨ç†æ—¶é—´\n            start_time = time.time()\n            output, attention_weights = model(data)\n            inference_time = time.time() - start_time\n            inference_times.append(inference_time)\n            \n            # è®¡ç®—æŸå¤±\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            \n            # æ”¶é›†é¢„æµ‹ç»“æœ (åº”ç”¨ sigmoid è·å¾—æ¦‚ç‡)\n            probs = torch.sigmoid(output)\n            predictions = (probs > 0.5).float()\n            all_predictions.extend(predictions.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n            all_scores.extend(probs.cpu().numpy())\n    \n    avg_loss = total_loss / len(test_loader)\n    avg_inference_time = np.mean(inference_times)\n    total_inference_time = np.sum(inference_times)\n    \n    print(f\"âœ… è¯„ä¼°å®Œæˆ\")\n    print(f\"å¹³å‡æŸå¤±: {avg_loss:.4f}\")\n    print(f\"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time*1000:.2f} ms/batch\")\n    \n    return {\n        'loss': avg_loss,\n        'predictions': np.array(all_predictions),\n        'targets': np.array(all_targets),\n        'scores': np.array(all_scores),\n        'avg_inference_time': avg_inference_time,\n        'total_inference_time': total_inference_time\n    }\n\ndef calculate_comprehensive_metrics(predictions, targets, scores):\n    \"\"\"è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡\"\"\"\n    # åŸºç¡€æŒ‡æ ‡\n    accuracy = accuracy_score(targets, predictions)\n    balanced_acc = balanced_accuracy_score(targets, predictions)\n    precision = precision_score(targets, predictions, zero_division=0)\n    recall = recall_score(targets, predictions, zero_division=0)\n    f1 = f1_score(targets, predictions, zero_division=0)\n    \n    # æ··æ·†çŸ©é˜µ\n    cm = confusion_matrix(targets, predictions)\n    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n    \n    # ç‰¹å¼‚æ€§å’Œè´Ÿé¢„æµ‹å€¼\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n    \n    # AUCæŒ‡æ ‡\n    try:\n        auc_roc = roc_auc_score(targets, scores)\n    except:\n        auc_roc = 0.0\n    \n    try:\n        precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)\n        auc_pr = auc(recall_curve, precision_curve)\n    except:\n        auc_pr = 0.0\n    \n    return {\n        'accuracy': accuracy,\n        'balanced_accuracy': balanced_acc,\n        'precision': precision,\n        'recall': recall,\n        'specificity': specificity,\n        'f1': f1,\n        'auc_roc': auc_roc,\n        'auc_pr': auc_pr,\n        'npv': npv,\n        'confusion_matrix': cm,\n        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n    }\n\ndef plot_enhanced_confusion_matrix(cm, save_path):\n    \"\"\"ç»˜åˆ¶å¢å¼ºçš„æ··æ·†çŸ©é˜µ\"\"\"\n    plt.figure(figsize=(10, 8))\n    \n    # è®¡ç®—ç™¾åˆ†æ¯”\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n    \n    # åˆ›å»ºæ ‡ç­¾\n    labels = np.array([[\n        f'{cm[i,j]}\\n({cm_percent[i,j]:.1f}%)' \n        for j in range(cm.shape[1])\n    ] for i in range(cm.shape[0])])\n    \n    # ç»˜åˆ¶çƒ­å›¾\n    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', \n                xticklabels=['çœŸå®', 'ä¼ªé€ '],\n                yticklabels=['çœŸå®', 'ä¼ªé€ '],\n                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})\n    \n    plt.title('å¢å¼ºæ··æ·†çŸ©é˜µ', fontsize=16, fontweight='bold')\n    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)\n    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)\n    \n    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯\n    tn, fp, fn, tp = cm.ravel()\n    accuracy = (tp + tn) / (tp + tn + fp + fn)\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    stats_text = f'å‡†ç¡®ç‡: {accuracy:.3f}\\nç²¾ç¡®ç‡: {precision:.3f}\\nå¬å›ç‡: {recall:.3f}\\nF1åˆ†æ•°: {f1:.3f}'\n    plt.text(2.1, 0.5, stats_text, fontsize=10, \n             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {save_path}\")\n\ndef plot_roc_pr_curves(targets, scores, save_path):\n    \"\"\"ç»˜åˆ¶ROCå’ŒPRæ›²çº¿\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # ROCæ›²çº¿\n    fpr, tpr, _ = roc_curve(targets, scores)\n    roc_auc = auc(fpr, tpr)\n    \n    ax1.plot(fpr, tpr, color='darkorange', lw=2,\n             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')\n    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    ax1.set_xlim([0.0, 1.0])\n    ax1.set_ylim([0.0, 1.05])\n    ax1.set_xlabel('å‡æ­£ç‡')\n    ax1.set_ylabel('çœŸæ­£ç‡')\n    ax1.set_title('ROCæ›²çº¿')\n    ax1.legend(loc='lower right')\n    ax1.grid(True, alpha=0.3)\n    \n    # PRæ›²çº¿\n    precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)\n    pr_auc = auc(recall_curve, precision_curve)\n    \n    ax2.plot(recall_curve, precision_curve, color='darkgreen', lw=2,\n             label=f'PRæ›²çº¿ (AUC = {pr_auc:.4f})')\n    ax2.set_xlim([0.0, 1.0])\n    ax2.set_ylim([0.0, 1.05])\n    ax2.set_xlabel('å¬å›ç‡')\n    ax2.set_ylabel('ç²¾ç¡®ç‡')\n    ax2.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')\n    ax2.legend(loc='lower left')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"ROC/PRæ›²çº¿å·²ä¿å­˜åˆ°: {save_path}\")\n\nprint(\"âœ… è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–å®šä¹‰å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.903817Z","iopub.execute_input":"2025-07-06T09:53:31.904033Z","iopub.status.idle":"2025-07-06T09:53:31.927720Z","shell.execute_reply.started":"2025-07-06T09:53:31.904018Z","shell.execute_reply":"2025-07-06T09:53:31.927023Z"}},"outputs":[{"name":"stdout","text":"âœ… è¯„ä¼°å‡½æ•°å’Œå¯è§†åŒ–å®šä¹‰å®Œæˆ\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Cell 9: æ•°æ®å¤„ç†å’Œå‡†å¤‡\n","metadata":{}},{"cell_type":"code","source":"# Cell 9: æ•°æ®å¤„ç†å’Œå‡†å¤‡\n\n# å¦‚æœéœ€è¦å¤„ç†æ•°æ®ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰\nif not os.path.exists('./data/train.csv'):\n    print(\"ğŸ“ å¼€å§‹æ•°æ®å¤„ç†...\")\n    data_list = process_videos_simple(BASE_DATA_DIR, max_videos_per_class=120, max_frames=16)\n    \n    if len(data_list) == 0:\n        print(\"âŒ æœªæ‰¾åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„\")\n        raise ValueError(\"æ•°æ®è·¯å¾„é”™è¯¯æˆ–æ•°æ®ä¸å­˜åœ¨\")\n    \n    train_data, val_data, test_data = create_dataset_split(data_list)\n    \n    # ä¿å­˜æ•°æ®é›†\n    save_dataset_to_csv(train_data, './data/train.csv')\n    save_dataset_to_csv(val_data, './data/val.csv')\n    save_dataset_to_csv(test_data, './data/test.csv')\n    \n    print(f\"è®­ç»ƒé›†: {len(train_data)} ä¸ªæ ·æœ¬\")\n    print(f\"éªŒè¯é›†: {len(val_data)} ä¸ªæ ·æœ¬\")\n    print(f\"æµ‹è¯•é›†: {len(test_data)} ä¸ªæ ·æœ¬\")\nelse:\n    print(\"ğŸ“Š æ•°æ®é›†å·²å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®å¤„ç†æ­¥éª¤\")\n    # è¯»å–ç°æœ‰æ•°æ®é›†ä¿¡æ¯\n    train_df = pd.read_csv('./data/train.csv')\n    val_df = pd.read_csv('./data/val.csv')\n    test_df = pd.read_csv('./data/test.csv')\n    \n    print(f\"è®­ç»ƒé›†: {len(train_df)} ä¸ªæ ·æœ¬\")\n    print(f\"éªŒè¯é›†: {len(val_df)} ä¸ªæ ·æœ¬\")\n    print(f\"æµ‹è¯•é›†: {len(test_df)} ä¸ªæ ·æœ¬\")\n    \n    # æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒ\n    print(\"\\næ•°æ®åˆ†å¸ƒ:\")\n    print(\"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:\")\n    print(train_df['label'].value_counts())\n    print(\"\\néªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ:\")\n    print(val_df['label'].value_counts())\n    print(\"\\næµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ:\")\n    print(test_df['label'].value_counts())\n\nprint(\"âœ… æ•°æ®å‡†å¤‡å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T09:53:31.929417Z","iopub.execute_input":"2025-07-06T09:53:31.929607Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ å¼€å§‹æ•°æ®å¤„ç†...\nå¼€å§‹å¤„ç†çœŸå®è§†é¢‘...\næ‰¾åˆ° 120 ä¸ªçœŸå®è§†é¢‘\n","output_type":"stream"},{"name":"stderr","text":"å¤„ç†çœŸå®è§†é¢‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [09:37<00:00,  4.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"å¼€å§‹å¤„ç†ä¼ªé€ è§†é¢‘...\nå¤„ç† Deepfakes: 120 ä¸ªè§†é¢‘\n","output_type":"stream"},{"name":"stderr","text":"å¤„ç†Deepfakes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [09:48<00:00,  4.91s/it]\n","output_type":"stream"},{"name":"stdout","text":"å¤„ç† Face2Face: 120 ä¸ªè§†é¢‘\n","output_type":"stream"},{"name":"stderr","text":"å¤„ç†Face2Face: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120/120 [08:53<00:00,  4.44s/it]\n","output_type":"stream"},{"name":"stdout","text":"å¤„ç† FaceShifter: 120 ä¸ªè§†é¢‘\n","output_type":"stream"},{"name":"stderr","text":"å¤„ç†FaceShifter:  19%|â–ˆâ–‰        | 23/120 [02:15<13:21,  8.26s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10: åˆ›å»ºæ•°æ®åŠ è½½å™¨\n","metadata":{}},{"cell_type":"code","source":"# Cell 10: åˆ›å»ºæ•°æ®åŠ è½½å™¨ - Kaggle T4 ä¼˜åŒ–ç‰ˆæœ¬\n\nprint(\"ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...\")\n\n# ç®€åŒ–æ•°æ®å˜æ¢ - ä½¿ç”¨GPUé¢„å¤„ç†æ›¿ä»£CPUå˜æ¢\ntrain_transform = None\nval_transform = None\n\nprint(f\"ğŸ”§ åˆ›å»ºæ•°æ®é›†ï¼ˆKaggle T4ä¼˜åŒ–é…ç½®ï¼‰...\")\nprint(f\"ğŸ“Š æ•°æ®ç±»å‹: FP32 (å…¼å®¹æ€§ä¼˜å…ˆ)\")\n\ntrain_dataset = DeepfakeVideoDataset(\n    csv_file='./data/train.csv',\n    transform=train_transform,\n    max_frames=16,\n    gpu_preprocessing=True,    # å¯ç”¨GPUé¢„å¤„ç†\n    cache_frames=False        # ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœå†…å­˜\n)\n\nval_dataset = DeepfakeVideoDataset(\n    csv_file='./data/val.csv',\n    transform=val_transform,\n    max_frames=16,\n    gpu_preprocessing=True,    # å¯ç”¨GPUé¢„å¤„ç†\n    cache_frames=False        # ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœå†…å­˜\n)\n\ntest_dataset = DeepfakeVideoDataset(\n    csv_file='./data/test.csv',\n    transform=val_transform,\n    max_frames=16,\n    gpu_preprocessing=True,    # å¯ç”¨GPUé¢„å¤„ç†\n    cache_frames=False        # ç¦ç”¨ç¼“å­˜ä»¥èŠ‚çœå†…å­˜\n)\nprint(\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œå·²ä¼˜åŒ–Kaggle T4ç¯å¢ƒé…ç½®\")\n\n# Kaggle T4 GPUæ‰¹æ¬¡å¤§å°ä¼˜åŒ–\nif torch.cuda.is_available():\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"GPUå†…å­˜: {gpu_memory:.1f} GB\")\n    \n    # ä¿å®ˆçš„æ‰¹æ¬¡å¤§å°è®¾ç½® - ç¡®ä¿ç¨³å®šæ€§å’Œå…¼å®¹æ€§\n    if gpu_memory >= 15:  # T4 GPU\n        batch_size = 8  # ä¿å®ˆè®¾ç½®ï¼Œç¡®ä¿ç¨³å®š\n    elif gpu_memory >= 8:\n        batch_size = 6\n    else:\n        batch_size = 4\nelse:\n    batch_size = 4\n\nprint(f\"ä½¿ç”¨æ‰¹æ¬¡å¤§å°: {batch_size} (Kaggle T4ä¼˜åŒ–ï¼Œç¨³å®šæ€§ä¼˜å…ˆ)\")\n\n# Kaggleç¯å¢ƒå¤šè¿›ç¨‹é…ç½® - ç®€åŒ–ç‰ˆæœ¬\nif IS_KAGGLE:\n    # Kaggleç¯å¢ƒï¼šä½¿ç”¨å•è¿›ç¨‹é¿å…åºåˆ—åŒ–é—®é¢˜\n    num_workers = 0\n    prefetch_factor = None\n    persistent_workers = False\n    print(\"ğŸ“ Kaggleç¯å¢ƒï¼šä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼\")\nelse:\n    # æœ¬åœ°ç¯å¢ƒï¼šä½¿ç”¨å°‘é‡worker\n    num_workers = 2\n    prefetch_factor = 2\n    persistent_workers = False\n    print(f\"ğŸ”¥ æœ¬åœ°ç¯å¢ƒï¼šä½¿ç”¨ {num_workers} workers\")\n\nprint(f\"æ•°æ®åŠ è½½é…ç½®: {num_workers} workers, é¢„å–å› å­: {prefetch_factor}\")\n\n# åˆ›å»ºæ•°æ®åŠ è½½å™¨ - Kaggle T4ä¼˜åŒ–ç‰ˆæœ¬\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    num_workers=num_workers,\n    pin_memory=False,  # GPUé¢„å¤„ç†ï¼Œæ— éœ€pin_memory\n    drop_last=True,\n    prefetch_factor=prefetch_factor if num_workers > 0 else None,\n    persistent_workers=persistent_workers if num_workers > 0 else False\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=num_workers,\n    pin_memory=False,\n    prefetch_factor=prefetch_factor if num_workers > 0 else None,\n    persistent_workers=persistent_workers if num_workers > 0 else False\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=num_workers,\n    pin_memory=False,\n    prefetch_factor=prefetch_factor if num_workers > 0 else None,\n    persistent_workers=persistent_workers if num_workers > 0 else False\n)\n\nprint(f\"\\nğŸ“Š æ•°æ®åŠ è½½å™¨ç»Ÿè®¡:\")\nprint(f\"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)} (æ‰¹æ¬¡å¤§å°: {batch_size})\")\nprint(f\"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}\")\nprint(f\"æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}\")\nprint(f\"æ•°æ®åŠ è½½workeræ•°: {num_workers}\")\nprint(\"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 11: æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒé…ç½®\n","metadata":{}},{"cell_type":"code","source":"# Cell 11: æ¨¡å‹åˆå§‹åŒ–å’Œè®­ç»ƒé…ç½® - Kaggle T4 GPUä¼˜åŒ–ç‰ˆæœ¬\n\nprint(\"ğŸ¤– åˆ›å»ºå’Œé…ç½®æ¨¡å‹...\")\n\n# åˆ›å»ºæ¨¡å‹ - é’ˆå¯¹Kaggle T4 GPUä¼˜åŒ–\nmodel = OptimizedDeepfakeDetector(\n    backbone='resnet50',\n    hidden_dim=512,      # é€‚ä¸­çš„éšè—å±‚ç»´åº¦\n    num_layers=2,        # å‡å°‘LSTMå±‚æ•°\n    dropout=0.3,         # é€‚ä¸­çš„dropout\n    use_attention=True\n).to(device)\n\n# å•GPUé…ç½®\nif torch.cuda.is_available():\n    torch.cuda.set_per_process_memory_fraction(0.9)\n    print(\"ä½¿ç”¨å•GPUè®­ç»ƒ\")\n\n# è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}\")\nprint(f\"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}\")\nprint(f\"æ¨¡å‹å¤§å°ä¼°è®¡: {total_params * 4 / 1024**2:.1f} MB\")\n\n# æŸå¤±å‡½æ•°\ncriterion = FocalLoss(alpha=0.25, gamma=2.0)\nprint(f\"æŸå¤±å‡½æ•°: FocalLoss\")\n\n# ä¼˜åŒ–å™¨\nbase_lr = 0.001\noptimizer = optim.AdamW(\n    model.parameters(), \n    lr=base_lr,\n    weight_decay=0.01\n)\nprint(f\"ä¼˜åŒ–å™¨: AdamW (lr={base_lr})\")\n\n# å­¦ä¹ ç‡è°ƒåº¦å™¨\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=base_lr * 5,\n    epochs=20,\n    steps_per_epoch=len(train_loader),\n    pct_start=0.3,\n    anneal_strategy='cos'\n)\nprint(f\"å­¦ä¹ ç‡è°ƒåº¦å™¨: OneCycleLR\")\n\n# æ—©åœæœºåˆ¶\nearly_stopping = EarlyStopping(patience=7, min_delta=0.001)\nprint(f\"æ—©åœæœºåˆ¶: patience=7, min_delta=0.001\")\n\n# è®­ç»ƒé…ç½® - ç»Ÿä¸€ä½¿ç”¨FP32æ•°æ®ç±»å‹\nscaler = None\nprint(\"æ•°æ®ç±»å‹: FP32 (ç¡®ä¿å…¼å®¹æ€§)\")\n\nnum_epochs = 20\nprint(f\"è®­ç»ƒè½®æ•°: {num_epochs}\")\n\n# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­\nprint(\"\\nğŸ” æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...\")\ntry:\n    model.eval()\n    with torch.no_grad():\n        sample_batch = next(iter(train_loader))\n        videos, labels = sample_batch\n        videos, labels = videos.to(device), labels.to(device)\n        \n        # å‰å‘ä¼ æ’­ï¼ˆç»Ÿä¸€ä½¿ç”¨FP32ï¼‰\n        outputs, attention_weights = model(videos)\n        loss = criterion(outputs, labels)\n        \n        print(f\"è¾“å…¥å½¢çŠ¶: {videos.shape}\")\n        print(f\"è¾“å…¥æ•°æ®ç±»å‹: {videos.dtype}\")\n        print(f\"è¾“å‡ºå½¢çŠ¶: {outputs.shape}\")\n        print(f\"æŸå¤±å€¼: {loss.item():.4f}\")\n        \n        # æ˜¾ç¤ºæ¦‚ç‡èŒƒå›´\n        probs = torch.sigmoid(outputs)\n        print(f\"æ¦‚ç‡èŒƒå›´: [{probs.min():.3f}, {probs.max():.3f}]\")\n        \n        print(\"âœ… æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ\")\nexcept Exception as e:\n    print(f\"âŒ æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}\")\n    raise e\n\nprint(\"âœ… æ¨¡å‹é…ç½®å®Œæˆï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒ\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 12: æ¨¡å‹è®­ç»ƒä¸»å¾ªç¯\n","metadata":{}},{"cell_type":"code","source":"# Cell 12: è®­ç»ƒå¾ªç¯ - Kaggle T4 GPUä¼˜åŒ–ç‰ˆæœ¬\n\n# ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨\nos.makedirs('./models', exist_ok=True)\n\nprint(\"ğŸš€ å¼€å§‹è®­ç»ƒ...\")\nprint(f\"ğŸ“Š è®­ç»ƒé…ç½®: {len(train_loader)} ä¸ªè®­ç»ƒæ‰¹æ¬¡, {len(val_loader)} ä¸ªéªŒè¯æ‰¹æ¬¡\")\nprint(f\"ğŸ¯ æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"ğŸ’¾ è®¾å¤‡: {device}\")\nprint(f\"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}\")\n\nif torch.cuda.is_available():\n    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n    torch.cuda.reset_peak_memory_stats()\n\n# è®­ç»ƒå†å²è®°å½•\ntrain_history = {\n    'train_loss': [],\n    'val_loss': [],\n    'train_acc': [],\n    'val_acc': [],\n    'train_auc': [],\n    'val_auc': []\n}\nbest_val_loss = float('inf')\nbest_val_acc = 0.0\nbest_val_auc = 0.0\nbest_model_state = None\n\n# è®­ç»ƒå¾ªç¯\nprint(\"\\nğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯...\")\nfor epoch in range(num_epochs):\n    epoch_start_time = time.time()\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    \n    # è®­ç»ƒé˜¶æ®µ\n    train_loss, train_acc, train_auc = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n    \n    # éªŒè¯é˜¶æ®µ\n    val_loss, val_acc, val_auc = validate_epoch(model, val_loader, criterion, device, scaler)\n    \n    # è®°å½•å†å²\n    train_history['train_loss'].append(train_loss)\n    train_history['train_acc'].append(train_acc)\n    train_history['train_auc'].append(train_auc)\n    train_history['val_loss'].append(val_loss)\n    train_history['val_acc'].append(val_acc)\n    train_history['val_auc'].append(val_auc)\n    \n    # å­¦ä¹ ç‡è°ƒåº¦\n    scheduler.step()\n    current_lr = optimizer.param_groups[0]['lr']\n    \n    # è®¡ç®—epochæ—¶é—´\n    epoch_time = time.time() - epoch_start_time\n    \n    # æ‰“å°ç»“æœ\n    print(f\"è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.2f}%, AUC={train_auc:.4f}\")\n    print(f\"éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.2f}%, AUC={val_auc:.4f}\")\n    print(f\"å­¦ä¹ ç‡: {current_lr:.2e}, ç”¨æ—¶: {epoch_time:.1f}s\")\n    \n    # ä¿å­˜æœ€ä½³æ¨¡å‹\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_val_acc = val_acc\n        best_val_auc = val_auc\n        best_model_state = model.state_dict().copy()\n        print(f\"ğŸ¯ æ–°çš„æœ€ä½³æ¨¡å‹! Loss: {best_val_loss:.4f}, Acc: {best_val_acc:.2f}%, AUC: {best_val_auc:.4f}\")\n        \n        # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°æ–‡ä»¶\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': best_model_state,\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_val_loss': best_val_loss,\n            'best_val_acc': best_val_acc,\n            'best_val_auc': best_val_auc,\n            'train_history': train_history\n        }, './models/best_model.pth')\n        print(\"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜\")\n    \n    # æ—©åœæ£€æŸ¥\n    if early_stopping(val_loss, model):\n        print(f\"\\nâ¹ï¸ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ\")\n        break\n    \n    # æ¸…ç†GPUç¼“å­˜\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\nprint(\"\\nâœ… è®­ç»ƒå®Œæˆ!\")\nprint(f\"ğŸ† æœ€ç»ˆæœ€ä½³æ€§èƒ½: Loss={best_val_loss:.4f}, Acc={best_val_acc:.2f}%, AUC={best_val_auc:.4f}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ’¾ å³°å€¼GPUå†…å­˜ä½¿ç”¨: {torch.cuda.max_memory_allocated() / 1024**3:.1f}GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 13: æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æ\n","metadata":{}},{"cell_type":"code","source":"# Cell 13: æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æ\n\nprint(\"ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...\")\nprint(\"=\" * 60)\n\n# åŠ è½½æœ€ä½³æ¨¡å‹\nprint(\"ğŸ”„ åŠ è½½æœ€ä½³æ¨¡å‹...\")\ntry:\n    checkpoint = torch.load('./models/best_model.pth', map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    best_epoch = checkpoint['epoch']\n    best_val_acc = checkpoint['best_val_acc']\n    best_val_auc = checkpoint['best_val_auc']\n    \n    print(f\"âœ… æˆåŠŸåŠ è½½ç¬¬ {best_epoch+1} è½®çš„æœ€ä½³æ¨¡å‹\")\n    print(f\"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%\")\n    print(f\"æœ€ä½³éªŒè¯AUC: {best_val_auc:.4f}\")\nexcept Exception as e:\n    print(f\"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}\")\n    print(\"ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œè¯„ä¼°\")\n\n# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹\nprint(\"\\nğŸ” åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...\")\neval_results = evaluate_model_optimized(model, test_loader, criterion, device)\n\n# è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡\nprint(\"\\nğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...\")\nmetrics = calculate_comprehensive_metrics(\n    eval_results['predictions'], \n    eval_results['targets'], \n    eval_results['scores']\n)\n\n# æ‰“å°è¯¦ç»†ç»“æœ\nprint(\"\\nğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ:\")\nprint(\"=\" * 50)\nprint(f\"æµ‹è¯•æŸå¤±: {eval_results['loss']:.4f}\")\nprint(f\"å‡†ç¡®ç‡: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\nprint(f\"å¹³è¡¡å‡†ç¡®ç‡: {metrics['balanced_accuracy']:.4f} ({metrics['balanced_accuracy']*100:.2f}%)\")\nprint(f\"ç²¾ç¡®ç‡: {metrics['precision']:.4f}\")\nprint(f\"å¬å›ç‡: {metrics['recall']:.4f}\")\nprint(f\"ç‰¹å¼‚æ€§: {metrics['specificity']:.4f}\")\nprint(f\"F1åˆ†æ•°: {metrics['f1']:.4f}\")\nprint(f\"AUC-ROC: {metrics['auc_roc']:.4f}\")\nprint(f\"AUC-PR: {metrics['auc_pr']:.4f}\")\nprint(f\"è´Ÿé¢„æµ‹å€¼: {metrics['npv']:.4f}\")\n\n# æ··æ·†çŸ©é˜µè¯¦ç»†ä¿¡æ¯\nprint(\"\\nğŸ” æ··æ·†çŸ©é˜µåˆ†æ:\")\nprint(f\"çœŸè´Ÿä¾‹ (TN): {metrics['tn']}\")\nprint(f\"å‡æ­£ä¾‹ (FP): {metrics['fp']}\")\nprint(f\"å‡è´Ÿä¾‹ (FN): {metrics['fn']}\")\nprint(f\"çœŸæ­£ä¾‹ (TP): {metrics['tp']}\")\n\n# æ€§èƒ½åˆ†æ\nprint(\"\\nâš¡ æ€§èƒ½åˆ†æ:\")\nprint(f\"å¹³å‡æ¨ç†æ—¶é—´: {eval_results['avg_inference_time']*1000:.2f} ms/batch\")\nprint(f\"æ€»æ¨ç†æ—¶é—´: {eval_results['total_inference_time']:.2f} ç§’\")\nprint(f\"æ¯ä¸ªæ ·æœ¬æ¨ç†æ—¶é—´: {eval_results['avg_inference_time']*1000/batch_size:.2f} ms\")\n\n# è®¡ç®—é¢å¤–æŒ‡æ ‡\ntotal_samples = len(eval_results['targets'])\nreal_samples = np.sum(eval_results['targets'] == 0)\nfake_samples = np.sum(eval_results['targets'] == 1)\nreal_accuracy = np.sum((eval_results['predictions'] == 0) & (eval_results['targets'] == 0)) / real_samples if real_samples > 0 else 0\nfake_accuracy = np.sum((eval_results['predictions'] == 1) & (eval_results['targets'] == 1)) / fake_samples if fake_samples > 0 else 0\n\nprint(\"\\nğŸ“‹ ç±»åˆ«ç‰¹å®šåˆ†æ:\")\nprint(f\"æ€»æ ·æœ¬æ•°: {total_samples}\")\nprint(f\"çœŸå®è§†é¢‘æ ·æœ¬: {real_samples} ({real_samples/total_samples*100:.1f}%)\")\nprint(f\"ä¼ªé€ è§†é¢‘æ ·æœ¬: {fake_samples} ({fake_samples/total_samples*100:.1f}%)\")\nprint(f\"çœŸå®è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {real_accuracy:.4f} ({real_accuracy*100:.2f}%)\")\nprint(f\"ä¼ªé€ è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {fake_accuracy:.4f} ({fake_accuracy*100:.2f}%)\")\n\n# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨\nprint(\"\\nğŸ“Š ç”Ÿæˆè¯„ä¼°å›¾è¡¨...\")\n\n# ç»˜åˆ¶å¢å¼ºæ··æ·†çŸ©é˜µ\nplot_enhanced_confusion_matrix(\n    metrics['confusion_matrix'], \n    './results/evaluation/confusion_matrix.png'\n)\n\n# ç»˜åˆ¶ROCå’ŒPRæ›²çº¿\nplot_roc_pr_curves(\n    eval_results['targets'], \n    eval_results['scores'], \n    './results/evaluation/roc_pr_curves.png'\n)\n\n# é¢„æµ‹åˆ†æ•°åˆ†å¸ƒå›¾\nplt.figure(figsize=(12, 5))\n\n# çœŸå®è§†é¢‘çš„é¢„æµ‹åˆ†æ•°åˆ†å¸ƒ\nplt.subplot(1, 2, 1)\nreal_scores = eval_results['scores'][eval_results['targets'] == 0]\nfake_scores = eval_results['scores'][eval_results['targets'] == 1]\n\nplt.hist(real_scores, bins=30, alpha=0.7, label='çœŸå®è§†é¢‘', color='blue', density=True)\nplt.hist(fake_scores, bins=30, alpha=0.7, label='ä¼ªé€ è§†é¢‘', color='red', density=True)\nplt.xlabel('é¢„æµ‹åˆ†æ•°')\nplt.ylabel('å¯†åº¦')\nplt.title('é¢„æµ‹åˆ†æ•°åˆ†å¸ƒ')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# é¢„æµ‹åˆ†æ•°ç®±çº¿å›¾\nplt.subplot(1, 2, 2)\nscores_data = [real_scores, fake_scores]\nlabels = ['çœŸå®è§†é¢‘', 'ä¼ªé€ è§†é¢‘']\nplt.boxplot(scores_data, labels=labels)\nplt.ylabel('é¢„æµ‹åˆ†æ•°')\nplt.title('é¢„æµ‹åˆ†æ•°ç®±çº¿å›¾')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('./results/evaluation/score_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"âœ… è¯„ä¼°å›¾è¡¨ç”Ÿæˆå®Œæˆ\")\nprint(\"=\" * 60)\nprint(\"ğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆï¼\")\nprint(\"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° ./results/evaluation/ ç›®å½•\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 14: ç»“æœä¿å­˜å’Œæ€»ç»“\n","metadata":{}},{"cell_type":"code","source":"# Cell 14: ç»“æœä¿å­˜å’Œæ€»ç»“\n\nprint(\"ğŸ’¾ ä¿å­˜å®éªŒç»“æœ...\")\nprint(\"=\" * 60)\n\n# å‡†å¤‡ä¿å­˜çš„ç»“æœæ•°æ®\nresults_summary = {\n    'experiment_info': {\n        'timestamp': datetime.now().isoformat(),\n        'model_architecture': 'OptimizedDeepfakeDetector',\n        'backbone': 'resnet50',\n        'total_epochs': len(train_history['train_loss']),\n        'early_stopping': True\n    },\n    'dataset_info': {\n        'train_samples': len(train_dataset),\n        'val_samples': len(val_dataset),\n        'test_samples': len(test_dataset),\n        'batch_size': batch_size\n    },\n    'training_config': {\n        'optimizer': 'AdamW',\n        'learning_rate': 1e-4,\n        'weight_decay': 1e-4,\n        'loss_function': 'FocalLoss',\n        'scheduler': 'OneCycleLR',\n        'early_stopping_patience': 7\n    },\n    'final_metrics': {\n        'test_loss': float(eval_results['loss']),\n        'accuracy': float(metrics['accuracy']),\n        'precision': float(metrics['precision']),\n        'recall': float(metrics['recall']),\n        'f1_score': float(metrics['f1']),\n        'auc_roc': float(metrics['auc_roc'])\n    },\n    'confusion_matrix': {\n        'tn': int(metrics['tn']),\n        'fp': int(metrics['fp']),\n        'fn': int(metrics['fn']),\n        'tp': int(metrics['tp'])\n    },\n    'training_history': {\n        'train_loss': [float(x) for x in train_history['train_loss']],\n        'train_acc': [float(x) for x in train_history['train_acc']],\n        'train_auc': [float(x) for x in train_history['train_auc']],\n        'val_loss': [float(x) for x in train_history['val_loss']],\n        'val_acc': [float(x) for x in train_history['val_acc']],\n        'val_auc': [float(x) for x in train_history['val_auc']]\n    },\n    'class_specific_metrics': {\n        'real_video_accuracy': float(real_accuracy),\n        'fake_video_accuracy': float(fake_accuracy),\n        'real_samples_count': int(real_samples),\n        'fake_samples_count': int(fake_samples)\n    }\n}\n\n# ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶\nresults_file = './results/experiment_results.json'\nwith open(results_file, 'w', encoding='utf-8') as f:\n    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n\nprint(f\"âœ… å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}\")\n\n# ä¿å­˜è®­ç»ƒå†å²åˆ°CSV\nhistory_df = pd.DataFrame(train_history)\nhistory_df.to_csv('./results/training_history.csv', index=False)\nprint(\"âœ… è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: ./results/training_history.csv\")\n\n# ä¿å­˜é¢„æµ‹ç»“æœ\npredictions_df = pd.DataFrame({\n    'true_label': eval_results['targets'],\n    'predicted_label': eval_results['predictions'],\n    'prediction_score': eval_results['scores']\n})\npredictions_df.to_csv('./results/test_predictions.csv', index=False)\nprint(\"âœ… æµ‹è¯•é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: ./results/test_predictions.csv\")\n\n# ç”Ÿæˆå®éªŒæŠ¥å‘Š\nprint(\"\\nğŸ“‹ ç”Ÿæˆå®éªŒæŠ¥å‘Š...\")\nreport = f\"\"\"\næ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹å®éªŒæŠ¥å‘Š\n{'='*50}\n\nå®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\næ¨¡å‹æ¶æ„: OptimizedDeepfakeDetector (ResNet50 + LSTM + Attention)\n\næ•°æ®é›†ä¿¡æ¯:\n- è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}\n- éªŒè¯æ ·æœ¬: {len(val_dataset):,}\n- æµ‹è¯•æ ·æœ¬: {len(test_dataset):,}\n- æ‰¹æ¬¡å¤§å°: {batch_size}\n\nè®­ç»ƒé…ç½®:\n- ä¼˜åŒ–å™¨: AdamW (lr=1e-4, weight_decay=1e-4)\n- æŸå¤±å‡½æ•°: Focal Loss\n- å­¦ä¹ ç‡è°ƒåº¦: OneCycleLR\n- æ—©åœæœºåˆ¶: patience=7\n\næœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:\n- å‡†ç¡®ç‡: {metrics['accuracy']*100:.2f}%\n- ç²¾ç¡®ç‡: {metrics['precision']:.4f}\n- å¬å›ç‡: {metrics['recall']:.4f}\n- F1åˆ†æ•°: {metrics['f1']:.4f}\n- AUC-ROC: {metrics['auc_roc']:.4f}\n\næ··æ·†çŸ©é˜µ:\n- çœŸè´Ÿä¾‹ (TN): {metrics['tn']}\n- å‡æ­£ä¾‹ (FP): {metrics['fp']}\n- å‡è´Ÿä¾‹ (FN): {metrics['fn']}\n- çœŸæ­£ä¾‹ (TP): {metrics['tp']}\n\nç±»åˆ«ç‰¹å®šæ€§èƒ½:\n- çœŸå®è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {real_accuracy*100:.2f}%\n- ä¼ªé€ è§†é¢‘æ£€æµ‹å‡†ç¡®ç‡: {fake_accuracy*100:.2f}%\n\nè®­ç»ƒæ€»ç»“:\n- è®­ç»ƒè½®æ•°: {len(train_history['train_loss'])}\n- æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(train_history['val_acc']):.2f}%\n- æœ€ä½³éªŒè¯AUC: {max(train_history['val_auc']):.4f}\n\næ–‡ä»¶è¾“å‡º:\n- æ¨¡å‹æƒé‡: ./models/best_model.pth\n- å®éªŒç»“æœ: ./results/experiment_results.json\n- è®­ç»ƒå†å²: ./results/training_history.csv\n- é¢„æµ‹ç»“æœ: ./results/test_predictions.csv\n\n{'='*50}\nå®éªŒå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\"\"\"\n\n# ä¿å­˜æŠ¥å‘Š\nwith open('./results/experiment_report.txt', 'w', encoding='utf-8') as f:\n    f.write(report)\n\nprint(\"âœ… å®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: ./results/experiment_report.txt\")\n\n# æ‰“å°æœ€ç»ˆæ€»ç»“\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ‰ æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼\")\nprint(\"=\"*60)\nprint(f\"ğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {metrics['accuracy']*100:.2f}%\")\nprint(f\"ğŸ“Š AUC-ROCåˆ†æ•°: {metrics['auc_roc']:.4f}\")\nprint(f\"ğŸ“Š F1åˆ†æ•°: {metrics['f1']:.4f}\")\nprint(\"\\nğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° ./results/ ç›®å½•\")\nprint(\"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° ./models/best_model.pth\")\nprint(\"\\nâœ¨ å®éªŒæˆåŠŸå®Œæˆï¼\")\nprint(\"=\"*60)\n\n# æ˜¾ç¤ºæ–‡ä»¶ç»“æ„\nprint(\"\\nğŸ“‚ ç”Ÿæˆçš„æ–‡ä»¶ç»“æ„:\")\nprint(\"\"\"\n./models/\n  â””â”€â”€ best_model.pth\n./results/\n  â”œâ”€â”€ experiment_results.json\n  â”œâ”€â”€ experiment_report.txt\n  â”œâ”€â”€ training_history.csv\n  â””â”€â”€ test_predictions.csv\n\"\"\")\n\nprint(\"\\nğŸš€ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†:\")\nprint(\"\"\"\n# åŠ è½½æ¨¡å‹\nmodel = OptimizedDeepfakeDetector(...)\ncheckpoint = torch.load('./models/best_model.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\"\"\")\n\nprint(\"\\nâœ… Kaggle T4 GPUä¼˜åŒ–ç‰ˆæœ¬ - è®­ç»ƒå®Œæˆï¼\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}