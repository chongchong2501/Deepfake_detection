#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹æ¨ç†è„šæœ¬ - æœ¬åœ°RTX4070ç‰ˆæœ¬

ä½¿ç”¨æ–¹æ³•:
    python inference.py --model MODEL_PATH --video VIDEO_PATH [--output OUTPUT_PATH]
    python inference.py --model MODEL_PATH --video-dir VIDEO_DIR [--output OUTPUT_DIR]
"""

import os
import sys
import argparse
import json
from pathlib import Path
import torch
import numpy as np
import pandas as pd
from datetime import datetime
import time
import cv2
from tqdm import tqdm

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / 'src'))

from config import config
from model import create_model
from data_processing import VideoProcessor
from utils import get_transforms, format_time

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹æ¨ç†')
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    
    # è¾“å…¥é€‰é¡¹ï¼ˆäº’æ–¥ï¼‰
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('--video', type=str, help='å•ä¸ªè§†é¢‘æ–‡ä»¶è·¯å¾„')
    input_group.add_argument('--video-dir', type=str, help='è§†é¢‘ç›®å½•è·¯å¾„')
    
    parser.add_argument('--output', type=str, help='è¾“å‡ºæ–‡ä»¶/ç›®å½•è·¯å¾„')
    parser.add_argument('--threshold', type=float, default=0.5, help='åˆ†ç±»é˜ˆå€¼ (é»˜è®¤: 0.5)')
    parser.add_argument('--batch-size', type=int, default=1, help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1)')
    parser.add_argument('--no-cuda', action='store_true', help='ç¦ç”¨CUDA')
    parser.add_argument('--save-frames', action='store_true', help='ä¿å­˜æå–çš„å¸§')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    return parser.parse_args()

def load_model_for_inference(model_path, device):
    """åŠ è½½æ¨¡å‹ç”¨äºæ¨ç†"""
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model()
    model = model.to(device)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(model_path, map_location=device)
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        if 'best_val_acc' in checkpoint:
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (éªŒè¯å‡†ç¡®ç‡: {checkpoint['best_val_acc']:.2f}%)")
        else:
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        # ç›´æ¥åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸
        model.load_state_dict(checkpoint)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    model.eval()
    return model

def preprocess_video(video_path, processor, transforms):
    """é¢„å¤„ç†å•ä¸ªè§†é¢‘"""
    try:
        # æå–å¸§
        frames = processor.extract_frames_gpu_accelerated(video_path)
        
        if frames is None or len(frames) == 0:
            return None, "æ— æ³•æå–å¸§"
        
        # åº”ç”¨å˜æ¢
        processed_frames = []
        for frame in frames:
            if transforms:
                frame = transforms(frame)
            processed_frames.append(frame)
        
        # è½¬æ¢ä¸ºtensor
        video_tensor = torch.stack(processed_frames)
        
        return video_tensor, None
    
    except Exception as e:
        return None, str(e)

def predict_single_video(model, video_tensor, device, threshold=0.5):
    """é¢„æµ‹å•ä¸ªè§†é¢‘"""
    model.eval()
    
    with torch.no_grad():
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        video_tensor = video_tensor.unsqueeze(0).to(device)
        
        # æ¨ç†
        start_time = time.time()
        output, attention_weights = model(video_tensor)
        inference_time = time.time() - start_time
        
        # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
        prob = torch.sigmoid(output).item()
        prediction = prob > threshold
        
        return {
            'probability': prob,
            'prediction': prediction,
            'label': 'FAKE' if prediction else 'REAL',
            'confidence': prob if prediction else (1 - prob),
            'inference_time': inference_time,
            'attention_weights': attention_weights.cpu().numpy() if attention_weights is not None else None
        }

def process_single_video(args, model, processor, transforms, device):
    """å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶"""
    video_path = Path(args.video)
    
    if not video_path.exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return 1
    
    print(f"\nğŸ¬ å¤„ç†è§†é¢‘: {video_path.name}")
    
    # é¢„å¤„ç†è§†é¢‘
    print("ğŸ“Š æå–å’Œé¢„å¤„ç†å¸§...")
    video_tensor, error = preprocess_video(video_path, processor, transforms)
    
    if video_tensor is None:
        print(f"âŒ è§†é¢‘é¢„å¤„ç†å¤±è´¥: {error}")
        return 1
    
    print(f"âœ… æˆåŠŸæå– {video_tensor.shape[0]} å¸§")
    
    # æ¨ç†
    print("ğŸ” è¿›è¡Œæ¨ç†...")
    result = predict_single_video(model, video_tensor, device, args.threshold)
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*50)
    print("ğŸ¯ æ¨ç†ç»“æœ")
    print("="*50)
    print(f"è§†é¢‘æ–‡ä»¶: {video_path.name}")
    print(f"é¢„æµ‹æ ‡ç­¾: {result['label']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.4f} ({result['confidence']*100:.2f}%)")
    print(f"åŸå§‹æ¦‚ç‡: {result['probability']:.4f}")
    print(f"æ¨ç†æ—¶é—´: {result['inference_time']*1000:.2f} ms")
    print("="*50)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        
        # å‡†å¤‡ç»“æœæ•°æ®
        result_data = {
            'video_path': str(video_path),
            'video_name': video_path.name,
            'prediction': result['label'],
            'probability': result['probability'],
            'confidence': result['confidence'],
            'threshold': args.threshold,
            'inference_time_ms': result['inference_time'] * 1000,
            'timestamp': datetime.now().isoformat(),
            'model_path': args.model
        }
        
        # ä¿å­˜ä¸ºJSON
        if output_path.suffix.lower() == '.json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
        else:
            # ä¿å­˜ä¸ºCSV
            df = pd.DataFrame([result_data])
            df.to_csv(output_path, index=False)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return 0

def process_video_directory(args, model, processor, transforms, device):
    """å¤„ç†è§†é¢‘ç›®å½•"""
    video_dir = Path(args.video_dir)
    
    if not video_dir.exists():
        print(f"âŒ è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {video_dir}")
        return 1
    
    # æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
    video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.webm'}
    video_files = []
    
    for ext in video_extensions:
        video_files.extend(video_dir.glob(f'*{ext}'))
        video_files.extend(video_dir.glob(f'*{ext.upper()}'))
    
    if not video_files:
        print(f"âŒ åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶: {video_dir}")
        return 1
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # å¤„ç†æ‰€æœ‰è§†é¢‘
    results = []
    
    for video_path in tqdm(video_files, desc="å¤„ç†è§†é¢‘"):
        if args.verbose:
            print(f"\nğŸ¬ å¤„ç†: {video_path.name}")
        
        # é¢„å¤„ç†è§†é¢‘
        video_tensor, error = preprocess_video(video_path, processor, transforms)
        
        if video_tensor is None:
            if args.verbose:
                print(f"âŒ é¢„å¤„ç†å¤±è´¥: {error}")
            
            result_data = {
                'video_path': str(video_path),
                'video_name': video_path.name,
                'prediction': 'ERROR',
                'probability': None,
                'confidence': None,
                'error': error,
                'threshold': args.threshold,
                'inference_time_ms': None,
                'timestamp': datetime.now().isoformat(),
                'model_path': args.model
            }
        else:
            # æ¨ç†
            result = predict_single_video(model, video_tensor, device, args.threshold)
            
            result_data = {
                'video_path': str(video_path),
                'video_name': video_path.name,
                'prediction': result['label'],
                'probability': result['probability'],
                'confidence': result['confidence'],
                'threshold': args.threshold,
                'inference_time_ms': result['inference_time'] * 1000,
                'timestamp': datetime.now().isoformat(),
                'model_path': args.model
            }
            
            if args.verbose:
                print(f"âœ… {result['label']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
        
        results.append(result_data)
    
    # ç»Ÿè®¡ç»“æœ
    successful_results = [r for r in results if r['prediction'] != 'ERROR']
    real_count = sum(1 for r in successful_results if r['prediction'] == 'REAL')
    fake_count = sum(1 for r in successful_results if r['prediction'] == 'FAKE')
    error_count = len(results) - len(successful_results)
    
    print("\n" + "="*60)
    print("ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœç»Ÿè®¡")
    print("="*60)
    print(f"æ€»è§†é¢‘æ•°: {len(results)}")
    print(f"æˆåŠŸå¤„ç†: {len(successful_results)}")
    print(f"å¤„ç†å¤±è´¥: {error_count}")
    print(f"é¢„æµ‹ä¸ºçœŸå®: {real_count}")
    print(f"é¢„æµ‹ä¸ºä¼ªé€ : {fake_count}")
    
    if successful_results:
        avg_confidence = np.mean([r['confidence'] for r in successful_results])
        avg_inference_time = np.mean([r['inference_time_ms'] for r in successful_results])
        print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.2f} ms")
    
    print("="*60)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºCSV
        df = pd.DataFrame(results)
        df.to_csv(output_path, index=False)
        print(f"\nğŸ’¾ æ‰¹é‡ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return 0

def main():
    """ä¸»æ¨ç†å‡½æ•°"""
    args = parse_args()
    
    # æ›´æ–°é…ç½®
    if args.no_cuda:
        config.USE_CUDA = False
    
    # è®¾ç½®ç¯å¢ƒ
    config.setup_environment()
    device = config.get_device()
    
    print("="*60)
    print("ğŸ” æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹æ¨ç†")
    print("="*60)
    print(f"æ¨¡å‹æ–‡ä»¶: {args.model}")
    print(f"è®¾å¤‡: {device}")
    print(f"åˆ†ç±»é˜ˆå€¼: {args.threshold}")
    
    if args.video:
        print(f"è¾“å…¥è§†é¢‘: {args.video}")
    else:
        print(f"è¾“å…¥ç›®å½•: {args.video_dir}")
    
    if args.output:
        print(f"è¾“å‡ºè·¯å¾„: {args.output}")
    
    print("="*60)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = Path(args.model)
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return 1
    
    # åŠ è½½æ¨¡å‹
    try:
        model = load_model_for_inference(model_path, device)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # åˆ›å»ºè§†é¢‘å¤„ç†å™¨å’Œå˜æ¢
    processor = VideoProcessor()
    transforms = get_transforms(mode='test')
    
    # æ ¹æ®è¾“å…¥ç±»å‹å¤„ç†
    try:
        if args.video:
            return process_single_video(args, model, processor, transforms, device)
        else:
            return process_video_directory(args, model, processor, transforms, device)
    
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¨ç†è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ æ¨ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)