#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# ç¬¬9æ®µï¼šå®Œæ•´çš„æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æ
# 
# Kaggle Deepfake Detection Module
# This module can be run as a single cell in Kaggle environment
# 
# Usage:
# 1. Create a new code cell in Kaggle
# 2. Copy the entire content of this file to the cell
# 3. Run the cell

# =============================================================================
# Kaggle å…¼å®¹çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è¯„ä¼°è„šæœ¬
# =============================================================================

import os
import json
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report, 
    roc_curve, auc, average_precision_score, precision_recall_curve
)
import cv2
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å®šä¹‰æ•°æ®è½¬æ¢
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# æ•°æ®é›†ç±»å®šä¹‰
class DeepfakeVideoDataset(Dataset):
    """æ·±åº¦ä¼ªé€ æ£€æµ‹æ•°æ®é›†ï¼ˆKaggleå…¼å®¹ç‰ˆæœ¬ï¼‰"""
    def __init__(self, csv_file, transform=None, max_frames=30, mode='train'):
        self.data = pd.read_csv(csv_file)
        self.transform = transform
        self.max_frames = max_frames
        self.mode = mode
        
        print(f"æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {len(self.data)} ä¸ªæ ·æœ¬ ({mode} æ¨¡å¼)")
        if 'label' in self.data.columns:
            print(f"çœŸå®è§†é¢‘: {len(self.data[self.data['label']==0])} ä¸ª")
            print(f"ä¼ªé€ è§†é¢‘: {len(self.data[self.data['label']==1])} ä¸ª")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        
        # å¤„ç†å¸§è·¯å¾„
        if 'frames' in row:
            try:
                frame_paths = eval(row['frames'])  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨
                if isinstance(frame_paths, str):
                    frame_paths = [frame_paths]
            except:
                frame_paths = [row['frames']]
        elif 'frame_path' in row:
            frame_paths = [row['frame_path']]
        else:
            frame_paths = []
        
        label = float(row['label']) if 'label' in row else 0.0
        
        # åŠ è½½å¸§
        frames = []
        for i, frame_path in enumerate(frame_paths[:self.max_frames]):
            try:
                if os.path.exists(frame_path):
                    frame = cv2.imread(frame_path)
                    if frame is not None:
                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        frame = cv2.resize(frame, (224, 224))
                    else:
                        frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
                else:
                    frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
            except:
                frame = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
            
            if self.transform:
                frame = self.transform(frame)
            else:
                frame = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1) / 255.0
            
            frames.append(frame)
        
        # å¦‚æœå¸§æ•°ä¸è¶³ï¼Œç”¨é›¶å¡«å……æˆ–é‡å¤æœ€åä¸€å¸§
        while len(frames) < self.max_frames:
            if frames:
                frames.append(frames[-1].clone())
            else:
                if self.transform:
                    dummy_frame = np.zeros((224, 224, 3), dtype=np.uint8)
                    frames.append(self.transform(dummy_frame))
                else:
                    frames.append(torch.zeros(3, 224, 224))
        
        # å°†å¸§å †å æˆå¼ é‡
        frames_tensor = torch.stack(frames[:self.max_frames])
        
        return frames_tensor, torch.tensor(label, dtype=torch.float32)

# æ¨¡å‹å®šä¹‰
class OptimizedDeepfakeDetector(nn.Module):
    """ä¼˜åŒ–çš„æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹"""
    def __init__(self, backbone='resnet50', hidden_dim=512, num_layers=2, dropout=0.3, use_attention=True):
        super().__init__()
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNetä½œä¸ºç‰¹å¾æå–å™¨
        if backbone == 'resnet50':
            self.backbone = models.resnet50(pretrained=True)
            self.backbone.fc = nn.Identity()  # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            feature_dim = 2048
        else:
            # ç®€åŒ–çš„CNNç‰¹å¾æå–å™¨
            self.backbone = nn.Sequential(
                nn.Conv2d(3, 64, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, 3, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(128, 256, 3, padding=1),
                nn.ReLU(),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten()
            )
            feature_dim = 256
        
        # æ—¶åºå¤„ç†
        self.lstm = nn.LSTM(feature_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.use_attention = use_attention
        if use_attention:
            self.attention = nn.MultiheadAttention(hidden_dim, 8, dropout=dropout, batch_first=True)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        batch_size, seq_len, c, h, w = x.shape
        
        # æå–æ¯å¸§ç‰¹å¾
        x = x.view(-1, c, h, w)
        features = self.backbone(x)
        features = features.view(batch_size, seq_len, -1)
        
        # LSTMå¤„ç†æ—¶åºä¿¡æ¯
        lstm_out, _ = self.lstm(features)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        if self.use_attention:
            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
            pooled = torch.mean(attn_out, dim=1)
        else:
            pooled = torch.mean(lstm_out, dim=1)
        
        # åˆ†ç±»
        output = self.classifier(pooled)
        
        return output.squeeze(-1)

# ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°å‡½æ•°
def evaluate_model_optimized(model, test_loader, criterion, device):
    """ä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°å‡½æ•°"""
    model.eval()
    running_loss = 0.0
    predictions = []
    targets = []
    scores = []
    
    total_inference_time = 0
    
    with torch.no_grad():
        for batch_idx, (inputs, labels) in enumerate(test_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            
            # è®°å½•æ¨ç†æ—¶é—´
            import time
            start_time = time.time()
            
            # å‰å‘ä¼ æ’­
            outputs = model(inputs)
            
            batch_time = time.time() - start_time
            total_inference_time += batch_time
            
            if len(outputs.shape) > 1:
                outputs = outputs.squeeze()
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(0)
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            batch_preds = (outputs > 0.5).float().cpu().numpy()
            batch_targets = labels.cpu().numpy()
            batch_scores = outputs.cpu().numpy()
            
            predictions.extend(batch_preds)
            targets.extend(batch_targets)
            scores.extend(batch_scores)
    
    avg_loss = running_loss / len(test_loader.dataset)
    avg_inference_time = total_inference_time / len(test_loader.dataset)
    
    return {
        'loss': avg_loss,
        'predictions': np.array(predictions),
        'targets': np.array(targets),
        'scores': np.array(scores),
        'total_inference_time': total_inference_time,
        'avg_inference_time': avg_inference_time
    }

# è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡
def calculate_comprehensive_metrics(predictions, targets, scores):
    """è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(targets, predictions)
    precision = precision_score(targets, predictions, zero_division=0)
    recall = recall_score(targets, predictions, zero_division=0)
    f1 = f1_score(targets, predictions, zero_division=0)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(targets, predictions)
    tn, fp, fn, tp = cm.ravel()
    
    # é¢å¤–æŒ‡æ ‡
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # è´Ÿé¢„æµ‹å€¼
    balanced_accuracy = (recall + specificity) / 2
    
    # AUCæŒ‡æ ‡
    try:
        auc_roc = roc_auc_score(targets, scores)
        auc_pr = average_precision_score(targets, scores)
    except:
        auc_roc = 0.0
        auc_pr = 0.0
    
    return {
        'accuracy': accuracy,
        'balanced_accuracy': balanced_accuracy,
        'precision': precision,
        'recall': recall,
        'specificity': specificity,
        'f1': f1,
        'npv': npv,
        'auc_roc': auc_roc,
        'auc_pr': auc_pr,
        'confusion_matrix': cm,
        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp
    }

# å¢å¼ºçš„æ··æ·†çŸ©é˜µå¯è§†åŒ–
def plot_enhanced_confusion_matrix(cm, save_path):
    """ç»˜åˆ¶å¢å¼ºçš„æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(10, 8))
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # åˆ›å»ºæ ‡ç­¾
    labels = np.array([[
        f'{cm[i,j]}\n({cm_percent[i,j]:.1f}%)' 
        for j in range(cm.shape[1])
    ] for i in range(cm.shape[0])])
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', 
                xticklabels=['çœŸå®', 'ä¼ªé€ '], 
                yticklabels=['çœŸå®', 'ä¼ªé€ '],
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'})
    
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.title('å¢å¼ºæ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

# ROCå’ŒPRæ›²çº¿
def plot_roc_pr_curves(targets, scores, save_path):
    """ç»˜åˆ¶ROCå’ŒPRæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(targets, scores)
    roc_auc = auc(fpr, tpr)
    
    ax1.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.4f})')
    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    ax1.set_xlim([0.0, 1.0])
    ax1.set_ylim([0.0, 1.05])
    ax1.set_xlabel('å‡é˜³æ€§ç‡')
    ax1.set_ylabel('çœŸé˜³æ€§ç‡')
    ax1.set_title('ROCæ›²çº¿')
    ax1.legend(loc='lower right')
    ax1.grid(True, alpha=0.3)
    
    # PRæ›²çº¿
    precision_curve, recall_curve, _ = precision_recall_curve(targets, scores)
    pr_auc = auc(recall_curve, precision_curve)
    
    ax2.plot(recall_curve, precision_curve, color='darkgreen', lw=2,
             label=f'PRæ›²çº¿ (AUC = {pr_auc:.4f})')
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('å¬å›ç‡')
    ax2.set_ylabel('ç²¾ç¡®ç‡')
    ax2.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿')
    ax2.legend(loc='lower left')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

# ä¸»è¯„ä¼°æµç¨‹
print("ğŸ”„ åˆ›å»ºä¼˜åŒ–çš„æµ‹è¯•æ•°æ®åŠ è½½å™¨...")

# ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡å¤§å°ä»¥æé«˜æ•ˆç‡
optimized_batch_size = 16 if torch.cuda.is_available() else 8

test_dataset = DeepfakeVideoDataset(
    csv_file='./data/val.csv',
    transform=transform,
    max_frames=30
)

test_loader = DataLoader(
    test_dataset,
    batch_size=optimized_batch_size,
    shuffle=False,
    num_workers=2,  # å‡å°‘workeræ•°é‡ä»¥é€‚åº”Kaggleç¯å¢ƒ
    pin_memory=True
)

print(f"ğŸ“Š æµ‹è¯•é›†å¤§å°: {len(test_dataset)} ä¸ªæ ·æœ¬")
print(f"ğŸ”§ æ‰¹æ¬¡å¤§å°: {optimized_batch_size}")
print(f"ğŸ”§ æ‰¹æ¬¡æ•°é‡: {len(test_loader)}")

# åŠ è½½æœ€ä½³æ¨¡å‹
print("\nğŸ¤– åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")

try:
    checkpoint = torch.load('./models/best_model.pth', 
                          map_location=device, 
                          weights_only=False)
except TypeError:
    checkpoint = torch.load('./models/best_model.pth', map_location=device)

# åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
model = OptimizedDeepfakeDetector(
    backbone='resnet50',
    hidden_dim=512,
    num_layers=2,
    dropout=0.3,
    use_attention=True
).to(device)

model.load_state_dict(checkpoint['model_state_dict'])
print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
print(f"ğŸ“ˆ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint.get('best_val_acc', 'N/A')}")
print(f"ğŸ”„ è®­ç»ƒè½®æ•°: {checkpoint.get('epoch', 'N/A')}")

# è®°å½•è¯„ä¼°å¼€å§‹æ—¶é—´
eval_start_time = datetime.now()
print(f"\nâ° è¯„ä¼°å¼€å§‹æ—¶é—´: {eval_start_time.strftime('%Y-%m-%d %H:%M:%S')}")

# æ‰§è¡Œä¼˜åŒ–çš„æ¨¡å‹è¯„ä¼°
print("\nğŸš€ å¼€å§‹æ‰§è¡Œæ¨¡å‹è¯„ä¼°...")
print("=" * 60)

criterion = nn.BCELoss()

# æ‰§è¡Œè¯„ä¼°
eval_results = evaluate_model_optimized(
    model=model,
    test_loader=test_loader,
    criterion=criterion,
    device=device
)

# è®¡ç®—å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡
metrics = calculate_comprehensive_metrics(
    predictions=eval_results['predictions'],
    targets=eval_results['targets'],
    scores=eval_results['scores']
)

# æ€§èƒ½åˆ†æ
eval_end_time = datetime.now()
total_eval_time = (eval_end_time - eval_start_time).total_seconds()

print(f"\nâ±ï¸ è¯„ä¼°æ€§èƒ½åˆ†æ:")
print(f"æ€»è¯„ä¼°æ—¶é—´: {total_eval_time:.2f} ç§’")
print(f"å¹³å‡æ¯æ ·æœ¬æ¨ç†æ—¶é—´: {eval_results['avg_inference_time']*1000:.2f} ms")
print(f"æ¨ç†ååé‡: {len(test_dataset)/eval_results['total_inference_time']:.1f} æ ·æœ¬/ç§’")

# æ‰“å°è¯¦ç»†è¯„ä¼°ç»“æœ
print(f"\nğŸ“Š è¯¦ç»†è¯„ä¼°ç»“æœ:")
print(f"æµ‹è¯•æŸå¤±: {eval_results['loss']:.4f}")
print(f"å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
print(f"å¹³è¡¡å‡†ç¡®ç‡: {metrics['balanced_accuracy']:.4f}")
print(f"ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
print(f"å¬å›ç‡: {metrics['recall']:.4f}")
print(f"ç‰¹å¼‚æ€§: {metrics['specificity']:.4f}")
print(f"F1åˆ†æ•°: {metrics['f1']:.4f}")
print(f"AUC-ROC: {metrics['auc_roc']:.4f}")
print(f"AUC-PR: {metrics['auc_pr']:.4f}")
print(f"è´Ÿé¢„æµ‹å€¼: {metrics['npv']:.4f}")

# åˆ›å»ºç»“æœç›®å½•
os.makedirs('./results/evaluation', exist_ok=True)

# ç»˜åˆ¶å¢å¼ºçš„å¯è§†åŒ–å›¾è¡¨
print("\nğŸ“Š ç”Ÿæˆè¯„ä¼°å›¾è¡¨...")

# 1. å¢å¼ºçš„æ··æ·†çŸ©é˜µ
plot_enhanced_confusion_matrix(
    cm=metrics['confusion_matrix'],
    save_path='./results/evaluation/enhanced_confusion_matrix.png'
)

# 2. ROCå’ŒPRæ›²çº¿
plot_roc_pr_curves(
    targets=eval_results['targets'],
    scores=eval_results['scores'],
    save_path='./results/evaluation/roc_pr_curves.png'
)

# ä¿å­˜è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
detailed_report = {
    'evaluation_info': {
        'timestamp': eval_start_time.isoformat(),
        'model_path': './models/best_model.pth',
        'test_dataset_size': len(test_dataset),
        'batch_size': optimized_batch_size
    },
    'performance_metrics': {
        'test_loss': float(eval_results['loss']),
        'accuracy': float(metrics['accuracy']),
        'balanced_accuracy': float(metrics['balanced_accuracy']),
        'precision': float(metrics['precision']),
        'recall': float(metrics['recall']),
        'specificity': float(metrics['specificity']),
        'f1_score': float(metrics['f1']),
        'auc_roc': float(metrics['auc_roc']),
        'auc_pr': float(metrics['auc_pr']),
        'npv': float(metrics['npv'])
    },
    'confusion_matrix': {
        'true_negative': int(metrics['tn']),
        'false_positive': int(metrics['fp']),
        'false_negative': int(metrics['fn']),
        'true_positive': int(metrics['tp'])
    },
    'performance_analysis': {
        'total_evaluation_time_seconds': total_eval_time,
        'average_inference_time_ms': eval_results['avg_inference_time'] * 1000,
        'throughput_samples_per_second': len(test_dataset) / eval_results['total_inference_time']
    }
}

# ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
with open('./results/evaluation/detailed_evaluation_report.json', 'w', encoding='utf-8') as f:
    json.dump(detailed_report, f, indent=2, ensure_ascii=False)

# ä¿å­˜CSVæ ¼å¼çš„ç®€è¦æŠ¥å‘Š
summary_df = pd.DataFrame([{
    'è¯„ä¼°æ—¶é—´': eval_start_time.strftime('%Y-%m-%d %H:%M:%S'),
    'æµ‹è¯•æŸå¤±': f"{eval_results['loss']:.4f}",
    'å‡†ç¡®ç‡': f"{metrics['accuracy']:.4f}",
    'å¹³è¡¡å‡†ç¡®ç‡': f"{metrics['balanced_accuracy']:.4f}",
    'ç²¾ç¡®ç‡': f"{metrics['precision']:.4f}",
    'å¬å›ç‡': f"{metrics['recall']:.4f}",
    'F1åˆ†æ•°': f"{metrics['f1']:.4f}",
    'AUC-ROC': f"{metrics['auc_roc']:.4f}",
    'AUC-PR': f"{metrics['auc_pr']:.4f}",
    'æ¨ç†æ—¶é—´(ms)': f"{eval_results['avg_inference_time']*1000:.2f}",
    'ååé‡(æ ·æœ¬/ç§’)': f"{len(test_dataset)/eval_results['total_inference_time']:.1f}"
}])

summary_df.to_csv('./results/evaluation/evaluation_summary.csv', index=False, encoding='utf-8')

print("\nğŸ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°:")
print("  ğŸ“Š ./results/evaluation/enhanced_confusion_matrix.png")
print("  ğŸ“ˆ ./results/evaluation/roc_pr_curves.png")
print("  ğŸ“‹ ./results/evaluation/detailed_evaluation_report.json")
print("  ğŸ“Š ./results/evaluation/evaluation_summary.csv")

print("\nğŸ‰ æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
print("=" * 60)