#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è¯„ä¼°è„šæœ¬ - æœ¬åœ°RTX4070ç‰ˆæœ¬

ä½¿ç”¨æ–¹æ³•:
    python evaluate.py --model MODEL_PATH [--data-dir DATA_DIR] [--output-dir OUTPUT_DIR]
"""

import os
import sys
import argparse
import json
from pathlib import Path
import torch
import numpy as np
import pandas as pd
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / 'src'))

from config import config
from data_processing import prepare_data
from dataset import create_data_loaders
from model import create_model
from utils import FocalLoss, load_model_checkpoint, format_time
from training import Evaluator

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è¯„ä¼°')
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data-dir', type=str, default=None, help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default=None, help='è¾“å‡ºç›®å½•è·¯å¾„')
    parser.add_argument('--batch-size', type=int, default=None, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--no-cuda', action='store_true', help='ç¦ç”¨CUDA')
    parser.add_argument('--save-predictions', action='store_true', help='ä¿å­˜é¢„æµ‹ç»“æœ')
    parser.add_argument('--plot-results', action='store_true', help='ç”Ÿæˆç»“æœå›¾è¡¨')
    
    return parser.parse_args()

def setup_directories(output_dir):
    """è®¾ç½®è¾“å‡ºç›®å½•"""
    output_dir = Path(output_dir)
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    directories = [
        output_dir,
        output_dir / 'results',
        output_dir / 'plots',
        output_dir / 'predictions'
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
    
    return output_dir

def load_model_for_evaluation(model_path, device):
    """åŠ è½½æ¨¡å‹ç”¨äºè¯„ä¼°"""
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model()
    model = model.to(device)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(model_path, map_location=device)
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        if 'config' in checkpoint:
            model_config = checkpoint['config']
            print(f"æ¨¡å‹é…ç½®: {model_config.get('backbone', 'Unknown')}")
        
        if 'epoch' in checkpoint:
            print(f"è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
        
        if 'best_val_acc' in checkpoint:
            print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {checkpoint['best_val_acc']:.2f}%")
        
        if 'best_val_auc' in checkpoint:
            print(f"æœ€ä½³éªŒè¯AUC: {checkpoint['best_val_auc']:.4f}")
    else:
        # ç›´æ¥åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸
        model.load_state_dict(checkpoint)
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼ˆç›´æ¥æ ¼å¼ï¼‰")
    
    model.eval()
    return model

def save_evaluation_results(results, metrics, output_dir, model_path):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†æŒ‡æ ‡
    metrics_path = output_dir / 'results' / f'evaluation_metrics_{timestamp}.json'
    
    # å‡†å¤‡å¯åºåˆ—åŒ–çš„æŒ‡æ ‡
    serializable_metrics = {}
    for key, value in metrics.items():
        if isinstance(value, np.ndarray):
            serializable_metrics[key] = value.tolist()
        elif isinstance(value, (np.integer, np.floating)):
            serializable_metrics[key] = float(value)
        else:
            serializable_metrics[key] = value
    
    # æ·»åŠ å…ƒä¿¡æ¯
    evaluation_info = {
        'timestamp': timestamp,
        'model_path': str(model_path),
        'evaluation_metrics': serializable_metrics,
        'performance': {
            'avg_inference_time_ms': results['avg_inference_time'] * 1000,
            'total_inference_time_s': results['total_inference_time'],
            'samples_per_second': len(results['predictions']) / results['total_inference_time']
        }
    }
    
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(evaluation_info, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_path}")
    
    # ä¿å­˜é¢„æµ‹ç»“æœ
    predictions_path = output_dir / 'predictions' / f'predictions_{timestamp}.csv'
    predictions_df = pd.DataFrame({
        'target': results['targets'],
        'prediction': results['predictions'],
        'score': results['scores']
    })
    predictions_df.to_csv(predictions_path, index=False)
    print(f"ğŸ¯ é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {predictions_path}")
    
    return metrics_path, predictions_path

def print_evaluation_summary(metrics, results):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
    print("="*60)
    
    # åŸºç¡€æŒ‡æ ‡
    print(f"å‡†ç¡®ç‡ (Accuracy):        {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"å¹³è¡¡å‡†ç¡®ç‡ (Balanced):     {metrics['balanced_accuracy']:.4f} ({metrics['balanced_accuracy']*100:.2f}%)")
    print(f"ç²¾ç¡®ç‡ (Precision):       {metrics['precision']:.4f}")
    print(f"å¬å›ç‡ (Recall):          {metrics['recall']:.4f}")
    print(f"ç‰¹å¼‚æ€§ (Specificity):     {metrics['specificity']:.4f}")
    print(f"F1åˆ†æ•° (F1-Score):        {metrics['f1']:.4f}")
    print(f"AUC-ROC:                  {metrics['auc_roc']:.4f}")
    print(f"AUC-PR:                   {metrics['auc_pr']:.4f}")
    print(f"è´Ÿé¢„æµ‹å€¼ (NPV):           {metrics['npv']:.4f}")
    
    print("\n" + "-"*40)
    print("æ··æ·†çŸ©é˜µ:")
    cm = metrics['confusion_matrix']
    print(f"çœŸè´Ÿä¾‹ (TN): {metrics['tn']:4d}  |  å‡æ­£ä¾‹ (FP): {metrics['fp']:4d}")
    print(f"å‡è´Ÿä¾‹ (FN): {metrics['fn']:4d}  |  çœŸæ­£ä¾‹ (TP): {metrics['tp']:4d}")
    
    print("\n" + "-"*40)
    print("æ€§èƒ½æŒ‡æ ‡:")
    print(f"å¹³å‡æ¨ç†æ—¶é—´:            {results['avg_inference_time']*1000:.2f} ms/batch")
    print(f"æ€»æ¨ç†æ—¶é—´:              {format_time(results['total_inference_time'])}")
    print(f"å¤„ç†é€Ÿåº¦:                {len(results['predictions'])/results['total_inference_time']:.1f} æ ·æœ¬/ç§’")
    
    print("="*60)

def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    args = parse_args()
    
    # æ›´æ–°é…ç½®
    if args.data_dir:
        config.DATA_DIR = Path(args.data_dir)
    if args.output_dir:
        config.OUTPUT_DIR = Path(args.output_dir)
    # æ³¨æ„ï¼šå¦‚æœæœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œå°†ä½¿ç”¨ config.py ä¸­çš„é»˜è®¤ OUTPUT_DIR
    if args.batch_size:
        config.BATCH_SIZE = args.batch_size
    if args.no_cuda:
        config.USE_CUDA = False
    
    # è®¾ç½®ç¯å¢ƒ
    config.setup_environment()
    device = config.get_device()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = setup_directories(config.OUTPUT_DIR)
    
    print("="*60)
    print("ğŸ” æ·±åº¦ä¼ªé€ æ£€æµ‹æ¨¡å‹è¯„ä¼°")
    print("="*60)
    print(f"æ¨¡å‹æ–‡ä»¶: {args.model}")
    print(f"æ•°æ®ç›®å½•: {config.DATA_DIR}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"è®¾å¤‡: {device}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.BATCH_SIZE}")
    print("="*60)
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_path = Path(args.model)
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return 1
    
    # æ•°æ®å‡†å¤‡
    print("\nğŸ“Š å‡†å¤‡æ•°æ®...")
    try:
        train_data, val_data, test_data = prepare_data(
            data_dir=config.DATA_DIR,
            force_reprocess=False
        )
        print(f"æµ‹è¯•é›†: {len(test_data)} ä¸ªæ ·æœ¬")
    except Exception as e:
        print(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
        return 1
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ”„ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    try:
        _, _, test_loader = create_data_loaders(
            train_data, val_data, test_data
        )
        print(f"æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        return 1
    
    # åŠ è½½æ¨¡å‹
    try:
        model = load_model_for_evaluation(model_path, device)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = FocalLoss()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Evaluator(model, test_loader, criterion)
    
    # å¼€å§‹è¯„ä¼°
    print("\nğŸ¯ å¼€å§‹è¯„ä¼°...")
    try:
        results = evaluator.evaluate()
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        metrics = evaluator.calculate_metrics(
            results['predictions'],
            results['targets'],
            results['scores']
        )
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print_evaluation_summary(metrics, results)
        
        # ä¿å­˜ç»“æœ
        metrics_path, predictions_path = save_evaluation_results(
            results, metrics, output_dir, model_path
        )
        
        # ç”Ÿæˆå›¾è¡¨
        if args.plot_results:
            print("\nğŸ“ˆ ç”Ÿæˆç»“æœå›¾è¡¨...")
            
            # æ··æ·†çŸ©é˜µ
            cm_path = output_dir / 'plots' / f'confusion_matrix_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
            evaluator.plot_confusion_matrix(metrics['confusion_matrix'], cm_path)
            
            # ROCå’ŒPRæ›²çº¿
            curves_path = output_dir / 'plots' / f'roc_pr_curves_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
            evaluator.plot_roc_pr_curves(results['targets'], results['scores'], curves_path)
        
        print("\nâœ… è¯„ä¼°å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)