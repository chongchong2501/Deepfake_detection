# Cell 4: æ•°æ®é›†ç±»å®šä¹‰

class DeepfakeVideoDataset(Dataset):
    """æ·±åº¦ä¼ªé€ è§†é¢‘æ•°æ®é›†ç±» - å…¨GPUåŠ é€Ÿç‰ˆæœ¬"""
    
    def __init__(self, csv_file=None, data_list=None, transform=None, max_frames=32, 
                 gpu_preprocessing=True, cache_frames=True, full_gpu_pipeline=True):
        if csv_file is not None:
            self.df = pd.read_csv(csv_file)
            self.data_list = None
        elif data_list is not None:
            self.data_list = data_list
            self.df = None
        else:
            raise ValueError("å¿…é¡»æä¾›csv_fileæˆ–data_list")
            
        self.transform = transform
        self.max_frames = max_frames
        self.gpu_preprocessing = gpu_preprocessing and torch.cuda.is_available()
        self.cache_frames = cache_frames
        self.full_gpu_pipeline = full_gpu_pipeline and torch.cuda.is_available()
        
        # å¤šçº§ç¼“å­˜ç³»ç»Ÿ
        self.frame_cache = {} if cache_frames else None  # CPUç¼“å­˜
        self.gpu_cache = {} if (cache_frames and self.full_gpu_pipeline) else None  # GPUç¼“å­˜
        self.cache_hits = 0
        self.cache_misses = 0
        
        # GPUé¢„å¤„ç†çš„æ ‡å‡†åŒ–å‚æ•°
        if self.gpu_preprocessing:
            self.mean = torch.tensor([0.485, 0.456, 0.406], device='cuda', dtype=torch.float16)
            self.std = torch.tensor([0.229, 0.224, 0.225], device='cuda', dtype=torch.float16)
            
        # GPUå†…å­˜ç®¡ç†
        if self.full_gpu_pipeline:
            self.max_gpu_cache_size = 100  # æœ€å¤§GPUç¼“å­˜è§†é¢‘æ•°
            self.gpu_cache_keys = []  # LRUç¼“å­˜é”®åˆ—è¡¨
            
        print(f"ğŸš€ æ•°æ®é›†åˆå§‹åŒ–: GPUæµæ°´çº¿={'å¯ç”¨' if self.full_gpu_pipeline else 'ç¦ç”¨'}, "
              f"GPUé¢„å¤„ç†={'å¯ç”¨' if self.gpu_preprocessing else 'ç¦ç”¨'}, "
              f"ç¼“å­˜={'å¯ç”¨' if self.cache_frames else 'ç¦ç”¨'}")
    
    def __len__(self):
        if self.df is not None:
            return len(self.df)
        return len(self.data_list)
    
    def __getitem__(self, idx):
        if self.data_list is not None:
            # ç›´æ¥ä»å†…å­˜ä¸­çš„æ•°æ®åˆ—è¡¨è·å–
            item = self.data_list[idx]
            frames = item['frames']
            label = item['label']
            video_path = None
        else:
            # ä»CSVæ–‡ä»¶è·å–è·¯å¾„
            row = self.df.iloc[idx]
            video_path = row['video_path']
            label = row['label']
            frames = None
        
        # å…¨GPUæµæ°´çº¿å¤„ç†
        if self.full_gpu_pipeline and video_path is not None:
            return self._process_video_gpu_pipeline(video_path, label)
        
        # ä¼ ç»Ÿå¤„ç†æµç¨‹ï¼ˆå…¼å®¹æ€§ï¼‰
        if frames is None:
            # æ£€æŸ¥CPUç¼“å­˜
            if self.cache_frames and video_path in self.frame_cache:
                frames = self.frame_cache[video_path]
                self.cache_hits += 1
            else:
                frames = extract_frames_gpu_accelerated(video_path, self.max_frames, target_size=(224, 224))
                self.cache_misses += 1
                # ç¼“å­˜å¸§æ•°æ®
                if self.cache_frames and len(frames) > 0:
                    self.frame_cache[video_path] = frames
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§
        if len(frames) == 0:
            frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.max_frames)]
        
        while len(frames) < self.max_frames:
            frames.append(frames[-1].copy() if frames else np.zeros((224, 224, 3), dtype=np.uint8))
        
        frames = frames[:self.max_frames]
        
        # é«˜æ•ˆGPUé¢„å¤„ç†æµæ°´çº¿
        if self.transform:
            frames = [self.transform(frame) for frame in frames]
            video_tensor = torch.stack(frames)
        elif self.gpu_preprocessing:
            # æ‰¹é‡GPUé¢„å¤„ç†ï¼šå‡å°‘CPU-GPUä¼ è¾“æ¬¡æ•°
            frames_array = np.stack(frames)  # (T, H, W, C)
            video_tensor = torch.from_numpy(frames_array).permute(0, 3, 1, 2).float()  # (T, C, H, W)
            
            # ä¸€æ¬¡æ€§ç§»åŠ¨åˆ°GPUå¹¶è¿›è¡Œæ‰€æœ‰é¢„å¤„ç†
            video_tensor = video_tensor.to('cuda', non_blocking=True, dtype=torch.float16) / 255.0
            
            # ä½¿ç”¨é¢„è®¡ç®—çš„æ ‡å‡†åŒ–å‚æ•°è¿›è¡Œé«˜æ•ˆæ ‡å‡†åŒ–
            video_tensor = (video_tensor - self.mean.view(1, 3, 1, 1)) / self.std.view(1, 3, 1, 1)
        else:
            # CPUé¢„å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            frames = [torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0 for frame in frames]
            video_tensor = torch.stack(frames)
        
        # æ ‡ç­¾å¤„ç†
        if self.gpu_preprocessing and not self.transform:
            label_tensor = torch.tensor(label, dtype=torch.float32, device='cuda')
        else:
            label_tensor = torch.tensor(label, dtype=torch.float32)
            if self.gpu_preprocessing:
                video_tensor = video_tensor.to('cuda', non_blocking=True)
                label_tensor = label_tensor.to('cuda', non_blocking=True)
        
        return video_tensor, label_tensor
    
    def _process_video_gpu_pipeline(self, video_path, label):
        """å®Œå…¨GPUç«¯åˆ°ç«¯çš„è§†é¢‘å¤„ç†æµæ°´çº¿"""
        # æ£€æŸ¥GPUç¼“å­˜
        if self.gpu_cache is not None and video_path in self.gpu_cache:
            self.cache_hits += 1
            self._update_gpu_cache_lru(video_path)
            video_tensor = self.gpu_cache[video_path]
            label_tensor = torch.tensor(label, dtype=torch.float32, device='cuda')
            return video_tensor, label_tensor
        
        self.cache_misses += 1
        
        try:
            # ä½¿ç”¨torchvisionç›´æ¥åœ¨GPUä¸Šè¯»å–å’Œå¤„ç†è§†é¢‘
            # importè¯­å¥å·²åœ¨cell_01ä¸­å®šä¹‰
            
            # è¯»å–è§†é¢‘
            video_tensor, _, _ = read_video(video_path, pts_unit='sec')
            
            if video_tensor.size(0) == 0:
                # åˆ›å»ºé»‘è‰²å¸§
                video_tensor = torch.zeros(self.max_frames, 224, 224, 3, dtype=torch.float16, device='cuda')
            else:
                # ç§»åŠ¨åˆ°GPU
                video_tensor = video_tensor.to('cuda', non_blocking=True, dtype=torch.float16)
                
                # æ™ºèƒ½å¸§é‡‡æ ·
                total_frames = video_tensor.size(0)
                if total_frames <= self.max_frames:
                    frame_indices = torch.arange(0, total_frames, device='cuda')
                else:
                    step = total_frames / self.max_frames
                    frame_indices = torch.arange(0, total_frames, step, device='cuda').long()[:self.max_frames]
                
                video_tensor = video_tensor[frame_indices]  # (T, H, W, C)
                
                # ç¡®ä¿å¸§æ•°è¶³å¤Ÿ
                current_frames = video_tensor.size(0)
                if current_frames < self.max_frames:
                    if current_frames > 0:
                        last_frame = video_tensor[-1:].repeat(self.max_frames - current_frames, 1, 1, 1)
                        video_tensor = torch.cat([video_tensor, last_frame], dim=0)
                    else:
                        video_tensor = torch.zeros(self.max_frames, 224, 224, 3, dtype=torch.float16, device='cuda')
                
                # è°ƒæ•´å°ºå¯¸åˆ°224x224
                video_tensor = video_tensor.permute(0, 3, 1, 2)  # (T, C, H, W)
                if video_tensor.size(-1) != 224 or video_tensor.size(-2) != 224:
                    video_tensor = F.interpolate(video_tensor, size=(224, 224), mode='bilinear', align_corners=False)
            
            # GPUä¸Šè¿›è¡Œæ ‡å‡†åŒ–
            video_tensor = video_tensor / 255.0
            video_tensor = (video_tensor - self.mean.view(1, 3, 1, 1)) / self.std.view(1, 3, 1, 1)
            
            # é™åˆ¶åˆ°æœ€å¤§å¸§æ•°
            video_tensor = video_tensor[:self.max_frames]
            
            # æ·»åŠ åˆ°GPUç¼“å­˜
            if self.gpu_cache is not None:
                self._add_to_gpu_cache(video_path, video_tensor)
            
            label_tensor = torch.tensor(label, dtype=torch.float32, device='cuda')
            return video_tensor, label_tensor
            
        except Exception as e:
            print(f"GPUæµæ°´çº¿å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•: {e}")
            # å›é€€åˆ°ä¼ ç»ŸCPUå¤„ç†
            frames = extract_frames_gpu_accelerated(video_path, self.max_frames, target_size=(224, 224))
            if len(frames) == 0:
                frames = [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.max_frames)]
            
            while len(frames) < self.max_frames:
                frames.append(frames[-1].copy() if frames else np.zeros((224, 224, 3), dtype=np.uint8))
            
            frames = frames[:self.max_frames]
            frames_array = np.stack(frames)
            video_tensor = torch.from_numpy(frames_array).permute(0, 3, 1, 2).float()
            video_tensor = video_tensor.to('cuda', non_blocking=True, dtype=torch.float16) / 255.0
            video_tensor = (video_tensor - self.mean.view(1, 3, 1, 1)) / self.std.view(1, 3, 1, 1)
            
            label_tensor = torch.tensor(label, dtype=torch.float32, device='cuda')
            return video_tensor, label_tensor
    
    def _add_to_gpu_cache(self, video_path, video_tensor):
        """æ·»åŠ è§†é¢‘åˆ°GPUç¼“å­˜ï¼Œä½¿ç”¨LRUç­–ç•¥"""
        if len(self.gpu_cache) >= self.max_gpu_cache_size:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = self.gpu_cache_keys.pop(0)
            del self.gpu_cache[oldest_key]
        
        self.gpu_cache[video_path] = video_tensor.clone()
        self.gpu_cache_keys.append(video_path)
    
    def _update_gpu_cache_lru(self, video_path):
        """æ›´æ–°GPUç¼“å­˜çš„LRUé¡ºåº"""
        if video_path in self.gpu_cache_keys:
            self.gpu_cache_keys.remove(video_path)
            self.gpu_cache_keys.append(video_path)
    
    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'gpu_cache_size': len(self.gpu_cache) if self.gpu_cache else 0,
            'cpu_cache_size': len(self.frame_cache) if self.frame_cache else 0
        }

print("âœ… æ•°æ®é›†ç±»å®šä¹‰å®Œæˆ")